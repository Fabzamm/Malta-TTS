{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "870f086e",
      "metadata": {
        "id": "870f086e"
      },
      "source": [
        "# XTTS Finetuning for Maltese"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a1c709",
      "metadata": {
        "id": "29a1c709"
      },
      "source": [
        "## Requirements\n",
        "- Python 3.8 or higher\n",
        "- PyTorch 1.7.0 or higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e30b65",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "a3e30b65",
        "outputId": "29391075-0b32-4dcc-d244-fb3a7e57536d"
      },
      "outputs": [],
      "source": [
        "# @title Takes 6min and you need to restart the session at the end\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/Wubpooz/Malta-TTS.git\n",
        "%cd Malta-TTS/FineTuning/NewLanguage\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Download NLTK and Spacy models\n",
        "!python -c \"import nltk; nltk.download('punkt')\"\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "\n",
        "# !apt-get remove -y ffmpeg\n",
        "# !pip uninstall -y torchcodec\n",
        "# !apt-get update\n",
        "# !apt-get install -y ffmpeg\n",
        "# !pip install torch==2.5.1+cu121 torchaudio==2.5.1 torchcodec==0.1.* --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install TTS\n",
        "\n",
        "!pip uninstall -y torch torchaudio torchvision\n",
        "!pip install torch==2.5.1+cu121 torchaudio==2.5.1 torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers==4.38.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697307d4",
      "metadata": {
        "id": "697307d4"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44ff4a2",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44ff4a2",
        "outputId": "e7b2d9f5-2891-4655-89c2-7c937d333122"
      },
      "outputs": [],
      "source": [
        "# @title Takes 35min initialy but after it's saved to GDrive it's instant\n",
        "# EXCRUATINGLY SLOW, 35min for saving 4900 audio files => concurrency if available\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Set the library path so torchcodec can find ffmpeg ?\n",
        "os.environ['LD_LIBRARY_PATH'] += \":/usr/lib/x86_64-linux-gnu/\"\n",
        "\n",
        "# def save_single_file(example, output_dir, i):\n",
        "#     audio_filename = example['audio']['path']\n",
        "#     audio_bytes = example['audio']['bytes']\n",
        "#     text = example['normalized_text']\n",
        "#     speaker_id = example['speaker_id']\n",
        "\n",
        "#     output_audio_path = os.path.join(output_dir, audio_filename)\n",
        "\n",
        "#     with open(output_audio_path, 'wb') as f:\n",
        "#       f.write(audio_bytes) # is it 24kHz ?\n",
        "\n",
        "#     return {\n",
        "#         'audio_file': os.path.join(\"wavs\", audio_filename),\n",
        "#         'text': text,\n",
        "#         'speaker_name': speaker_id\n",
        "#     }\n",
        "\n",
        "# def save_dataset_split_concurent(split, filename):\n",
        "#     data = []\n",
        "#     with ThreadPoolExecutor(max_workers=16) as executor:\n",
        "#         futures = [executor.submit(save_single_file, example, wavs_dir, i) for i, example in enumerate(ds[split])]\n",
        "\n",
        "#         for future in tqdm(futures, desc=f\"Processing {split} split\"):\n",
        "#             data.append(future.result())\n",
        "\n",
        "#     df = pd.DataFrame(data)\n",
        "#     df.to_csv(os.path.join(output_dir, filename), sep=\"|\", index=False)\n",
        "#     print(f\"Saved {len(df)} files to {filename}\")\n",
        "\n",
        "\n",
        "def save_dataset_split(split, filename, save_audio=True):\n",
        "  data = []\n",
        "  for example in tqdm(ds[split].to_list(), desc=f\"Processing {split} split\"):\n",
        "    audio_filename = example['audio']['path']\n",
        "    audio_bytes = example['audio']['bytes']\n",
        "    text = example['normalized_text']\n",
        "    speaker_id = example['speaker_id']\n",
        "\n",
        "    if(save_audio):\n",
        "      with open(os.path.join(wavs_dir, audio_filename), 'wb') as f:\n",
        "        f.write(audio_bytes) #TODO is it 24kHz ?\n",
        "\n",
        "    # Use LJSpeech format (extended)\n",
        "    # /!\\ audio_file shouldn't have extension, else fails | also they should just be filenames, the loader will add wav/ before and .wav after\n",
        "    name, ext = os.path.splitext(audio_filename)\n",
        "    audio_file_without_ext = name\n",
        "\n",
        "    data.append({\n",
        "      'audio_file': audio_file_without_ext,\n",
        "      'text': text,\n",
        "      'normalized_text': text,\n",
        "      'speaker_name': speaker_id\n",
        "    })\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df.to_csv(os.path.join(output_dir, filename), sep=\"|\", index=False)\n",
        "  print(f\"Saved {len(df)} files to {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/XTTS_Maltese_Data\"\n",
        "if os.path.exists(output_dir) and os.path.exists(os.path.join(output_dir, \"wavs\")) and os.path.exists(os.path.join(output_dir, \"metadata_train.csv\")):\n",
        "  print(f\"Processed dataset already exists at {output_dir}\")\n",
        "else:\n",
        "  print(\"Loading dataset from Hugging Face...\")\n",
        "  ds = load_dataset(\"Bluefir/MASRI_HEADSET_v2\")\n",
        "\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  wavs_dir = os.path.join(output_dir, \"wavs\")\n",
        "  os.makedirs(wavs_dir, exist_ok=True)\n",
        "  if os.path.exists(wavs_dir):\n",
        "    save_audio = False\n",
        "  else:\n",
        "    save_audio = True\n",
        "\n",
        "  print(\"Preparing and saving dataset files...\")\n",
        "  save_dataset_split(\"train\", \"metadata_train.csv\", save_audio)\n",
        "  save_dataset_split(\"test\", \"metadata_eval.csv\", save_audio)\n",
        "  print(\"Dataset preparation complete (saved to Google Drive too).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A1JiBiiFYWC2",
      "metadata": {
        "cellView": "form",
        "id": "A1JiBiiFYWC2"
      },
      "outputs": [],
      "source": [
        "# @title Dataset repartition\n",
        "import os\n",
        "import io\n",
        "import tempfile\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "# Don't decode audio — just keep metadata\n",
        "ds = load_dataset(\"Bluefir/MASRI_HEADSET_v2\")\n",
        "ds = ds.cast_column(\"audio\", Audio(decode=False))\n",
        "\n",
        "text_lengths = []\n",
        "audio_durations = []\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    print(f\"Processing split: {split}\")\n",
        "    for example in ds[split]:\n",
        "        # Text length\n",
        "        text_lengths.append(len(example[\"normalized_text\"]))\n",
        "\n",
        "        # Save audio bytes to temp file, read duration with soundfile\n",
        "        audio_bytes = example[\"audio\"][\"bytes\"]\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmpf:\n",
        "            tmpf.write(audio_bytes)\n",
        "            tmpf.flush()\n",
        "            with sf.SoundFile(tmpf.name) as f:\n",
        "                duration = len(f) / f.samplerate\n",
        "                audio_durations.append(duration)\n",
        "\n",
        "print(f\"Text length range: {min(text_lengths)} - {max(text_lengths)} characters\")\n",
        "print(f\"Audio duration range: {min(audio_durations):.2f} - {max(audio_durations):.2f} seconds\")\n",
        "print(f\"Average text length: {sum(text_lengths)/len(text_lengths):.2f} characters\")\n",
        "print(f\"Average audio duration: {sum(audio_durations)/len(audio_durations):.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e498624b",
      "metadata": {
        "id": "e498624b"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64e3b84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f64e3b84",
        "outputId": "48c5df1c-ae42-43de-a39c-3d9c11b8d0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n",
            "env: OMP_NUM_THREADS=1\n",
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "/content/Malta-TTS/FineTuning/NewLanguage\n",
            "Finetuning for mt\n",
            "Step 1: Downloading XTTS base model files.\n",
            " > Downloading XTTS v-main files...\n",
            " > Downloading XTTS config file...\n",
            "0.00iB [00:00, ?iB/s] > XTTS model files downloaded successfully!\n",
            "Step 2: Extending the XTTS tokenizer with the new language.\n",
            "Original tokenizer loaded with 21161 tokens.\n",
            "Training new tokenizer with 3983 texts...\n",
            "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 10066    /    10066\n",
            "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 10066    /    10066\n",
            "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 15307    /    15307\n",
            "New tokenizer trained with 15345 tokens.\n",
            "Merging tokenizers from /content/drive/MyDrive/XTTS_Maltese_Training/output/old_tokenizer/ and /content/drive/MyDrive/XTTS_Maltese_Training/output/new_tokenizer/ into /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/\n",
            "Old tokenizer vocabulary size: 21161\n",
            "New tokenizer vocabulary size: 15345\n",
            "Combined vocabulary size: 21161\n",
            "Combined vocabulary saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/vocab.json\n",
            "Tokenizer has been successfully extended and saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/vocab.json\n",
            "Updating the XTTS checkpoint...\n",
            "Cleaning checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/output/model.pth\n",
            "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
            "Cleaned checkpoint saved.\n",
            "Updating the XTTS config file...\n",
            "Updated config file saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/config.json. Added new language: mt\n",
            "Step 3: Starting GPT training.\n",
            " > Training XTTS model for Maltese with 1 datasets, 8 epochs, batch size 1, grad_acumm 48, output path: /content/drive/MyDrive/XTTS_Maltese_Training/output/training\n",
            " > Using the following datasets:\n",
            "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
            " > Downloading XTTS model files...\n",
            " > Downloading XTTS v-main files...\n",
            " > Downloading XTTS config file...\n",
            "\n",
            "4.37kiB [00:13, 313iB/s]\n",
            " > XTTS model files downloaded successfully!\n",
            " > XTTS model files downloaded successfully!\n",
            "Setting up model arguments...\n",
            "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=map_location, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/tortoise/arch_utils.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.mel_norms = torch.load(f)\n",
            "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/trainer/gpt_trainer.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n",
            ">> DVAE weights restored from: /content/drive/MyDrive/XTTS_Maltese_Training/output/dvae.pth\n",
            "Loading datasets...\n",
            " | > Found 3984 files in /content/drive/MyDrive/XTTS_Maltese_Data\n",
            " > Loaded 3984 training samples and 997 evaluation samples.\n",
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: False\n",
            " | > Precision: float32\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 1\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            "2025-08-12 14:16:02.650844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755008162.732418    8041 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755008162.756114    8041 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755008162.806003    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755008162.806032    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755008162.806039    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755008162.806046    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce\n",
            "\n",
            " > Model has 548111567 parameters\n",
            "Starting training...\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/8\u001b[0m\n",
            " --> /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce\n",
            " > Sampling by language: dict_keys(['mt'])\n",
            "\n",
            "\u001b[1m > TRAINING (2025-08-12 14:16:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:16:11 -- STEP: 0/3984 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > loss_text_ce: 0.09928756207227707  (0.09928756207227707)\n",
            "     | > loss_mel_ce: 6.890477180480957  (6.890477180480957)\n",
            "     | > loss: 0.14562010765075684  (0.14562010765075684)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.794  (0.7939767837524414)\n",
            "     | > loader_time: 2.2878  (2.2877612113952637)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:16:38 -- STEP: 50/3984 -- GLOBAL_STEP: 50\u001b[0m\n",
            "     | > loss_text_ce: 0.09945293515920639  (0.09942027702927589)\n",
            "     | > loss_mel_ce: 6.973715782165527  (nan)\n",
            "     | > loss: 0.14735768735408783  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1831  (0.16856659889221196)\n",
            "     | > loader_time: 0.0104  (0.31308802127838137)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:16:52 -- STEP: 100/3984 -- GLOBAL_STEP: 100\u001b[0m\n",
            "     | > loss_text_ce: 0.09928271174430847  (0.09940921388566494)\n",
            "     | > loss_mel_ce: 4.668110370635986  (nan)\n",
            "     | > loss: 0.09932069480419159  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1284  (0.16634904384613045)\n",
            "     | > loader_time: 0.0085  (0.17005764722824096)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:17:05 -- STEP: 150/3984 -- GLOBAL_STEP: 150\u001b[0m\n",
            "     | > loss_text_ce: 0.0993969514966011  (0.09939988871415456)\n",
            "     | > loss_mel_ce: 6.246588230133057  (nan)\n",
            "     | > loss: 0.13220803439617157  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1358  (0.16444286823272714)\n",
            "     | > loader_time: 0.0076  (0.12666624704996743)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:17:17 -- STEP: 200/3984 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > loss_text_ce: 0.09927795082330704  (0.09938823904842138)\n",
            "     | > loss_mel_ce: 6.259814739227295  (nan)\n",
            "     | > loss: 0.13248109817504883  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0993  (0.1629643666744233)\n",
            "     | > loader_time: 0.0115  (0.09805421829223632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:17:39 -- STEP: 250/3984 -- GLOBAL_STEP: 250\u001b[0m\n",
            "     | > loss_text_ce: 0.099264957010746  (0.09938302928209305)\n",
            "     | > loss_mel_ce: 5.111857891082764  (nan)\n",
            "     | > loss: 0.1085650622844696  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1409  (0.16213764381408696)\n",
            "     | > loader_time: 0.0115  (0.11901351642608642)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:18:02 -- STEP: 300/3984 -- GLOBAL_STEP: 300\u001b[0m\n",
            "     | > loss_text_ce: 0.09940383583307266  (0.09938052833080291)\n",
            "     | > loss_mel_ce: 4.946959972381592  (nan)\n",
            "     | > loss: 0.1051325798034668  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1354  (0.1589464704195659)\n",
            "     | > loader_time: 0.5438  (0.14166177272796632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:18:29 -- STEP: 350/3984 -- GLOBAL_STEP: 350\u001b[0m\n",
            "     | > loss_text_ce: 0.09936606884002686  (0.09937650371875084)\n",
            "     | > loss_mel_ce: 5.004878044128418  (nan)\n",
            "     | > loss: 0.10633842647075653  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1961  (0.15912888663155692)\n",
            "     | > loader_time: 0.4374  (0.16278553826468345)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:18:51 -- STEP: 400/3984 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > loss_text_ce: 0.0991375595331192  (0.09937523253262044)\n",
            "     | > loss_mel_ce: 3.144663095474243  (nan)\n",
            "     | > loss: 0.06757918000221252  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.175  (0.15892186701297756)\n",
            "     | > loader_time: 0.5783  (0.1683490478992463)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:19:15 -- STEP: 450/3984 -- GLOBAL_STEP: 450\u001b[0m\n",
            "     | > loss_text_ce: 0.09929818660020828  (0.09937064692378046)\n",
            "     | > loss_mel_ce: 4.862645626068115  (nan)\n",
            "     | > loss: 0.10337382555007935  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2494  (0.16169842455122202)\n",
            "     | > loader_time: 0.0061  (0.17523845407697897)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:19:40 -- STEP: 500/3984 -- GLOBAL_STEP: 500\u001b[0m\n",
            "     | > loss_text_ce: 0.09924977272748947  (0.09936700002849104)\n",
            "     | > loss_mel_ce: 4.890869617462158  (nan)\n",
            "     | > loss: 0.10396082699298859  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1445  (0.16023208856582638)\n",
            "     | > loader_time: 0.0094  (0.1858855328559876)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:20:06 -- STEP: 550/3984 -- GLOBAL_STEP: 550\u001b[0m\n",
            "     | > loss_text_ce: 0.09932360798120499  (0.09936148482290183)\n",
            "     | > loss_mel_ce: nan  (nan)\n",
            "     | > loss: nan  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2032  (0.16064514420249246)\n",
            "     | > loader_time: 0.0066  (0.19378360791639848)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:20:29 -- STEP: 600/3984 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > loss_text_ce: 0.09934771060943604  (0.09935668869564931)\n",
            "     | > loss_mel_ce: 4.994697093963623  (nan)\n",
            "     | > loss: 0.10612593591213226  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1254  (0.1609055916468303)\n",
            "     | > loader_time: 0.0056  (0.1958686021963756)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:20:50 -- STEP: 650/3984 -- GLOBAL_STEP: 650\u001b[0m\n",
            "     | > loss_text_ce: 0.09927915036678314  (0.09935260821993534)\n",
            "     | > loss_mel_ce: 4.099350452423096  (nan)\n",
            "     | > loss: 0.08747144788503647  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.102  (0.16041925833775458)\n",
            "     | > loader_time: 0.0056  (0.19444965142470158)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:21:13 -- STEP: 700/3984 -- GLOBAL_STEP: 700\u001b[0m\n",
            "     | > loss_text_ce: 0.09928202629089355  (0.09934935531445913)\n",
            "     | > loss_mel_ce: 5.430572986602783  (nan)\n",
            "     | > loss: 0.1152053102850914  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2143  (0.16049770627702994)\n",
            "     | > loader_time: 1.0631  (0.19604456629071937)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:21:35 -- STEP: 750/3984 -- GLOBAL_STEP: 750\u001b[0m\n",
            "     | > loss_text_ce: 0.09934276342391968  (0.0993479255537192)\n",
            "     | > loss_mel_ce: 4.6960320472717285  (nan)\n",
            "     | > loss: 0.0999036431312561  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1152  (0.16187055238087977)\n",
            "     | > loader_time: 0.0105  (0.1946799637476605)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:21:59 -- STEP: 800/3984 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > loss_text_ce: 0.09938839077949524  (0.09934662770479917)\n",
            "     | > loss_mel_ce: 5.075809955596924  (nan)\n",
            "     | > loss: 0.10781663656234741  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2478  (0.1629818463325501)\n",
            "     | > loader_time: 0.352  (0.19665489494800587)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:22:21 -- STEP: 850/3984 -- GLOBAL_STEP: 850\u001b[0m\n",
            "     | > loss_text_ce: 0.09924538433551788  (0.09934283975292654)\n",
            "     | > loss_mel_ce: 5.049042701721191  (nan)\n",
            "     | > loss: 0.10725601017475128  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2122  (0.1631670407687917)\n",
            "     | > loader_time: 0.006  (0.19645514460170987)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:22:42 -- STEP: 900/3984 -- GLOBAL_STEP: 900\u001b[0m\n",
            "     | > loss_text_ce: 0.0993143618106842  (0.09933980262113942)\n",
            "     | > loss_mel_ce: 4.41800594329834  (nan)\n",
            "     | > loss: 0.09411083906888962  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1074  (0.16330833832422897)\n",
            "     | > loader_time: 0.6636  (0.19477371162838422)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:23:03 -- STEP: 950/3984 -- GLOBAL_STEP: 950\u001b[0m\n",
            "     | > loss_text_ce: 0.09945549070835114  (0.09933572740146988)\n",
            "     | > loss_mel_ce: 4.069403171539307  (nan)\n",
            "     | > loss: 0.08685122430324554  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2057  (0.16301529809048307)\n",
            "     | > loader_time: 0.0087  (0.19448888803783232)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:23:26 -- STEP: 1000/3984 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > loss_text_ce: 0.0992567166686058  (0.09933151377737522)\n",
            "     | > loss_mel_ce: 5.088573932647705  (nan)\n",
            "     | > loss: 0.10807980597019196  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.145  (0.16290694594383245)\n",
            "     | > loader_time: 0.0058  (0.19553615546226516)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:23:52 -- STEP: 1050/3984 -- GLOBAL_STEP: 1050\u001b[0m\n",
            "     | > loss_text_ce: 0.09929149597883224  (0.09932853177899406)\n",
            "     | > loss_mel_ce: 3.373035430908203  (nan)\n",
            "     | > loss: 0.07234014570713043  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.102  (0.1629888452802386)\n",
            "     | > loader_time: 0.0065  (0.19963491916656506)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:24:12 -- STEP: 1100/3984 -- GLOBAL_STEP: 1100\u001b[0m\n",
            "     | > loss_text_ce: 0.0992569625377655  (0.09932512729005381)\n",
            "     | > loss_mel_ce: 5.114090919494629  (nan)\n",
            "     | > loss: 0.10861141979694366  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.279  (0.16413203196092085)\n",
            "     | > loader_time: 0.447  (0.19629223563454376)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:24:31 -- STEP: 1150/3984 -- GLOBAL_STEP: 1150\u001b[0m\n",
            "     | > loss_text_ce: 0.09925679117441177  (0.09932210992859761)\n",
            "     | > loss_mel_ce: 4.760846138000488  (nan)\n",
            "     | > loss: 0.10125215351581573  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2052  (0.164254375955333)\n",
            "     | > loader_time: 0.0636  (0.1937816545237666)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:24:54 -- STEP: 1200/3984 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > loss_text_ce: 0.09885226935148239  (0.09931814010565487)\n",
            "     | > loss_mel_ce: 5.438046455383301  (nan)\n",
            "     | > loss: 0.11535205692052841  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1098  (0.16417876005172746)\n",
            "     | > loader_time: 0.0055  (0.19487470130125686)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:25:16 -- STEP: 1250/3984 -- GLOBAL_STEP: 1250\u001b[0m\n",
            "     | > loss_text_ce: 0.09906523674726486  (0.09931496726870545)\n",
            "     | > loss_mel_ce: 5.379488468170166  (nan)\n",
            "     | > loss: 0.11413653939962387  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1064  (0.1647187438964844)\n",
            "     | > loader_time: 0.0094  (0.1936907978057862)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:25:35 -- STEP: 1300/3984 -- GLOBAL_STEP: 1300\u001b[0m\n",
            "     | > loss_text_ce: 0.09925433248281479  (0.09931252147715834)\n",
            "     | > loss_mel_ce: 4.310297966003418  (nan)\n",
            "     | > loss: 0.091865673661232  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1043  (0.1648214312700125)\n",
            "     | > loader_time: 0.0077  (0.19157507584645203)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:25:57 -- STEP: 1350/3984 -- GLOBAL_STEP: 1350\u001b[0m\n",
            "     | > loss_text_ce: 0.09925287216901779  (0.09930909138586795)\n",
            "     | > loss_mel_ce: 4.419900417327881  (nan)\n",
            "     | > loss: 0.09414902329444885  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.126  (0.1650100881082041)\n",
            "     | > loader_time: 0.0076  (0.1913108412424724)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:26:16 -- STEP: 1400/3984 -- GLOBAL_STEP: 1400\u001b[0m\n",
            "     | > loss_text_ce: 0.09921146184206009  (0.09930578429784102)\n",
            "     | > loss_mel_ce: 5.3632636070251465  (nan)\n",
            "     | > loss: 0.11380156874656677  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2113  (0.16516511559486394)\n",
            "     | > loader_time: 0.1599  (0.18995167255401618)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:26:37 -- STEP: 1450/3984 -- GLOBAL_STEP: 1450\u001b[0m\n",
            "     | > loss_text_ce: 0.09918704628944397  (0.09930209123882762)\n",
            "     | > loss_mel_ce: 5.265410900115967  (nan)\n",
            "     | > loss: 0.11176245659589767  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2146  (0.16576877610436805)\n",
            "     | > loader_time: 0.0081  (0.18876476879777587)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:26:55 -- STEP: 1500/3984 -- GLOBAL_STEP: 1500\u001b[0m\n",
            "     | > loss_text_ce: 0.09904791414737701  (0.09929870647192009)\n",
            "     | > loss_mel_ce: 4.15358829498291  (nan)\n",
            "     | > loss: 0.08859659731388092  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2475  (0.16635495408376053)\n",
            "     | > loader_time: 0.0493  (0.18529991420110073)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:27:19 -- STEP: 1550/3984 -- GLOBAL_STEP: 1550\u001b[0m\n",
            "     | > loss_text_ce: 0.09911007434129715  (0.09929572561575528)\n",
            "     | > loss_mel_ce: 4.9388041496276855  (nan)\n",
            "     | > loss: 0.10495655238628387  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2169  (0.16639484097880694)\n",
            "     | > loader_time: 0.1706  (0.18663279994841553)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:27:40 -- STEP: 1600/3984 -- GLOBAL_STEP: 1600\u001b[0m\n",
            "     | > loss_text_ce: 0.09810701757669449  (0.09929270564578481)\n",
            "     | > loss_mel_ce: 3.950199842453003  (nan)\n",
            "     | > loss: 0.08433973044157028  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0924  (0.16641944229602806)\n",
            "     | > loader_time: 0.0107  (0.1864498174190522)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:27:58 -- STEP: 1650/3984 -- GLOBAL_STEP: 1650\u001b[0m\n",
            "     | > loss_text_ce: 0.09910426288843155  (0.09928868659969539)\n",
            "     | > loss_mel_ce: 4.9734625816345215  (nan)\n",
            "     | > loss: 0.1056784838438034  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1686  (0.1663196997209028)\n",
            "     | > loader_time: 0.0068  (0.18411164717240774)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:28:16 -- STEP: 1700/3984 -- GLOBAL_STEP: 1700\u001b[0m\n",
            "     | > loss_text_ce: 0.09913639724254608  (0.09928555809837937)\n",
            "     | > loss_mel_ce: 5.142073154449463  (nan)\n",
            "     | > loss: 0.10919186472892761  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2019  (0.16651707747403308)\n",
            "     | > loader_time: 0.0057  (0.18191664162804105)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:28:33 -- STEP: 1750/3984 -- GLOBAL_STEP: 1750\u001b[0m\n",
            "     | > loss_text_ce: 0.0995119959115982  (0.09928312161139086)\n",
            "     | > loss_mel_ce: 3.8471474647521973  (nan)\n",
            "     | > loss: 0.08222207427024841  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1022  (0.16663083771296902)\n",
            "     | > loader_time: 0.0076  (0.17942268303462444)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:28:51 -- STEP: 1800/3984 -- GLOBAL_STEP: 1800\u001b[0m\n",
            "     | > loss_text_ce: 0.09920361638069153  (0.09928011961281306)\n",
            "     | > loss_mel_ce: 4.469452857971191  (nan)\n",
            "     | > loss: 0.09518034756183624  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1817  (0.16710487418704556)\n",
            "     | > loader_time: 0.0095  (0.1772287559509278)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:29:09 -- STEP: 1850/3984 -- GLOBAL_STEP: 1850\u001b[0m\n",
            "     | > loss_text_ce: 0.09947633743286133  (0.09927763602620852)\n",
            "     | > loss_mel_ce: 4.063547134399414  (nan)\n",
            "     | > loss: 0.08672966063022614  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0979  (0.1667776948052483)\n",
            "     | > loader_time: 0.007  (0.17560865363559214)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:29:27 -- STEP: 1900/3984 -- GLOBAL_STEP: 1900\u001b[0m\n",
            "     | > loss_text_ce: 0.09901562333106995  (0.0992737999401595)\n",
            "     | > loss_mel_ce: 4.578951835632324  (nan)\n",
            "     | > loss: 0.09745766222476959  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.218  (0.16648219384645155)\n",
            "     | > loader_time: 0.4737  (0.17457472663176693)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:29:43 -- STEP: 1950/3984 -- GLOBAL_STEP: 1950\u001b[0m\n",
            "     | > loss_text_ce: 0.09910154342651367  (0.09927026824691362)\n",
            "     | > loss_mel_ce: 3.8459911346435547  (nan)\n",
            "     | > loss: 0.08218943327665329  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1278  (0.16680658095922218)\n",
            "     | > loader_time: 0.1  (0.17140764089731075)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:30:01 -- STEP: 2000/3984 -- GLOBAL_STEP: 2000\u001b[0m\n",
            "     | > loss_text_ce: 0.09903059154748917  (0.09926672193780547)\n",
            "     | > loss_mel_ce: 4.320775508880615  (nan)\n",
            "     | > loss: 0.092079296708107  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1075  (0.16698893833160394)\n",
            "     | > loader_time: 0.007  (0.16954512476921088)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:30:19 -- STEP: 2050/3984 -- GLOBAL_STEP: 2050\u001b[0m\n",
            "     | > loss_text_ce: 0.09922541677951813  (0.09926187506536163)\n",
            "     | > loss_mel_ce: 5.077208518981934  (nan)\n",
            "     | > loss: 0.10784237831830978  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2421  (0.16700738255570569)\n",
            "     | > loader_time: 0.021  (0.16851480739872635)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:30:39 -- STEP: 2100/3984 -- GLOBAL_STEP: 2100\u001b[0m\n",
            "     | > loss_text_ce: 0.099253810942173  (0.09925926876210038)\n",
            "     | > loss_mel_ce: 4.917597770690918  (nan)\n",
            "     | > loss: 0.10451774299144745  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1345  (0.16725592658633273)\n",
            "     | > loader_time: 0.3623  (0.16754365080878852)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:30:54 -- STEP: 2150/3984 -- GLOBAL_STEP: 2150\u001b[0m\n",
            "     | > loss_text_ce: 0.09915103763341904  (0.09925505162671563)\n",
            "     | > loss_mel_ce: 4.742849349975586  (nan)\n",
            "     | > loss: 0.10087501257658005  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.248  (0.16705307539119266)\n",
            "     | > loader_time: 0.0082  (0.16513582052186485)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:31:09 -- STEP: 2200/3984 -- GLOBAL_STEP: 2200\u001b[0m\n",
            "     | > loss_text_ce: 0.0994110107421875  (0.09924949416382751)\n",
            "     | > loss_mel_ce: 3.498537063598633  (nan)\n",
            "     | > loss: 0.07495725154876709  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1215  (0.16682647553357197)\n",
            "     | > loader_time: 0.0176  (0.1631722355972637)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:31:29 -- STEP: 2250/3984 -- GLOBAL_STEP: 2250\u001b[0m\n",
            "     | > loss_text_ce: 0.0990557000041008  (0.09924543208546112)\n",
            "     | > loss_mel_ce: 4.901980876922607  (nan)\n",
            "     | > loss: 0.10418826341629028  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1289  (0.1669076452255247)\n",
            "     | > loader_time: 0.0058  (0.16261970021989613)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:31:47 -- STEP: 2300/3984 -- GLOBAL_STEP: 2300\u001b[0m\n",
            "     | > loss_text_ce: 0.09896831214427948  (0.09924113561277816)\n",
            "     | > loss_mel_ce: 4.305311679840088  (nan)\n",
            "     | > loss: 0.09175583720207214  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2144  (0.16689699452856294)\n",
            "     | > loader_time: 0.006  (0.1614470565837364)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:32:06 -- STEP: 2350/3984 -- GLOBAL_STEP: 2350\u001b[0m\n",
            "     | > loss_text_ce: 0.09923054277896881  (0.09923724190668867)\n",
            "     | > loss_mel_ce: 3.6421656608581543  (nan)\n",
            "     | > loss: 0.0779457539319992  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1248  (0.1671043727753008)\n",
            "     | > loader_time: 0.0072  (0.16073257770944158)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:32:25 -- STEP: 2400/3984 -- GLOBAL_STEP: 2400\u001b[0m\n",
            "     | > loss_text_ce: 0.09900063276290894  (0.09923192887256554)\n",
            "     | > loss_mel_ce: 5.0621795654296875  (nan)\n",
            "     | > loss: 0.10752458870410919  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2065  (0.1670918854077656)\n",
            "     | > loader_time: 0.0148  (0.16026364902655293)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:32:43 -- STEP: 2450/3984 -- GLOBAL_STEP: 2450\u001b[0m\n",
            "     | > loss_text_ce: 0.09915607422590256  (0.09922881427468097)\n",
            "     | > loss_mel_ce: 3.696291923522949  (nan)\n",
            "     | > loss: 0.07907183468341827  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1753  (0.16719801338351492)\n",
            "     | > loader_time: 0.0078  (0.1590521474760407)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:33:01 -- STEP: 2500/3984 -- GLOBAL_STEP: 2500\u001b[0m\n",
            "     | > loss_text_ce: 0.09900812804698944  (0.0992251816838981)\n",
            "     | > loss_mel_ce: 4.996554374694824  (nan)\n",
            "     | > loss: 0.10615755617618561  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2563  (0.16747089748382557)\n",
            "     | > loader_time: 0.0081  (0.15803108749389666)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:33:18 -- STEP: 2550/3984 -- GLOBAL_STEP: 2550\u001b[0m\n",
            "     | > loss_text_ce: 0.09910818934440613  (0.09922173894503551)\n",
            "     | > loss_mel_ce: 3.5637500286102295  (nan)\n",
            "     | > loss: 0.07630954682826996  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1158  (0.16745516683541078)\n",
            "     | > loader_time: 0.0068  (0.15692467885858882)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:33:34 -- STEP: 2600/3984 -- GLOBAL_STEP: 2600\u001b[0m\n",
            "     | > loss_text_ce: 0.09902603179216385  (0.09921823533968296)\n",
            "     | > loss_mel_ce: 4.0298542976379395  (nan)\n",
            "     | > loss: 0.08601834625005722  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1358  (0.16747378569382876)\n",
            "     | > loader_time: 0.1158  (0.15507091338817902)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:33:48 -- STEP: 2650/3984 -- GLOBAL_STEP: 2650\u001b[0m\n",
            "     | > loss_text_ce: 0.09922916442155838  (0.09921431927748459)\n",
            "     | > loss_mel_ce: 4.732815265655518  (nan)\n",
            "     | > loss: 0.10066759586334229  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.102  (0.16761385395841766)\n",
            "     | > loader_time: 0.0055  (0.15283002295584075)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:34:07 -- STEP: 2700/3984 -- GLOBAL_STEP: 2700\u001b[0m\n",
            "     | > loss_text_ce: 0.09927524626255035  (0.09921041772873326)\n",
            "     | > loss_mel_ce: 2.5958306789398193  (nan)\n",
            "     | > loss: 0.056148044764995575  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1313  (0.16787862309703117)\n",
            "     | > loader_time: 0.0082  (0.15187078511273444)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:34:23 -- STEP: 2750/3984 -- GLOBAL_STEP: 2750\u001b[0m\n",
            "     | > loss_text_ce: 0.0990535169839859  (0.09920689148523604)\n",
            "     | > loss_mel_ce: 3.9753029346466064  (nan)\n",
            "     | > loss: 0.08488243073225021  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1982  (0.16798879987543278)\n",
            "     | > loader_time: 0.4137  (0.15051718096299632)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:34:40 -- STEP: 2800/3984 -- GLOBAL_STEP: 2800\u001b[0m\n",
            "     | > loss_text_ce: 0.09901135414838791  (0.09920288054272544)\n",
            "     | > loss_mel_ce: 4.519765853881836  (nan)\n",
            "     | > loss: 0.09622453153133392  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2192  (0.16808502307959977)\n",
            "     | > loader_time: 0.7186  (0.14919208083833999)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:34:58 -- STEP: 2850/3984 -- GLOBAL_STEP: 2850\u001b[0m\n",
            "     | > loss_text_ce: 0.09894263744354248  (0.0992000456781765)\n",
            "     | > loss_mel_ce: 4.305030345916748  (nan)\n",
            "     | > loss: 0.09174944460391998  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0956  (0.16790959826686938)\n",
            "     | > loader_time: 0.0071  (0.14870943211672638)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:35:15 -- STEP: 2900/3984 -- GLOBAL_STEP: 2900\u001b[0m\n",
            "     | > loss_text_ce: 0.0989212691783905  (0.09919582306054144)\n",
            "     | > loss_mel_ce: 4.544407844543457  (nan)\n",
            "     | > loss: 0.09673602879047394  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1921  (0.1679590216998398)\n",
            "     | > loader_time: 0.0109  (0.1478414799427167)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:35:32 -- STEP: 2950/3984 -- GLOBAL_STEP: 2950\u001b[0m\n",
            "     | > loss_text_ce: 0.0988435372710228  (0.0991920073633478)\n",
            "     | > loss_mel_ce: 2.6837947368621826  (nan)\n",
            "     | > loss: 0.05797163397073746  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1356  (0.16831267114413)\n",
            "     | > loader_time: 0.006  (0.1463754369444769)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:35:48 -- STEP: 3000/3984 -- GLOBAL_STEP: 3000\u001b[0m\n",
            "     | > loss_text_ce: 0.09907123446464539  (0.09918809323012841)\n",
            "     | > loss_mel_ce: 3.716797351837158  (nan)\n",
            "     | > loss: 0.07949726283550262  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2448  (0.16844318016370158)\n",
            "     | > loader_time: 0.0073  (0.14496210710207655)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:36:04 -- STEP: 3050/3984 -- GLOBAL_STEP: 3050\u001b[0m\n",
            "     | > loss_text_ce: 0.0988815650343895  (0.09918462589627418)\n",
            "     | > loss_mel_ce: 4.454521179199219  (nan)\n",
            "     | > loss: 0.09486256539821625  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1723  (0.16843553980842982)\n",
            "     | > loader_time: 0.0074  (0.14373937239412438)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:36:21 -- STEP: 3100/3984 -- GLOBAL_STEP: 3100\u001b[0m\n",
            "     | > loss_text_ce: 0.09918961673974991  (0.09918133110769345)\n",
            "     | > loss_mel_ce: 4.306186676025391  (nan)\n",
            "     | > loss: 0.09177868068218231  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2081  (0.1684418339114037)\n",
            "     | > loader_time: 0.0074  (0.14312923985142884)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:36:37 -- STEP: 3150/3984 -- GLOBAL_STEP: 3150\u001b[0m\n",
            "     | > loss_text_ce: 0.09898459911346436  (0.09917746400312782)\n",
            "     | > loss_mel_ce: 4.395498752593994  (nan)\n",
            "     | > loss: 0.09363507479429245  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2446  (0.16851408935728543)\n",
            "     | > loader_time: 0.0073  (0.14166833029852996)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:36:56 -- STEP: 3200/3984 -- GLOBAL_STEP: 3200\u001b[0m\n",
            "     | > loss_text_ce: 0.09879665821790695  (0.09917357806116353)\n",
            "     | > loss_mel_ce: 4.076699733734131  (nan)\n",
            "     | > loss: 0.08698951452970505  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2033  (0.16850412361323847)\n",
            "     | > loader_time: 0.0079  (0.14174663409590743)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:37:12 -- STEP: 3250/3984 -- GLOBAL_STEP: 3250\u001b[0m\n",
            "     | > loss_text_ce: 0.09875252842903137  (0.09916841031496353)\n",
            "     | > loss_mel_ce: 3.8546860218048096  (nan)\n",
            "     | > loss: 0.0823633074760437  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0925  (0.16851386238978475)\n",
            "     | > loader_time: 0.0059  (0.14051638023669927)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:37:27 -- STEP: 3300/3984 -- GLOBAL_STEP: 3300\u001b[0m\n",
            "     | > loss_text_ce: 0.0988442599773407  (0.09916456483530287)\n",
            "     | > loss_mel_ce: nan  (nan)\n",
            "     | > loss: nan  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1706  (0.16843164465644156)\n",
            "     | > loader_time: 0.0063  (0.139426286437295)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:37:42 -- STEP: 3350/3984 -- GLOBAL_STEP: 3350\u001b[0m\n",
            "     | > loss_text_ce: 0.0986340120434761  (0.09916050177456738)\n",
            "     | > loss_mel_ce: 2.8958826065063477  (nan)\n",
            "     | > loss: 0.062385763972997665  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1035  (0.1682345783176708)\n",
            "     | > loader_time: 0.555  (0.13830230947750743)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:37:57 -- STEP: 3400/3984 -- GLOBAL_STEP: 3400\u001b[0m\n",
            "     | > loss_text_ce: 0.09887988865375519  (0.09915679870063777)\n",
            "     | > loss_mel_ce: 3.994640827178955  (nan)\n",
            "     | > loss: 0.08528168499469757  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1125  (0.16843470938065486)\n",
            "     | > loader_time: 0.0074  (0.13675671998192301)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:38:13 -- STEP: 3450/3984 -- GLOBAL_STEP: 3450\u001b[0m\n",
            "     | > loss_text_ce: 0.0990915596485138  (0.09915287245874829)\n",
            "     | > loss_mel_ce: 4.071514129638672  (nan)\n",
            "     | > loss: 0.08688762038946152  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1454  (0.16854400461998553)\n",
            "     | > loader_time: 0.0073  (0.13577776411305315)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:38:29 -- STEP: 3500/3984 -- GLOBAL_STEP: 3500\u001b[0m\n",
            "     | > loss_text_ce: 0.09905171394348145  (0.09914960445250794)\n",
            "     | > loss_mel_ce: 4.137903690338135  (nan)\n",
            "     | > loss: 0.0882699117064476  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2432  (0.16879733937127261)\n",
            "     | > loader_time: 0.0231  (0.13438297319412248)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:38:44 -- STEP: 3550/3984 -- GLOBAL_STEP: 3550\u001b[0m\n",
            "     | > loss_text_ce: 0.09884722530841827  (0.09914549376343347)\n",
            "     | > loss_mel_ce: 4.433685302734375  (nan)\n",
            "     | > loss: 0.09442776441574097  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2484  (0.16870984117749724)\n",
            "     | > loader_time: 0.0064  (0.1335793922988463)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:38:59 -- STEP: 3600/3984 -- GLOBAL_STEP: 3600\u001b[0m\n",
            "     | > loss_text_ce: 0.09891901165246964  (0.09914152186570906)\n",
            "     | > loss_mel_ce: 4.31115198135376  (nan)\n",
            "     | > loss: 0.09187647700309753  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0887  (0.1689253882567089)\n",
            "     | > loader_time: 0.0064  (0.13213685817188697)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:39:14 -- STEP: 3650/3984 -- GLOBAL_STEP: 3650\u001b[0m\n",
            "     | > loss_text_ce: 0.09832419455051422  (0.09913759155224457)\n",
            "     | > loss_mel_ce: 3.3833611011505127  (nan)\n",
            "     | > loss: 0.07253511250019073  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1017  (0.16907663632745626)\n",
            "     | > loader_time: 0.0075  (0.13085194300298839)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:39:31 -- STEP: 3700/3984 -- GLOBAL_STEP: 3700\u001b[0m\n",
            "     | > loss_text_ce: 0.09878326952457428  (0.09913351684607373)\n",
            "     | > loss_mel_ce: 4.585738182067871  (nan)\n",
            "     | > loss: 0.09759420156478882  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1975  (0.16913102349719497)\n",
            "     | > loader_time: 0.0079  (0.13021846732577774)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:39:47 -- STEP: 3750/3984 -- GLOBAL_STEP: 3750\u001b[0m\n",
            "     | > loss_text_ce: 0.09873799979686737  (0.09912849052548417)\n",
            "     | > loss_mel_ce: 3.954206705093384  (nan)\n",
            "     | > loss: 0.08443634957075119  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1151  (0.16901484362284355)\n",
            "     | > loader_time: 0.0064  (0.12945754489898706)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:40:04 -- STEP: 3800/3984 -- GLOBAL_STEP: 3800\u001b[0m\n",
            "     | > loss_text_ce: 0.09872560948133469  (0.0991241999950849)\n",
            "     | > loss_mel_ce: 3.5955264568328857  (nan)\n",
            "     | > loss: 0.07696358859539032  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1251  (0.1690831440373472)\n",
            "     | > loader_time: 0.0054  (0.1289726554720028)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:40:23 -- STEP: 3850/3984 -- GLOBAL_STEP: 3850\u001b[0m\n",
            "     | > loss_text_ce: 0.09875394403934479  (0.09912042394280443)\n",
            "     | > loss_mel_ce: 3.8053245544433594  (nan)\n",
            "     | > loss: 0.08133497089147568  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1108  (0.16902123637013633)\n",
            "     | > loader_time: 0.008  (0.12896455448943325)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:40:39 -- STEP: 3900/3984 -- GLOBAL_STEP: 3900\u001b[0m\n",
            "     | > loss_text_ce: 0.09896961599588394  (0.09911665627780641)\n",
            "     | > loss_mel_ce: 4.8789286613464355  (nan)\n",
            "     | > loss: 0.10370621085166931  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2274  (0.1690597430864971)\n",
            "     | > loader_time: 0.2173  (0.12827847963724415)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:40:53 -- STEP: 3950/3984 -- GLOBAL_STEP: 3950\u001b[0m\n",
            "     | > loss_text_ce: 0.09865924715995789  (0.09911268950451788)\n",
            "     | > loss_mel_ce: 4.431578159332275  (nan)\n",
            "     | > loss: 0.09437994658946991  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1317  (0.16916086571126057)\n",
            "     | > loader_time: 0.0063  (0.12695075831835814)\n",
            "\n",
            " > Filtering invalid eval samples!!\n",
            " > Total eval samples after filtering: 858\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.03765715211803737 \u001b[0m(+0)\n",
            "     | > avg_loss_text_ce: 0.09876723881363163 \u001b[0m(+0)\n",
            "     | > avg_loss_mel_ce: 4.2465973203630165 \u001b[0m(+0)\n",
            "     | > avg_loss: 4.3453645564293994 \u001b[0m(+0)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce/best_model_3984.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/8\u001b[0m\n",
            " --> /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce\n",
            "\n",
            "\u001b[1m > TRAINING (2025-08-12 14:52:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:52:32 -- STEP: 16/3984 -- GLOBAL_STEP: 4000\u001b[0m\n",
            "     | > loss_text_ce: 0.09864630550146103  (0.09876311616972089)\n",
            "     | > loss_mel_ce: 4.018118381500244  (4.221291705965996)\n",
            "     | > loss: 0.08576592803001404  (0.09000114304944873)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1575  (0.19664740562438965)\n",
            "     | > loader_time: 0.0146  (0.0919196754693985)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:52:52 -- STEP: 66/3984 -- GLOBAL_STEP: 4050\u001b[0m\n",
            "     | > loss_text_ce: 0.0989271029829979  (0.09875963978243596)\n",
            "     | > loss_mel_ce: 4.334038734436035  (4.200586434566613)\n",
            "     | > loss: 0.092353455722332  (0.08956971154971556)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.6712  (0.29141980229002057)\n",
            "     | > loader_time: 0.067  (0.05848944187164306)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:53:13 -- STEP: 116/3984 -- GLOBAL_STEP: 4100\u001b[0m\n",
            "     | > loss_text_ce: 0.0987841933965683  (0.09876143534121844)\n",
            "     | > loss_mel_ce: 4.58010196685791  (nan)\n",
            "     | > loss: 0.09747679531574249  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.178  (0.2842027688848561)\n",
            "     | > loader_time: 0.0088  (0.06815818260455954)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:53:29 -- STEP: 166/3984 -- GLOBAL_STEP: 4150\u001b[0m\n",
            "     | > loss_text_ce: 0.09891452640295029  (0.09874986848199224)\n",
            "     | > loss_mel_ce: 4.251620769500732  (nan)\n",
            "     | > loss: 0.09063615649938583  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2072  (0.2618206322911275)\n",
            "     | > loader_time: 0.7247  (0.06436208070042619)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:53:47 -- STEP: 216/3984 -- GLOBAL_STEP: 4200\u001b[0m\n",
            "     | > loss_text_ce: 0.09880383312702179  (0.09873968369706913)\n",
            "     | > loss_mel_ce: 3.678452491760254  (nan)\n",
            "     | > loss: 0.07869283854961395  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1381  (0.25295420598100743)\n",
            "     | > loader_time: 0.0134  (0.06432807666284063)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:54:04 -- STEP: 266/3984 -- GLOBAL_STEP: 4250\u001b[0m\n",
            "     | > loss_text_ce: 0.09880032390356064  (0.09873750590180096)\n",
            "     | > loss_mel_ce: 4.646416664123535  (nan)\n",
            "     | > loss: 0.09885868430137634  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.391  (0.2478815397821871)\n",
            "     | > loader_time: 0.0461  (0.06287638854263423)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:54:20 -- STEP: 316/3984 -- GLOBAL_STEP: 4300\u001b[0m\n",
            "     | > loss_text_ce: 0.09867607802152634  (0.098734160201459)\n",
            "     | > loss_mel_ce: 4.453042507171631  (nan)\n",
            "     | > loss: 0.09482747316360474  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2499  (0.23871880694280698)\n",
            "     | > loader_time: 0.0086  (0.06050515401212479)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:54:37 -- STEP: 366/3984 -- GLOBAL_STEP: 4350\u001b[0m\n",
            "     | > loss_text_ce: 0.09875158965587616  (0.09873205356177736)\n",
            "     | > loss_mel_ce: 4.521453380584717  (nan)\n",
            "     | > loss: 0.09625427424907684  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2773  (0.23095187333112205)\n",
            "     | > loader_time: 0.8658  (0.061850730187254566)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:54:52 -- STEP: 416/3984 -- GLOBAL_STEP: 4400\u001b[0m\n",
            "     | > loss_text_ce: 0.09765622764825821  (0.09872639544594747)\n",
            "     | > loss_mel_ce: nan  (nan)\n",
            "     | > loss: nan  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1078  (0.2244183042874703)\n",
            "     | > loader_time: 0.0078  (0.059188501192973246)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:55:06 -- STEP: 466/3984 -- GLOBAL_STEP: 4450\u001b[0m\n",
            "     | > loss_text_ce: 0.09862083941698074  (0.09872359395538788)\n",
            "     | > loss_mel_ce: 4.504619598388672  (nan)\n",
            "     | > loss: 0.09590084850788116  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1414  (0.21873143660664046)\n",
            "     | > loader_time: 0.0089  (0.056904322599648435)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:55:21 -- STEP: 516/3984 -- GLOBAL_STEP: 4500\u001b[0m\n",
            "     | > loss_text_ce: 0.09878622740507126  (0.09871689497327157)\n",
            "     | > loss_mel_ce: 4.3668212890625  (nan)\n",
            "     | > loss: 0.09303349256515503  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.214  (0.21494033770967824)\n",
            "     | > loader_time: 0.0101  (0.05486897350281711)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:55:37 -- STEP: 566/3984 -- GLOBAL_STEP: 4550\u001b[0m\n",
            "     | > loss_text_ce: 0.09858929365873337  (0.09871010266428701)\n",
            "     | > loss_mel_ce: 4.432419300079346  (nan)\n",
            "     | > loss: 0.09439601749181747  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1824  (0.2121753064987937)\n",
            "     | > loader_time: 0.0067  (0.055011560133404976)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:55:52 -- STEP: 616/3984 -- GLOBAL_STEP: 4600\u001b[0m\n",
            "     | > loss_text_ce: 0.09858401864767075  (0.09870664899951065)\n",
            "     | > loss_mel_ce: 4.2974019050598145  (nan)\n",
            "     | > loss: 0.09158304333686829  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1846  (0.20954065856995507)\n",
            "     | > loader_time: 0.0085  (0.05396885647402184)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:56:07 -- STEP: 666/3984 -- GLOBAL_STEP: 4650\u001b[0m\n",
            "     | > loss_text_ce: 0.0988227128982544  (0.09870434592704518)\n",
            "     | > loss_mel_ce: 4.145873069763184  (nan)\n",
            "     | > loss: 0.08843116462230682  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1404  (0.2055612644991716)\n",
            "     | > loader_time: 0.3391  (0.05542350101757338)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:56:22 -- STEP: 716/3984 -- GLOBAL_STEP: 4700\u001b[0m\n",
            "     | > loss_text_ce: 0.09893987327814102  (0.09870218658330722)\n",
            "     | > loss_mel_ce: 3.4640564918518066  (nan)\n",
            "     | > loss: 0.07422909140586853  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.0961  (0.20325112842314724)\n",
            "     | > loader_time: 0.0058  (0.05426904542486097)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:56:36 -- STEP: 766/3984 -- GLOBAL_STEP: 4750\u001b[0m\n",
            "     | > loss_text_ce: 0.09894536435604095  (0.09869773580884811)\n",
            "     | > loss_mel_ce: 3.7679102420806885  (nan)\n",
            "     | > loss: 0.08055949211120605  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1124  (0.2004220678663128)\n",
            "     | > loader_time: 0.0081  (0.05369145378431516)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:56:51 -- STEP: 816/3984 -- GLOBAL_STEP: 4800\u001b[0m\n",
            "     | > loss_text_ce: 0.09863734990358353  (0.0986934019088307)\n",
            "     | > loss_mel_ce: 4.883990287780762  (nan)\n",
            "     | > loss: 0.10380475223064423  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1123  (0.198600026322346)\n",
            "     | > loader_time: 0.0154  (0.05262343468619329)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:57:06 -- STEP: 866/3984 -- GLOBAL_STEP: 4850\u001b[0m\n",
            "     | > loss_text_ce: 0.09866681694984436  (0.09868967032391261)\n",
            "     | > loss_mel_ce: 4.424931526184082  (nan)\n",
            "     | > loss: 0.09424163401126862  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.249  (0.1977760956964646)\n",
            "     | > loader_time: 0.037  (0.05222702053898203)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:57:21 -- STEP: 916/3984 -- GLOBAL_STEP: 4900\u001b[0m\n",
            "     | > loss_text_ce: 0.09866340458393097  (0.09867481780787483)\n",
            "     | > loss_mel_ce: 4.952493667602539  (nan)\n",
            "     | > loss: 0.10523243993520737  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.2891  (0.19678650568666406)\n",
            "     | > loader_time: 0.0076  (0.05128348862760454)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:57:36 -- STEP: 966/3984 -- GLOBAL_STEP: 4950\u001b[0m\n",
            "     | > loss_text_ce: 0.09859098494052887  (0.09867227469295203)\n",
            "     | > loss_mel_ce: 3.475177049636841  (nan)\n",
            "     | > loss: 0.07445350289344788  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1129  (0.19651616977115094)\n",
            "     | > loader_time: 0.0089  (0.05008790162285911)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:57:53 -- STEP: 1016/3984 -- GLOBAL_STEP: 5000\u001b[0m\n",
            "     | > loss_text_ce: 0.09860765188932419  (0.09866993121186815)\n",
            "     | > loss_mel_ce: 4.893533229827881  (nan)\n",
            "     | > loss: 0.1040029376745224  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.256  (0.19536193055430726)\n",
            "     | > loader_time: 0.0078  (0.05193655509648362)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce/checkpoint_5000.pth\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-08-12 14:58:51 -- STEP: 1066/3984 -- GLOBAL_STEP: 5050\u001b[0m\n",
            "     | > loss_text_ce: 0.09848766028881073  (0.09866385172360612)\n",
            "     | > loss_mel_ce: 3.4435598850250244  (nan)\n",
            "     | > loss: 0.07379265874624252  (nan)\n",
            "     | > current_lr: 5e-06 \n",
            "     | > step_time: 0.1668  (0.1943942469459089)\n",
            "     | > loader_time: 0.0069  (0.051590025536785905)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define paths and parameters\n",
        "metadata_train_path = \"/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv\"\n",
        "metadata_eval_path = \"/content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv\"\n",
        "output_path = \"/content/drive/MyDrive/XTTS_Maltese_Training/output\"\n",
        "language_code = \"mt\"\n",
        "extended_vocab_size_param = 100000\n",
        "\n",
        "%env TOKENIZERS_PARALLELISM=false\n",
        "%env OMP_NUM_THREADS=1\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "\n",
        "import sys\n",
        "_original_stdout = sys.stdout\n",
        "_original_stderr = sys.stderr\n",
        "_log_file = None\n",
        "\n",
        "def output_redirect(redirect=True):\n",
        "  global _log_file\n",
        "  if not redirect:\n",
        "    sys.stdout = _original_stdout\n",
        "    sys.stderr = _original_stderr\n",
        "    if _log_file:\n",
        "      _log_file.close()\n",
        "      _log_file = None\n",
        "  else:\n",
        "    import os\n",
        "    log_path = \"/content/drive/MyDrive/XTTS_Maltese_Training/output/full_training.log\"\n",
        "    # Clear the log file by opening in write mode and closing immediately\n",
        "    with open(log_path, \"w\"):\n",
        "      pass\n",
        "    _log_file = open(log_path, \"a\", buffering=1)  # line-buffered\n",
        "\n",
        "    class Tee(object):\n",
        "      def __init__(self, *streams):\n",
        "        self.streams = streams\n",
        "      def write(self, data):\n",
        "        for s in self.streams:\n",
        "          s.write(data)\n",
        "          s.flush()\n",
        "      def flush(self):\n",
        "        for s in self.streams:\n",
        "          s.flush()\n",
        "\n",
        "    sys.stdout = Tee(_original_stdout, _log_file)\n",
        "    sys.stderr = Tee(_original_stderr, _log_file)\n",
        "\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "%cd /content/Malta-TTS/FineTuning/NewLanguage\n",
        "\n",
        "\n",
        "output_redirect(False)\n",
        "\n",
        "print(f\"Finetuning for {language_code}\")\n",
        "\n",
        "\n",
        "\n",
        "!python new_language_training_cli.py \\\n",
        "    --is_download \\\n",
        "    --is_tokenizer_extension \\\n",
        "    --output_path \"{output_path}\" \\\n",
        "    --metadatas \"{metadata_train_path},{metadata_eval_path},{language_code}\" \\\n",
        "    --num_epochs 1 \\\n",
        "    --batch_size 1 \\\n",
        "    --grad_acumm 48 \\\n",
        "    --max_audio_length 176400 \\\n",
        "    --max_text_length 200 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --lr 5e-6 \\\n",
        "    --save_step 5000 \\\n",
        "    --custom_model=custom_model_name \\\n",
        "    --version=main \\\n",
        "    --metadata_path \"{metadata_train_path}\" \\\n",
        "    --language \"{language_code}\" \\\n",
        "    --extended_vocab_size {extended_vocab_size_param}\n",
        "\n",
        "\n",
        "# 35min/epoch on one T4 with batch_size=1, grad_acumm=48, audio_length=176400, max_text=200, weight=1e-2, save_step=5000\n",
        "\n",
        "output_redirect(False)\n",
        "\n",
        "# Default values are:\n",
        "# batch-size: 3\n",
        "# grad_acc: 84\n",
        "# max_audio: 255995 = 11.6s\n",
        "# save_step: 10_000\n",
        "# epoch: 10 => 100\n",
        "# --multi-gpu\n",
        "\n",
        "print(\"Finetuning process completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CodxZgxfUa2H",
      "metadata": {
        "id": "CodxZgxfUa2H"
      },
      "source": [
        "### Logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g7PMcFPdUG1Y",
      "metadata": {
        "id": "g7PMcFPdUG1Y"
      },
      "source": [
        "```\n",
        "# Define paths and parameters\n",
        "metadata_train_path = \"/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv\"\n",
        "metadata_eval_path = \"/content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv\"\n",
        "output_path = \"/content/drive/MyDrive/XTTS_Maltese_Training/output\"\n",
        "language_code = \"mt\"\n",
        "extended_vocab_size_param = 100000\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "%cd /content/Malta-TTS/FineTuning/NewLanguage\n",
        "\n",
        "/content/Malta-TTS/FineTuning/NewLanguage\n",
        "Step 1: Downloading XTTS base model files.\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "0.00iB [00:00, ?iB/s] > XTTS model files downloaded successfully!\n",
        "Step 2: Extending the XTTS tokenizer with the new language.\n",
        "Original tokenizer loaded with 21161 tokens.\n",
        "Training new tokenizer with 3984 texts...\n",
        "[00:00:00] Tokenize words                 ██████████████████ 10067    /    10067\n",
        "[00:00:00] Count pairs                    ██████████████████ 10067    /    10067\n",
        "[00:00:00] Compute merges                 ██████████████████ 15308    /    15308\n",
        "New tokenizer trained with 15346 tokens.\n",
        "Merging tokenizers from /content/drive/MyDrive/XTTS_Maltese_Training/output/old_tokenizer/ and /content/drive/MyDrive/XTTS_Maltese_Training/output/new_tokenizer/ into /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/\n",
        "Old tokenizer vocabulary size: 21161\n",
        "New tokenizer vocabulary size: 15346\n",
        "Combined vocabulary size: 21161\n",
        "Combined vocabulary saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/vocab.json\n",
        "Tokenizer has been successfully extended and saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/vocab.json\n",
        "Updating the XTTS checkpoint...\n",
        "Cleaning checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/output/model.pth\n",
        "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
        "Cleaned checkpoint saved.\n",
        "Updating the XTTS config file...\n",
        "Step 3: Starting GPT training.\n",
        " > Training XTTS model for Maltese with 1 datasets, 8 epochs, batch size 3, grad_acumm 84, output path: /content/drive/MyDrive/XTTS_Maltese_Training/output/run/training\n",
        " > Using the following datasets:\n",
        "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
        " > Downloading XTTS model files...\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "\n",
        "4.37kiB [01:03, 69.2iB/s]\n",
        " > XTTS model files downloaded successfully!\n",
        " > XTTS model files downloaded successfully!\n",
        "Setting up model arguments...\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  return torch.load(f, map_location=map_location, **kwargs)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/tortoise/arch_utils.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  self.mel_norms = torch.load(f)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/trainer/gpt_trainer.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n",
        ">> DVAE weights restored from: /content/drive/MyDrive/XTTS_Maltese_Training/output/dvae.pth\n",
        "Loading datasets...\n",
        " | > Found 3983 files in /content/drive/MyDrive/XTTS_Maltese_Data\n",
        " > Loaded 3983 training samples and 996 evaluation samples.\n",
        " > Training Environment:\n",
        " | > Backend: Torch\n",
        " | > Mixed precision: False\n",
        " | > Precision: float32\n",
        " | > Current device: 0\n",
        " | > Num. of GPUs: 1\n",
        " | > Num. of CPUs: 2\n",
        " | > Num. of Torch Threads: 1\n",
        " | > Torch seed: 1\n",
        " | > Torch CUDNN: True\n",
        " | > Torch CUDNN deterministic: False\n",
        " | > Torch CUDNN benchmark: False\n",
        " | > Torch TF32 MatMul: False\n",
        "2025-08-11 08:47:24.121184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
        "E0000 00:00:1754902044.384214    2626 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
        "E0000 00:00:1754902044.456423    2626 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
        "W0000 00:00:1754902044.985120    2626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1754902044.985179    2626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1754902044.985190    2626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1754902044.985197    2626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/XTTS_Maltese_Training/output/run/training/GPT_XTTS_FT-August-11-2025_08+47AM-3fb6ef8\n",
        "\n",
        " > Model has 548111567 parameters\n",
        "Starting training...\n",
        "\n",
        " > EPOCH: 0/8\n",
        " --> /content/drive/MyDrive/XTTS_Maltese_Training/output/run/training/GPT_XTTS_FT-August-11-2025_08+47AM-3fb6ef8\n",
        " > Sampling by language: dict_keys(['mt'])\n",
        "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
        "  warnings.warn(\n",
        "\n",
        " > TRAINING (2025-08-11 08:47:30)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "To disable this warning, you can either:\n",
        "\t- Avoid using `tokenizers` before the fork if possible\n",
        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RfgKuAoEPZo9",
      "metadata": {
        "id": "RfgKuAoEPZo9"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "/content/Malta-TTS/FineTuning/NewLanguage\n",
        "Step 1: Downloading XTTS base model files.\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "0.00iB [00:00, ?iB/s] > XTTS model files downloaded successfully!\n",
        "Step 2: Extending the XTTS tokenizer with the new language.\n",
        "Original tokenizer loaded with 21161 tokens.\n",
        "Training new tokenizer with 3983 texts...\n",
        "[00:00:00] Tokenize words                 ██████████████████ 10066    /    10066\n",
        "[00:00:00] Count pairs                    ██████████████████ 10066    /    10066\n",
        "[00:00:00] Compute merges                 ██████████████████ 15307    /    15307\n",
        "New tokenizer trained with 15345 tokens.\n",
        "Merging tokenizers from /content/drive/MyDrive/XTTS_Maltese_Training/output/old_tokenizer/ and /content/drive/MyDrive/XTTS_Maltese_Training/output/new_tokenizer/ into /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/\n",
        "Old tokenizer vocabulary size: 21161\n",
        "New tokenizer vocabulary size: 15345\n",
        "Combined vocabulary size: 21161\n",
        "Combined vocabulary saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/vocab.json\n",
        "Tokenizer has been successfully extended and saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/vocab.json\n",
        "Updating the XTTS checkpoint...\n",
        "Cleaning checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/output/model.pth\n",
        "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
        "Cleaned checkpoint saved.\n",
        "Updating the XTTS config file...\n",
        "Step 3: Starting GPT training.\n",
        " > Training XTTS model for Maltese with 1 datasets, 8 epochs, batch size 3, grad_acumm 84, output path: /content/drive/MyDrive/XTTS_Maltese_Training/output/training\n",
        " > Using the following datasets:\n",
        "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
        " > Downloading XTTS model files...\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "\n",
        "4.37kiB [00:16, 259iB/s]\n",
        " > XTTS model files downloaded successfully!\n",
        " > XTTS model files downloaded successfully!\n",
        "Setting up model arguments...\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B4gu20w7k9U1",
      "metadata": {
        "id": "B4gu20w7k9U1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "env: TOKENIZERS_PARALLELISM=false\n",
        "env: OMP_NUM_THREADS=1\n",
        "/content/Malta-TTS/FineTuning/NewLanguage\n",
        "Step 1: Downloading XTTS base model files.\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "0.00iB [00:00, ?iB/s] > XTTS model files downloaded successfully!\n",
        "Step 2: Extending the XTTS tokenizer with the new language.\n",
        "Original tokenizer loaded with 21161 tokens.\n",
        "Training new tokenizer with 3983 texts...\n",
        "[00:00:00] Tokenize words                 ██████████████████ 10066    /    10066\n",
        "[00:00:00] Count pairs                    ██████████████████ 10066    /    10066\n",
        "[00:00:00] Compute merges                 ██████████████████ 15307    /    15307\n",
        "New tokenizer trained with 15345 tokens.\n",
        "Merging tokenizers from /content/drive/MyDrive/XTTS_Maltese_Training/output/old_tokenizer/ and /content/drive/MyDrive/XTTS_Maltese_Training/output/new_tokenizer/ into /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/\n",
        "Old tokenizer vocabulary size: 21161\n",
        "New tokenizer vocabulary size: 15345\n",
        "Combined vocabulary size: 21161\n",
        "Combined vocabulary saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/vocab.json\n",
        "Tokenizer has been successfully extended and saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/vocab.json\n",
        "Updating the XTTS checkpoint...\n",
        "Cleaning checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/output/model.pth\n",
        "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
        "Cleaned checkpoint saved.\n",
        "Updating the XTTS config file...\n",
        "Updated config file saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/config.json. Added new language: mt\n",
        "Step 3: Starting GPT training.\n",
        " > Training XTTS model for Maltese with 1 datasets, 8 epochs, batch size 3, grad_acumm 84, output path: /content/drive/MyDrive/XTTS_Maltese_Training/output/training\n",
        " > Using the following datasets:\n",
        "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
        " > Downloading XTTS model files...\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "\n",
        "4.37kiB [00:44, 98.1iB/s]\n",
        " > XTTS model files downloaded successfully!\n",
        " > XTTS model files downloaded successfully!\n",
        "Setting up model arguments...\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  return torch.load(f, map_location=map_location, **kwargs)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/tortoise/arch_utils.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  self.mel_norms = torch.load(f)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/trainer/gpt_trainer.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n",
        ">> DVAE weights restored from: /content/drive/MyDrive/XTTS_Maltese_Training/output/dvae.pth\n",
        "Loading datasets...\n",
        " | > Found 3984 files in /content/drive/MyDrive/XTTS_Maltese_Data\n",
        " > Loaded 3984 training samples and 997 evaluation samples.\n",
        " > Training Environment:\n",
        " | > Backend: Torch\n",
        " | > Mixed precision: False\n",
        " | > Precision: float32\n",
        " | > Current device: 0\n",
        " | > Num. of GPUs: 1\n",
        " | > Num. of CPUs: 2\n",
        " | > Num. of Torch Threads: 1\n",
        " | > Torch seed: 1\n",
        " | > Torch CUDNN: True\n",
        " | > Torch CUDNN deterministic: False\n",
        " | > Torch CUDNN benchmark: False\n",
        " | > Torch TF32 MatMul: False\n",
        "2025-08-12 13:58:09.694672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
        "E0000 00:00:1755007089.960026    3292 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
        "E0000 00:00:1755007090.035688    3292 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
        "W0000 00:00:1755007090.567483    3292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755007090.567517    3292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755007090.567523    3292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755007090.567527    3292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_01+58PM-87ed0ce\n",
        "\n",
        " > Model has 548111567 parameters\n",
        "Starting training...\n",
        "\n",
        " > EPOCH: 0/8\n",
        " --> /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_01+58PM-87ed0ce\n",
        " > Sampling by language: dict_keys(['mt'])\n",
        "\n",
        " > TRAINING (2025-08-12 13:58:15)\n",
        "\n",
        "   --> TIME: 2025-08-12 13:58:36 -- STEP: 0/1328 -- GLOBAL_STEP: 0\n",
        "     | > loss_text_ce: 0.09944003075361252  (0.09944003075361252)\n",
        "     | > loss_mel_ce: 5.0861430168151855  (5.0861430168151855)\n",
        "     | > loss: 0.06173313409090042  (0.06173313409090042)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 1.673  (1.6729779243469238)\n",
        "     | > loader_time: 18.7882  (18.78819513320923)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NXRCTwPsYiA",
      "metadata": {
        "id": "2NXRCTwPsYiA"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "env: TOKENIZERS_PARALLELISM=false\n",
        "env: OMP_NUM_THREADS=1\n",
        "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "/content/Malta-TTS/FineTuning/NewLanguage\n",
        "Finetuning for mt\n",
        "Step 1: Downloading XTTS base model files.\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "0.00iB [00:00, ?iB/s] > XTTS model files downloaded successfully!\n",
        "Step 2: Extending the XTTS tokenizer with the new language.\n",
        "Original tokenizer loaded with 21161 tokens.\n",
        "Training new tokenizer with 3983 texts...\n",
        "[00:00:00] Tokenize words                 ██████████████████ 10066    /    10066\n",
        "[00:00:00] Count pairs                    ██████████████████ 10066    /    10066\n",
        "[00:00:00] Compute merges                 ██████████████████ 15307    /    15307\n",
        "New tokenizer trained with 15345 tokens.\n",
        "Merging tokenizers from /content/drive/MyDrive/XTTS_Maltese_Training/output/old_tokenizer/ and /content/drive/MyDrive/XTTS_Maltese_Training/output/new_tokenizer/ into /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/\n",
        "Old tokenizer vocabulary size: 21161\n",
        "New tokenizer vocabulary size: 15345\n",
        "Combined vocabulary size: 21161\n",
        "Combined vocabulary saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/merged_tokenizer/vocab.json\n",
        "Tokenizer has been successfully extended and saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/vocab.json\n",
        "Updating the XTTS checkpoint...\n",
        "Cleaning checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/output/model.pth\n",
        "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
        "Cleaned checkpoint saved.\n",
        "Updating the XTTS config file...\n",
        "Updated config file saved to /content/drive/MyDrive/XTTS_Maltese_Training/output/config.json. Added new language: mt\n",
        "Step 3: Starting GPT training.\n",
        " > Training XTTS model for Maltese with 1 datasets, 8 epochs, batch size 1, grad_acumm 48, output path: /content/drive/MyDrive/XTTS_Maltese_Training/output/training\n",
        " > Using the following datasets:\n",
        "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
        " > Downloading XTTS model files...\n",
        " > Downloading XTTS v-main files...\n",
        " > Downloading XTTS config file...\n",
        "\n",
        "4.37kiB [00:13, 313iB/s]\n",
        " > XTTS model files downloaded successfully!\n",
        " > XTTS model files downloaded successfully!\n",
        "Setting up model arguments...\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  return torch.load(f, map_location=map_location, **kwargs)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/tortoise/arch_utils.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  self.mel_norms = torch.load(f)\n",
        "/usr/local/lib/python3.11/dist-packages/TTS/tts/layers/xtts/trainer/gpt_trainer.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
        "  dvae_checkpoint = torch.load(self.args.dvae_checkpoint, map_location=torch.device(\"cpu\"))\n",
        ">> DVAE weights restored from: /content/drive/MyDrive/XTTS_Maltese_Training/output/dvae.pth\n",
        "Loading datasets...\n",
        " | > Found 3984 files in /content/drive/MyDrive/XTTS_Maltese_Data\n",
        " > Loaded 3984 training samples and 997 evaluation samples.\n",
        " > Training Environment:\n",
        " | > Backend: Torch\n",
        " | > Mixed precision: False\n",
        " | > Precision: float32\n",
        " | > Current device: 0\n",
        " | > Num. of GPUs: 1\n",
        " | > Num. of CPUs: 2\n",
        " | > Num. of Torch Threads: 1\n",
        " | > Torch seed: 1\n",
        " | > Torch CUDNN: True\n",
        " | > Torch CUDNN deterministic: False\n",
        " | > Torch CUDNN benchmark: False\n",
        " | > Torch TF32 MatMul: False\n",
        "2025-08-12 14:16:02.650844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
        "E0000 00:00:1755008162.732418    8041 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
        "E0000 00:00:1755008162.756114    8041 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
        "W0000 00:00:1755008162.806003    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755008162.806032    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755008162.806039    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        "W0000 00:00:1755008162.806046    8041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
        " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce\n",
        "\n",
        " > Model has 548111567 parameters\n",
        "Starting training...\n",
        "\n",
        " > EPOCH: 0/8\n",
        " --> /content/drive/MyDrive/XTTS_Maltese_Training/output/training/GPT_XTTS_FT-August-12-2025_02+16PM-87ed0ce\n",
        " > Sampling by language: dict_keys(['mt'])\n",
        "\n",
        " > TRAINING (2025-08-12 14:16:08)\n",
        "\n",
        "   --> TIME: 2025-08-12 14:16:11 -- STEP: 0/3984 -- GLOBAL_STEP: 0\n",
        "     | > loss_text_ce: 0.09928756207227707  (0.09928756207227707)\n",
        "     | > loss_mel_ce: 6.890477180480957  (6.890477180480957)\n",
        "     | > loss: 0.14562010765075684  (0.14562010765075684)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.794  (0.7939767837524414)\n",
        "     | > loader_time: 2.2878  (2.2877612113952637)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:16:38 -- STEP: 50/3984 -- GLOBAL_STEP: 50\n",
        "     | > loss_text_ce: 0.09945293515920639  (0.09942027702927589)\n",
        "     | > loss_mel_ce: 6.973715782165527  (nan)\n",
        "     | > loss: 0.14735768735408783  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1831  (0.16856659889221196)\n",
        "     | > loader_time: 0.0104  (0.31308802127838137)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:16:52 -- STEP: 100/3984 -- GLOBAL_STEP: 100\n",
        "     | > loss_text_ce: 0.09928271174430847  (0.09940921388566494)\n",
        "     | > loss_mel_ce: 4.668110370635986  (nan)\n",
        "     | > loss: 0.09932069480419159  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1284  (0.16634904384613045)\n",
        "     | > loader_time: 0.0085  (0.17005764722824096)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:17:05 -- STEP: 150/3984 -- GLOBAL_STEP: 150\n",
        "     | > loss_text_ce: 0.0993969514966011  (0.09939988871415456)\n",
        "     | > loss_mel_ce: 6.246588230133057  (nan)\n",
        "     | > loss: 0.13220803439617157  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1358  (0.16444286823272714)\n",
        "     | > loader_time: 0.0076  (0.12666624704996743)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:17:17 -- STEP: 200/3984 -- GLOBAL_STEP: 200\n",
        "     | > loss_text_ce: 0.09927795082330704  (0.09938823904842138)\n",
        "     | > loss_mel_ce: 6.259814739227295  (nan)\n",
        "     | > loss: 0.13248109817504883  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.0993  (0.1629643666744233)\n",
        "     | > loader_time: 0.0115  (0.09805421829223632)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:17:39 -- STEP: 250/3984 -- GLOBAL_STEP: 250\n",
        "     | > loss_text_ce: 0.099264957010746  (0.09938302928209305)\n",
        "     | > loss_mel_ce: 5.111857891082764  (nan)\n",
        "     | > loss: 0.1085650622844696  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1409  (0.16213764381408696)\n",
        "     | > loader_time: 0.0115  (0.11901351642608642)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:18:02 -- STEP: 300/3984 -- GLOBAL_STEP: 300\n",
        "     | > loss_text_ce: 0.09940383583307266  (0.09938052833080291)\n",
        "     | > loss_mel_ce: 4.946959972381592  (nan)\n",
        "     | > loss: 0.1051325798034668  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1354  (0.1589464704195659)\n",
        "     | > loader_time: 0.5438  (0.14166177272796632)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:18:29 -- STEP: 350/3984 -- GLOBAL_STEP: 350\n",
        "     | > loss_text_ce: 0.09936606884002686  (0.09937650371875084)\n",
        "     | > loss_mel_ce: 5.004878044128418  (nan)\n",
        "     | > loss: 0.10633842647075653  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1961  (0.15912888663155692)\n",
        "     | > loader_time: 0.4374  (0.16278553826468345)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:18:51 -- STEP: 400/3984 -- GLOBAL_STEP: 400\n",
        "     | > loss_text_ce: 0.0991375595331192  (0.09937523253262044)\n",
        "     | > loss_mel_ce: 3.144663095474243  (nan)\n",
        "     | > loss: 0.06757918000221252  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.175  (0.15892186701297756)\n",
        "     | > loader_time: 0.5783  (0.1683490478992463)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:19:15 -- STEP: 450/3984 -- GLOBAL_STEP: 450\n",
        "     | > loss_text_ce: 0.09929818660020828  (0.09937064692378046)\n",
        "     | > loss_mel_ce: 4.862645626068115  (nan)\n",
        "     | > loss: 0.10337382555007935  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2494  (0.16169842455122202)\n",
        "     | > loader_time: 0.0061  (0.17523845407697897)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:19:40 -- STEP: 500/3984 -- GLOBAL_STEP: 500\n",
        "     | > loss_text_ce: 0.09924977272748947  (0.09936700002849104)\n",
        "     | > loss_mel_ce: 4.890869617462158  (nan)\n",
        "     | > loss: 0.10396082699298859  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1445  (0.16023208856582638)\n",
        "     | > loader_time: 0.0094  (0.1858855328559876)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:20:06 -- STEP: 550/3984 -- GLOBAL_STEP: 550\n",
        "     | > loss_text_ce: 0.09932360798120499  (0.09936148482290183)\n",
        "     | > loss_mel_ce: nan  (nan)\n",
        "     | > loss: nan  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2032  (0.16064514420249246)\n",
        "     | > loader_time: 0.0066  (0.19378360791639848)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:20:29 -- STEP: 600/3984 -- GLOBAL_STEP: 600\n",
        "     | > loss_text_ce: 0.09934771060943604  (0.09935668869564931)\n",
        "     | > loss_mel_ce: 4.994697093963623  (nan)\n",
        "     | > loss: 0.10612593591213226  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1254  (0.1609055916468303)\n",
        "     | > loader_time: 0.0056  (0.1958686021963756)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:20:50 -- STEP: 650/3984 -- GLOBAL_STEP: 650\n",
        "     | > loss_text_ce: 0.09927915036678314  (0.09935260821993534)\n",
        "     | > loss_mel_ce: 4.099350452423096  (nan)\n",
        "     | > loss: 0.08747144788503647  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.102  (0.16041925833775458)\n",
        "     | > loader_time: 0.0056  (0.19444965142470158)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:21:13 -- STEP: 700/3984 -- GLOBAL_STEP: 700\n",
        "     | > loss_text_ce: 0.09928202629089355  (0.09934935531445913)\n",
        "     | > loss_mel_ce: 5.430572986602783  (nan)\n",
        "     | > loss: 0.1152053102850914  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2143  (0.16049770627702994)\n",
        "     | > loader_time: 1.0631  (0.19604456629071937)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:21:35 -- STEP: 750/3984 -- GLOBAL_STEP: 750\n",
        "     | > loss_text_ce: 0.09934276342391968  (0.0993479255537192)\n",
        "     | > loss_mel_ce: 4.6960320472717285  (nan)\n",
        "     | > loss: 0.0999036431312561  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1152  (0.16187055238087977)\n",
        "     | > loader_time: 0.0105  (0.1946799637476605)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:21:59 -- STEP: 800/3984 -- GLOBAL_STEP: 800\n",
        "     | > loss_text_ce: 0.09938839077949524  (0.09934662770479917)\n",
        "     | > loss_mel_ce: 5.075809955596924  (nan)\n",
        "     | > loss: 0.10781663656234741  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2478  (0.1629818463325501)\n",
        "     | > loader_time: 0.352  (0.19665489494800587)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:22:21 -- STEP: 850/3984 -- GLOBAL_STEP: 850\n",
        "     | > loss_text_ce: 0.09924538433551788  (0.09934283975292654)\n",
        "     | > loss_mel_ce: 5.049042701721191  (nan)\n",
        "     | > loss: 0.10725601017475128  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2122  (0.1631670407687917)\n",
        "     | > loader_time: 0.006  (0.19645514460170987)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:22:42 -- STEP: 900/3984 -- GLOBAL_STEP: 900\n",
        "     | > loss_text_ce: 0.0993143618106842  (0.09933980262113942)\n",
        "     | > loss_mel_ce: 4.41800594329834  (nan)\n",
        "     | > loss: 0.09411083906888962  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1074  (0.16330833832422897)\n",
        "     | > loader_time: 0.6636  (0.19477371162838422)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:23:03 -- STEP: 950/3984 -- GLOBAL_STEP: 950\n",
        "     | > loss_text_ce: 0.09945549070835114  (0.09933572740146988)\n",
        "     | > loss_mel_ce: 4.069403171539307  (nan)\n",
        "     | > loss: 0.08685122430324554  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2057  (0.16301529809048307)\n",
        "     | > loader_time: 0.0087  (0.19448888803783232)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:23:26 -- STEP: 1000/3984 -- GLOBAL_STEP: 1000\n",
        "     | > loss_text_ce: 0.0992567166686058  (0.09933151377737522)\n",
        "     | > loss_mel_ce: 5.088573932647705  (nan)\n",
        "     | > loss: 0.10807980597019196  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.145  (0.16290694594383245)\n",
        "     | > loader_time: 0.0058  (0.19553615546226516)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:23:52 -- STEP: 1050/3984 -- GLOBAL_STEP: 1050\n",
        "     | > loss_text_ce: 0.09929149597883224  (0.09932853177899406)\n",
        "     | > loss_mel_ce: 3.373035430908203  (nan)\n",
        "     | > loss: 0.07234014570713043  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.102  (0.1629888452802386)\n",
        "     | > loader_time: 0.0065  (0.19963491916656506)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:24:12 -- STEP: 1100/3984 -- GLOBAL_STEP: 1100\n",
        "     | > loss_text_ce: 0.0992569625377655  (0.09932512729005381)\n",
        "     | > loss_mel_ce: 5.114090919494629  (nan)\n",
        "     | > loss: 0.10861141979694366  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.279  (0.16413203196092085)\n",
        "     | > loader_time: 0.447  (0.19629223563454376)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:24:31 -- STEP: 1150/3984 -- GLOBAL_STEP: 1150\n",
        "     | > loss_text_ce: 0.09925679117441177  (0.09932210992859761)\n",
        "     | > loss_mel_ce: 4.760846138000488  (nan)\n",
        "     | > loss: 0.10125215351581573  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2052  (0.164254375955333)\n",
        "     | > loader_time: 0.0636  (0.1937816545237666)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:24:54 -- STEP: 1200/3984 -- GLOBAL_STEP: 1200\n",
        "     | > loss_text_ce: 0.09885226935148239  (0.09931814010565487)\n",
        "     | > loss_mel_ce: 5.438046455383301  (nan)\n",
        "     | > loss: 0.11535205692052841  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1098  (0.16417876005172746)\n",
        "     | > loader_time: 0.0055  (0.19487470130125686)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:25:16 -- STEP: 1250/3984 -- GLOBAL_STEP: 1250\n",
        "     | > loss_text_ce: 0.09906523674726486  (0.09931496726870545)\n",
        "     | > loss_mel_ce: 5.379488468170166  (nan)\n",
        "     | > loss: 0.11413653939962387  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1064  (0.1647187438964844)\n",
        "     | > loader_time: 0.0094  (0.1936907978057862)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:25:35 -- STEP: 1300/3984 -- GLOBAL_STEP: 1300\n",
        "     | > loss_text_ce: 0.09925433248281479  (0.09931252147715834)\n",
        "     | > loss_mel_ce: 4.310297966003418  (nan)\n",
        "     | > loss: 0.091865673661232  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1043  (0.1648214312700125)\n",
        "     | > loader_time: 0.0077  (0.19157507584645203)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:25:57 -- STEP: 1350/3984 -- GLOBAL_STEP: 1350\n",
        "     | > loss_text_ce: 0.09925287216901779  (0.09930909138586795)\n",
        "     | > loss_mel_ce: 4.419900417327881  (nan)\n",
        "     | > loss: 0.09414902329444885  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.126  (0.1650100881082041)\n",
        "     | > loader_time: 0.0076  (0.1913108412424724)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:26:16 -- STEP: 1400/3984 -- GLOBAL_STEP: 1400\n",
        "     | > loss_text_ce: 0.09921146184206009  (0.09930578429784102)\n",
        "     | > loss_mel_ce: 5.3632636070251465  (nan)\n",
        "     | > loss: 0.11380156874656677  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2113  (0.16516511559486394)\n",
        "     | > loader_time: 0.1599  (0.18995167255401618)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:26:37 -- STEP: 1450/3984 -- GLOBAL_STEP: 1450\n",
        "     | > loss_text_ce: 0.09918704628944397  (0.09930209123882762)\n",
        "     | > loss_mel_ce: 5.265410900115967  (nan)\n",
        "     | > loss: 0.11176245659589767  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2146  (0.16576877610436805)\n",
        "     | > loader_time: 0.0081  (0.18876476879777587)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:26:55 -- STEP: 1500/3984 -- GLOBAL_STEP: 1500\n",
        "     | > loss_text_ce: 0.09904791414737701  (0.09929870647192009)\n",
        "     | > loss_mel_ce: 4.15358829498291  (nan)\n",
        "     | > loss: 0.08859659731388092  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2475  (0.16635495408376053)\n",
        "     | > loader_time: 0.0493  (0.18529991420110073)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:27:19 -- STEP: 1550/3984 -- GLOBAL_STEP: 1550\n",
        "     | > loss_text_ce: 0.09911007434129715  (0.09929572561575528)\n",
        "     | > loss_mel_ce: 4.9388041496276855  (nan)\n",
        "     | > loss: 0.10495655238628387  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2169  (0.16639484097880694)\n",
        "     | > loader_time: 0.1706  (0.18663279994841553)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:27:40 -- STEP: 1600/3984 -- GLOBAL_STEP: 1600\n",
        "     | > loss_text_ce: 0.09810701757669449  (0.09929270564578481)\n",
        "     | > loss_mel_ce: 3.950199842453003  (nan)\n",
        "     | > loss: 0.08433973044157028  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.0924  (0.16641944229602806)\n",
        "     | > loader_time: 0.0107  (0.1864498174190522)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:27:58 -- STEP: 1650/3984 -- GLOBAL_STEP: 1650\n",
        "     | > loss_text_ce: 0.09910426288843155  (0.09928868659969539)\n",
        "     | > loss_mel_ce: 4.9734625816345215  (nan)\n",
        "     | > loss: 0.1056784838438034  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1686  (0.1663196997209028)\n",
        "     | > loader_time: 0.0068  (0.18411164717240774)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:28:16 -- STEP: 1700/3984 -- GLOBAL_STEP: 1700\n",
        "     | > loss_text_ce: 0.09913639724254608  (0.09928555809837937)\n",
        "     | > loss_mel_ce: 5.142073154449463  (nan)\n",
        "     | > loss: 0.10919186472892761  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2019  (0.16651707747403308)\n",
        "     | > loader_time: 0.0057  (0.18191664162804105)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:28:33 -- STEP: 1750/3984 -- GLOBAL_STEP: 1750\n",
        "     | > loss_text_ce: 0.0995119959115982  (0.09928312161139086)\n",
        "     | > loss_mel_ce: 3.8471474647521973  (nan)\n",
        "     | > loss: 0.08222207427024841  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1022  (0.16663083771296902)\n",
        "     | > loader_time: 0.0076  (0.17942268303462444)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:28:51 -- STEP: 1800/3984 -- GLOBAL_STEP: 1800\n",
        "     | > loss_text_ce: 0.09920361638069153  (0.09928011961281306)\n",
        "     | > loss_mel_ce: 4.469452857971191  (nan)\n",
        "     | > loss: 0.09518034756183624  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1817  (0.16710487418704556)\n",
        "     | > loader_time: 0.0095  (0.1772287559509278)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:29:09 -- STEP: 1850/3984 -- GLOBAL_STEP: 1850\n",
        "     | > loss_text_ce: 0.09947633743286133  (0.09927763602620852)\n",
        "     | > loss_mel_ce: 4.063547134399414  (nan)\n",
        "     | > loss: 0.08672966063022614  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.0979  (0.1667776948052483)\n",
        "     | > loader_time: 0.007  (0.17560865363559214)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:29:27 -- STEP: 1900/3984 -- GLOBAL_STEP: 1900\n",
        "     | > loss_text_ce: 0.09901562333106995  (0.0992737999401595)\n",
        "     | > loss_mel_ce: 4.578951835632324  (nan)\n",
        "     | > loss: 0.09745766222476959  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.218  (0.16648219384645155)\n",
        "     | > loader_time: 0.4737  (0.17457472663176693)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:29:43 -- STEP: 1950/3984 -- GLOBAL_STEP: 1950\n",
        "     | > loss_text_ce: 0.09910154342651367  (0.09927026824691362)\n",
        "     | > loss_mel_ce: 3.8459911346435547  (nan)\n",
        "     | > loss: 0.08218943327665329  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1278  (0.16680658095922218)\n",
        "     | > loader_time: 0.1  (0.17140764089731075)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:30:01 -- STEP: 2000/3984 -- GLOBAL_STEP: 2000\n",
        "     | > loss_text_ce: 0.09903059154748917  (0.09926672193780547)\n",
        "     | > loss_mel_ce: 4.320775508880615  (nan)\n",
        "     | > loss: 0.092079296708107  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1075  (0.16698893833160394)\n",
        "     | > loader_time: 0.007  (0.16954512476921088)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:30:19 -- STEP: 2050/3984 -- GLOBAL_STEP: 2050\n",
        "     | > loss_text_ce: 0.09922541677951813  (0.09926187506536163)\n",
        "     | > loss_mel_ce: 5.077208518981934  (nan)\n",
        "     | > loss: 0.10784237831830978  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.2421  (0.16700738255570569)\n",
        "     | > loader_time: 0.021  (0.16851480739872635)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:30:39 -- STEP: 2100/3984 -- GLOBAL_STEP: 2100\n",
        "     | > loss_text_ce: 0.099253810942173  (0.09925926876210038)\n",
        "     | > loss_mel_ce: 4.917597770690918  (nan)\n",
        "     | > loss: 0.10451774299144745  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1345  (0.16725592658633273)\n",
        "     | > loader_time: 0.3623  (0.16754365080878852)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:30:54 -- STEP: 2150/3984 -- GLOBAL_STEP: 2150\n",
        "     | > loss_text_ce: 0.09915103763341904  (0.09925505162671563)\n",
        "     | > loss_mel_ce: 4.742849349975586  (nan)\n",
        "     | > loss: 0.10087501257658005  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.248  (0.16705307539119266)\n",
        "     | > loader_time: 0.0082  (0.16513582052186485)\n",
        "\n",
        "\n",
        "   --> TIME: 2025-08-12 14:31:09 -- STEP: 2200/3984 -- GLOBAL_STEP: 2200\n",
        "     | > loss_text_ce: 0.0994110107421875  (0.09924949416382751)\n",
        "     | > loss_mel_ce: 3.498537063598633  (nan)\n",
        "     | > loss: 0.07495725154876709  (nan)\n",
        "     | > current_lr: 5e-06\n",
        "     | > step_time: 0.1215  (0.16682647553357197)\n",
        "     | > loader_time: 0.0176  (0.1631722355972637)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4A-u_YIQsDA",
      "metadata": {
        "id": "c4A-u_YIQsDA"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RWDQzkXxQtEK",
      "metadata": {
        "id": "RWDQzkXxQtEK"
      },
      "outputs": [],
      "source": [
        "output_path = \"/content/drive/MyDrive/XTTS_Maltese_Training/output\"\n",
        "xtts_checkpoint = os.path.join(output_path, \"xtts.pth\")\n",
        "xtts_config = os.path.join(output_path, \"config.json\")\n",
        "xtts_vocab = os.path.join(output_path, \"vocab.json\")\n",
        "\n",
        "tts_text = \"Il-kelma Maltija 'bonġu' tfisser 'good morning'.\"\n",
        "speaker_audio_file = \"/content/drive/MyDrive/XTTS_Maltese_Data/wavs/MSRHS_F_02_P03U015_0046.wav\"\n",
        "lang = \"mt\"\n",
        "output_file = \"output_maltese.wav\"\n",
        "\n",
        "!python inference.py \\\n",
        "    --xtts_checkpoint=\"{xtts_checkpoint}\" \\\n",
        "    --xtts_config=\"{xtts_config}\" \\\n",
        "    --xtts_vocab=\"{xtts_vocab}\" \\\n",
        "    --tts_text=\"{tts_text}\" \\\n",
        "    --speaker_audio_file=\"{speaker_audio_file}\" \\\n",
        "    --lang=\"{lang}\" \\\n",
        "    --output_file=\"{output_file}\"\n",
        "\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saozmoyuMCK7",
      "metadata": {
        "id": "saozmoyuMCK7"
      },
      "source": [
        "inference parameters\n",
        "text: The text to be synthesized.\n",
        "\n",
        "language: The language of the text to be synthesized.\n",
        "\n",
        "gpt_cond_latent: The latent vector you get with get_conditioning_latents. (You can cache for faster inference with same speaker)\n",
        "\n",
        "speaker_embedding: The speaker embedding you get with get_conditioning_latents. (You can cache for faster inference with same speaker)\n",
        "\n",
        "temperature: The softmax temperature of the autoregressive model. Defaults to 0.65.\n",
        "\n",
        "length_penalty: A length penalty applied to the autoregressive decoder. Higher settings causes the model to produce more terse outputs. Defaults to 1.0.\n",
        "\n",
        "repetition_penalty: A penalty that prevents the autoregressive decoder from repeating itself during decoding. Can be used to reduce the incidence of long silences or “uhhhhhhs”, etc. Defaults to 2.0.\n",
        "\n",
        "top_k: Lower values mean the decoder produces more “likely” (aka boring) outputs. Defaults to 50.\n",
        "\n",
        "top_p: Lower values mean the decoder produces more “likely” (aka boring) outputs. Defaults to 0.8.\n",
        "\n",
        "speed: The speed rate of the generated audio. Defaults to 1.0. (can produce artifacts if far from 1.0)\n",
        "\n",
        "enable_text_splitting: Whether to split the text into sentences and generate audio for each sentence. It allows you to have infinite input length but might loose important context between sentences. Defaults to True."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "29a1c709",
        "697307d4",
        "c4A-u_YIQsDA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "xtts_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
