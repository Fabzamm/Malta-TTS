{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "870f086e",
      "metadata": {
        "id": "870f086e"
      },
      "source": [
        "# XTTS Finetuning for Maltese"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l74DKA3BPNSu",
      "metadata": {
        "id": "l74DKA3BPNSu"
      },
      "source": [
        "**Author: [Mathieu Waharte](mailto:mathieu.waharte@universite-paris-saclay.fr)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a1c709",
      "metadata": {
        "id": "29a1c709"
      },
      "source": [
        "## Requirements\n",
        "- Python <3.11 for originial coqui's TTS and >=3.11 for TTS port\n",
        "- HF_TOKEN\n",
        "- Around 4GB for original files + 5GB per checkpoint\n",
        "\n",
        "Installation takes around 6min (you may need to restart the session at the end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a3e30b65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a3e30b65",
        "outputId": "9693ffdb-833a-470c-d02a-e57cf18c0f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "Cloning into 'Malta-TTS'...\n",
            "remote: Enumerating objects: 1026, done.\u001b[K\n",
            "remote: Counting objects: 100% (291/291), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 1026 (delta 197), reused 192 (delta 100), pack-reused 735 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1026/1026), 9.66 MiB | 10.57 MiB/s, done.\n",
            "Resolving deltas: 100% (636/636), done.\n",
            "/content/Malta-TTS/FineTuning/NewLanguage\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Ignoring TTS: markers 'python_version < \"3.11\"' don't match your environment\n",
            "Ignoring trainer: markers 'python_version < \"3.11\"' don't match your environment\n",
            "Ignoring coqpit: markers 'python_version < \"3.11\"' don't match your environment\n",
            "Ignoring transformers: markers 'python_version < \"3.11\"' don't match your environment\n",
            "Collecting torch==2.5.1 (from -r requirements.txt (line 2))\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchaudio==2.5.1 (from -r requirements.txt (line 3))\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.20.1 (from -r requirements.txt (line 4))\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting coqui-tts==0.27.0 (from -r requirements.txt (line 10))\n",
            "  Downloading coqui_tts-0.27.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting coqui-tts-trainer==0.3.1 (from -r requirements.txt (line 11))\n",
            "  Downloading coqui_tts_trainer-0.3.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit-config==0.2.1 (from -r requirements.txt (line 12))\n",
            "  Downloading coqpit_config-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting unidecode>=1.3.2 (from -r requirements.txt (line 15))\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words>=0.5.12 (from -r requirements.txt (line 16))\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (3.8.7)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.21.4)\n",
            "Collecting transformers==4.52.1 (from -r requirements.txt (line 20))\n",
            "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (1.16.1)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (3.9.1)\n",
            "Requirement already satisfied: datasets>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 30)) (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (0.34.4)\n",
            "Collecting dataclasses (from -r requirements.txt (line 32))\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (0.17.1)\n",
            "Collecting stanza (from -r requirements.txt (line 38))\n",
            "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting spacy-stanza (from -r requirements.txt (line 39))\n",
            "  Downloading spacy_stanza-1.0.4-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting deepspeed (from -r requirements.txt (line 44))\n",
            "  Downloading deepspeed-0.17.5.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 47)) (2.19.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 50)) (7.34.0)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 53)) (1.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1->-r requirements.txt (line 2)) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->-r requirements.txt (line 2))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1->-r requirements.txt (line 4)) (11.3.0)\n",
            "Collecting anyascii>=0.3.0 (from coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: cython>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts==0.27.0->-r requirements.txt (line 10)) (3.0.12)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts==0.27.0->-r requirements.txt (line 10)) (0.8.1)\n",
            "Collecting encodec>=0.1.1 (from coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut>=2.4.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut-2.4.0.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts==0.27.0->-r requirements.txt (line 10)) (7.5.0)\n",
            "Requirement already satisfied: matplotlib>=3.8.4 in /usr/local/lib/python3.12/dist-packages (from coqui-tts==0.27.0->-r requirements.txt (line 10)) (3.10.0)\n",
            "Collecting monotonic-alignment-search>=0.1.0 (from coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading monotonic_alignment_search-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from coqui-tts==0.27.0->-r requirements.txt (line 10)) (25.0)\n",
            "Collecting pysbd>=0.3.4 (from coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from coqui-tts-trainer==0.3.1->-r requirements.txt (line 11)) (5.9.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.1->-r requirements.txt (line 20)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.1->-r requirements.txt (line 20)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.1->-r requirements.txt (line 20)) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.12->-r requirements.txt (line 16))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (0.16.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (2.11.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.0.0->-r requirements.txt (line 17)) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->-r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.1->-r requirements.txt (line 20)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.1->-r requirements.txt (line 20)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.1->-r requirements.txt (line 20)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.1->-r requirements.txt (line 20)) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.0->-r requirements.txt (line 17)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 17)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 17)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 17)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 17)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 17)) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.0->-r requirements.txt (line 17)) (1.17.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.0->-r requirements.txt (line 25)) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 26)) (1.1.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 30)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 30)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 30)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 30)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 30)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (3.12.15)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft->-r requirements.txt (line 33)) (1.10.1)\n",
            "Collecting emoji (from stanza->-r requirements.txt (line 38))\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza->-r requirements.txt (line 38)) (5.29.5)\n",
            "Collecting stanza (from -r requirements.txt (line 38))\n",
            "  Downloading stanza-1.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hjson (from deepspeed->-r requirements.txt (line 44))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed->-r requirements.txt (line 44))\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 44)) (9.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (3.8.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 47)) (3.1.3)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 50))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->-r requirements.txt (line 50)) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 50)) (0.2.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.0->-r requirements.txt (line 25)) (2.22)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (2.17.0)\n",
            "Collecting dateparser~=1.1.1 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from dateparser~=1.1.1->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from dateparser~=1.1.1->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from dateparser~=1.1.1->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (5.3.1)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts==0.27.0->-r requirements.txt (line 10))\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.12/dist-packages (from inflect>=5.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from inflect>=5.6.0->coqui-tts==0.27.0->-r requirements.txt (line 10)) (4.4.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 50)) (0.8.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->-r requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts==0.27.0->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts==0.27.0->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts==0.27.0->-r requirements.txt (line 10)) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts==0.27.0->-r requirements.txt (line 10)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts==0.27.0->-r requirements.txt (line 10)) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa>=0.10.0->-r requirements.txt (line 26)) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 50)) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.0->-r requirements.txt (line 26)) (4.3.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 17)) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->-r requirements.txt (line 17)) (0.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.0->-r requirements.txt (line 26)) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 47)) (3.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.12.0->-r requirements.txt (line 30)) (2025.2)\n",
            "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqui_tts-0.27.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.5/857.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqui_tts_trainer-0.3.1-py3-none-any.whl (57 kB)\n",
            "Downloading coqpit_config-0.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading transformers-4.52.1-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading spacy_stanza-1.0.4-py3-none-any.whl (9.7 kB)\n",
            "Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic_alignment_search-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (606 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.8/606.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Building wheels for collected packages: deepspeed, docopt, encodec, gruut, gruut-ipa, gruut_lang_en, gruut_lang_de, gruut_lang_es, gruut_lang_fr\n",
            "\u001b[33m  DEPRECATION: Building 'deepspeed' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'deepspeed'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.17.5-py3-none-any.whl size=1726357 sha256=b1b387b17a9eac7c6fc7e54594258baf98a4682db70e96fd7f376769dbc7caf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/3a/01/079b5f719e6fd280d79f210f54f5e5c6cbc7b956c9905ea274\n",
            "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e8a232535601c1da34fd075d3fcea50f27f8b3426e015ffa1325e339dea087df\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "\u001b[33m  DEPRECATION: Building 'encodec' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'encodec'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=7e81fe4959ea6858d65c00385eaf3e1125dcf00a56bb407b7ce01275cd23899e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/eb/9f/e13610cc46ab39d3199fbabebd1c3e142d44b679526e0f228a\n",
            "\u001b[33m  DEPRECATION: Building 'gruut' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.4.0-py3-none-any.whl size=86758 sha256=eb19df1064f02d6f77eee8dbf0e5715fd9f617bb9797206a1262a9b4ed588e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/93/9c/fbd0d8778ac586a48a20e73d2a64d882984d9501fd5e8daf24\n",
            "\u001b[33m  DEPRECATION: Building 'gruut-ipa' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut-ipa'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=63da4665200d4980eb4fc2708cc054a4c6d6559535907fa3a977ff7ca2d4124e\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/9d/d2/d6f6eb77784f063fcd497427fd93324cebf974247984bba85b\n",
            "\u001b[33m  DEPRECATION: Building 'gruut_lang_en' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut_lang_en'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326858 sha256=8bd413456e1a242640cfb2fc872457a96bdbeac7a73f7e2cdfd99d6ba06483a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/ca/a4/d1a6f20e47b857313689ca1f31684102ba67cecda2acae368d\n",
            "\u001b[33m  DEPRECATION: Building 'gruut_lang_de' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut_lang_de'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498314 sha256=89ed54a9900ff518423eefd665d1c7d020b9993e30b4bfafc12e26cab2098978\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/75/26/e627d52dac0253ad7d11e5b9f74d51d82e040d07432f53ad9b\n",
            "\u001b[33m  DEPRECATION: Building 'gruut_lang_es' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut_lang_es'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173927 sha256=e5375bc9785ca8df1bdda2fb342e6caf22b5e1d47fc579fbf6762e3985a49ffc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/19/79/0a65f77c4921ae0daa8d01e5b11502a909b55bd22fa188962d\n",
            "\u001b[33m  DEPRECATION: Building 'gruut_lang_fr' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gruut_lang_fr'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=78a80efe62ff41b4e485fcb71ece48d4e798f52174a1a1c4596e234f194c343e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/91/46/0ab326f9e46bc2cc2fe2f35b0e0e6f3b8284d78efd25192d96\n",
            "Successfully built deepspeed docopt encodec gruut gruut-ipa gruut_lang_en gruut_lang_de gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: hjson, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, dataclasses, unidecode, triton, sympy, python-crfsuite, pysbd, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, ninja, monotonic-alignment-search, jsonlines, jedi, gruut-ipa, emoji, coqpit-config, anyascii, nvidia-cusparse-cu12, nvidia-cudnn-cu12, dateparser, nvidia-cusolver-cu12, gruut, transformers, torch, torchvision, torchaudio, stanza, deepspeed, coqui-tts-trainer, spacy-stanza, encodec, coqui-tts\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.4.0\n",
            "\u001b[2K    Uninstalling triton-3.4.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.4.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.13.3\n",
            "\u001b[2K    Uninstalling sympy-1.13.3:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.13.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.55.4\n",
            "\u001b[2K    Uninstalling transformers-4.55.4:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.55.4\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torch-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.23.0+cu126\n",
            "\u001b[2K    Uninstalling torchvision-0.23.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torchaudio-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/45\u001b[0m [coqui-tts]\n",
            "\u001b[1A\u001b[2KSuccessfully installed anyascii-0.3.3 coqpit-config-0.2.1 coqui-tts-0.27.0 coqui-tts-trainer-0.3.1 dataclasses-0.6 dateparser-1.1.8 deepspeed-0.17.5 docopt-0.6.2 emoji-2.14.1 encodec-0.1.1 gruut-2.4.0 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hjson-3.1.0 jedi-0.19.2 jsonlines-1.2.0 monotonic-alignment-search-0.2.0 ninja-1.13.0 num2words-0.5.14 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pysbd-0.3.4 python-crfsuite-0.9.11 spacy-stanza-1.0.4 stanza-1.6.1 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 transformers-4.52.1 triton-3.1.0 unidecode-1.4.0\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 72.7MB/s]        \n",
            "2025-09-02 15:58:07 INFO: Downloading default packages for language: mt (Maltese) ...\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-mt/resolve/v1.6.0/models/default.zip: 100% 142M/142M [00:02<00:00, 53.7MB/s]\n",
            "2025-09-02 15:58:12 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/\n",
        "%rm -rf Malta-TTS\n",
        "!git clone https://github.com/Wubpooz/Malta-TTS.git\n",
        "\n",
        "%cd Malta-TTS/FineTuning/NewLanguage\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "!pip install tf-keras tensorflow-decision-forests tensorflow-text --upgrade\n",
        "\n",
        "!python -c \"import stanza; stanza.download('mt')\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697307d4",
      "metadata": {
        "id": "697307d4"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "Tc8HkCwGeLNT",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Tc8HkCwGeLNT",
        "outputId": "eb2b2f4f-62ad-41a0-9b66-c68e8c52ba52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Malta-TTS/FineTuning/NewLanguage\n",
            "Loading dataset from Hugging Face...\n",
            "Resampling to 22050Hz and saving...\n",
            "Processing split: train (workers=16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Resampling train: 100%|██████████| 3983/3983 [00:00<00:00, 4346.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 3983 entries to metadata_train.csv\n",
            "Processing split: test (workers=16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Resampling test: 100%|██████████| 996/996 [00:00<00:00, 4185.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 996 entries to metadata_eval.csv\n",
            "Dataset saved!\n"
          ]
        }
      ],
      "source": [
        "# @title Save Metadata and Resample audio (takes 2:30min with 16 workers)\n",
        "\n",
        "%cd /content/Malta-TTS/FineTuning/NewLanguage/\n",
        "\n",
        "from prepare_maltese_dataset import load_and_resample\n",
        "\n",
        "load_and_resample(output_dir='/content/drive/MyDrive/XTTS_Maltese_Data', dataset='Bluefir/MASRI_HEADSET_v2', sampling_rate=22050, num_workers=16, save_audio=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "A1JiBiiFYWC2",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "A1JiBiiFYWC2",
        "outputId": "83d2e83e-2f59-457a-e1c8-ddd55b412585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing split: train\n",
            "Processing split: test\n",
            "Text length range: 1 - 188 characters\n",
            "Audio duration range: 0.62 - 10.89 seconds\n",
            "Average text length: 65.99 characters\n",
            "Average audio duration: 4.82 seconds\n"
          ]
        }
      ],
      "source": [
        "# @title Dataset repartition\n",
        "from prepare_maltese_dataset import dataset_repartition\n",
        "\n",
        "dataset_repartition('Bluefir/MASRI_HEADSET_v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e498624b",
      "metadata": {
        "id": "e498624b"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "H-sP_jEF-5fm",
      "metadata": {
        "cellView": "form",
        "id": "H-sP_jEF-5fm"
      },
      "outputs": [],
      "source": [
        "# @title Output Redirection\n",
        "\n",
        "import os\n",
        "import sys\n",
        "_original_stdout = sys.stdout\n",
        "_original_stderr = sys.stderr\n",
        "_log_file = None\n",
        "\n",
        "def output_redirect(output_path: str, redirect: bool = True):\n",
        "  global _log_file\n",
        "  if not redirect:\n",
        "    sys.stdout = _original_stdout\n",
        "    sys.stderr = _original_stderr\n",
        "    if _log_file:\n",
        "      _log_file.close()\n",
        "      _log_file = None\n",
        "  else:\n",
        "    log_path = os.path.join(output_path, \"full_training.log\")\n",
        "    # Clear the log file by opening in write mode and closing immediately\n",
        "    if os.path.exists(log_path):\n",
        "      with open(log_path, \"w\"):\n",
        "        pass\n",
        "\n",
        "    _log_file = open(log_path, \"a\", buffering=1)  # line-buffered\n",
        "\n",
        "    class Tee(object):\n",
        "      def __init__(self, *streams):\n",
        "        self.streams = streams\n",
        "      def write(self, data):\n",
        "        for s in self.streams:\n",
        "          s.write(data)\n",
        "          s.flush()\n",
        "      def flush(self):\n",
        "        for s in self.streams:\n",
        "          s.flush()\n",
        "\n",
        "    sys.stdout = Tee(_original_stdout, _log_file)\n",
        "    sys.stderr = Tee(_original_stderr, _log_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f64e3b84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "f64e3b84",
        "outputId": "ed92d0a5-8af3-4ae7-cb24-c05cf62c2094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: OUTPUT_PATH=/content/drive/MyDrive/XTTS_Maltese_Training\n",
            "env: META_TRAIN=/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv\n",
            "env: META_EVAL=/content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv\n",
            "env: LANG=mt\n",
            "env: TOKENIZERS_PARALLELISM=false\n",
            "env: OMP_NUM_THREADS=1\n",
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "Finetuning for mt...\n",
            "/content/Malta-TTS/FineTuning/NewLanguage\n",
            "2025-09-02 08:55:31.707960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756803331.736909   43984 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756803331.746040   43984 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756803331.791442   43984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756803331.791472   43984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756803331.791476   43984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756803331.791481   43984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-02 08:55:31.796412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Step 1: Downloading XTTS base model files.\n",
            " > Downloading XTTS v-main files...\n",
            " > XTTS model files downloaded successfully!\n",
            "Step 2: Extending the XTTS tokenizer with the new language.\n",
            "==== Extending Tokenizer ====\n",
            "Applying Maltese-specific text preprocessing...\n",
            "Creating text chunks...\n",
            "Training BPE...\n",
            "\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 10085    /    10085\n",
            "\u001b[2K[00:00:00] Count pairs                    ██████████████████ 10085    /    10085\n",
            "\u001b[2K[00:00:00] Compute merges                 ██████████████████ 4963     /     4963\n",
            "Filtering BPE tokens for conflicts...\n",
            "Performing conflict detection...\n",
            "Adding missing character tokens...\n",
            "Finalizing new BPE tokens...\n",
            "No safe BPE tokens found to add.\n",
            "Saving extended tokenizer...\n",
            "Extended tokenizer saved to /content/drive/MyDrive/XTTS_Maltese_Training/vocab.json.\n",
            "Updating model embeddings and configuration...\n",
            "Resizing checkpoint embeddings: /content/drive/MyDrive/XTTS_Maltese_Training/model.pth\n",
            "/content/Malta-TTS/FineTuning/NewLanguage/tokenizer_extension.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(xtts_checkpoint_path, map_location=\"cpu\")\n",
            "Current vocab size: 10537, New vocab size: 10537\n",
            "Vocabulary sizes match, no resizing needed.\n",
            "Updated config file saved to /content/drive/MyDrive/XTTS_Maltese_Training/config.json. Added new language: mt. Vocab size: 10537\n",
            "=== TOKENIZER EXTENSION COMPLETE ===\n",
            "Added 0 new tokens total.\n",
            "Final vocabulary size: 10537\n",
            "\n",
            "=== RUNNING CORRUPTION VALIDATION ===\n",
            "=== TOKENIZER CORRUPTION DEBUG ===\n",
            "\n",
            "Loading tokenizers...\n",
            "Original vocab size: 10537\n",
            "Extended vocab size: 10537\n",
            "\n",
            "=== TOKEN ID COMPARISON ===\n",
            "✅ OK: 'hello' kept ID NOT_FOUND\n",
            "✅ OK: 'world' kept ID NOT_FOUND\n",
            "✅ OK: 'the' kept ID 42\n",
            "✅ OK: 'and' kept ID 53\n",
            "✅ OK: 'is' kept ID 54\n",
            "✅ OK: 'that' kept ID 73\n",
            "✅ OK: 'you' kept ID 74\n",
            "✅ OK: 'for' kept ID 87\n",
            "✅ OK: 'magic' kept ID NOT_FOUND\n",
            "✅ OK: 'doctor' kept ID NOT_FOUND\n",
            "✅ OK: 'hi' kept ID 1173\n",
            "✅ OK: 'truth' kept ID NOT_FOUND\n",
            "✅ OK: 'a' kept ID 14\n",
            "✅ OK: 'two' kept ID 2420\n",
            "✅ OK: 'rt' kept ID 5225\n",
            "\n",
            "=== TOKENIZATION COMPARISON ===\n",
            "Original tokenization: [5767, 18, 84, 28, 179, 79, 7, 21, 69, 59, 18, 74, 51, 17, 88, 13]\n",
            "Extended tokenization:  [5767, 18, 84, 28, 179, 79, 7, 21, 69, 59, 18, 74, 51, 17, 88, 13]\n",
            "✅ OK: Same tokenization preserved\n",
            "\n",
            "=== TOKEN ID SHIFT ANALYSIS ===\n",
            "Token ID shifts detected: 0/50 (0.0%)\n",
            "\n",
            "=== DIAGNOSIS ===\n",
            "✅ Tokenizer appears intact - issue might be elsewhere\n",
            "✅ Validation passed - tokenizer extension successful!\n",
            "Extended vocabulary size: 10537\n",
            "Step 3: Starting GPT training.\n",
            "Using updated checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/model.pth\n",
            "Using updated tokenizer: /content/drive/MyDrive/XTTS_Maltese_Training/vocab.json\n",
            "Using vocab size: 10537\n",
            " > Training XTTS model for Maltese with 1 datasets, 1 epochs, batch size 3, grad_acumm 48, output path: /content/drive/MyDrive/XTTS_Maltese_Training/training\n",
            " > Using the following datasets:\n",
            "/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv /content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv mt\n",
            "Setting up model arguments...\n",
            "Loading datasets...\n",
            " > Loaded 3984 training samples and 997 evaluation samples.\n",
            "Added char_limits for 'mt' language.\n",
            "mt added to tokenizer.py!\n",
            " > Training Environment:\n",
            " | > Backend: Torch\n",
            " | > Mixed precision: False\n",
            " | > Precision: float32\n",
            " | > Current device: 0\n",
            " | > Num. of GPUs: 1\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 1\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            " | > Torch TF32 MatMul: False\n",
            " > Start Tensorboard: tensorboard --logdir=/content/drive/MyDrive/XTTS_Maltese_Training/training/GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78\n",
            "\n",
            " > Model has 526342991 parameters\n",
            "Starting training...\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/0\u001b[0m\n",
            " --> /content/drive/MyDrive/XTTS_Maltese_Training/training/GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78\n",
            "\n",
            "\u001b[1m > TRAINING (2025-09-02 08:57:51) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 08:57:55 -- STEP: 0/1327 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > current_lr: 5e-06  (5e-06)\n",
            "     | > loss_text_ce: 0.20810700953006744  (0.20810700953006744)\n",
            "     | > loss_mel_ce: 4.997337341308594  (4.997337341308594)\n",
            "     | > loss: 0.10844676196575165  (0.10844676196575165)\n",
            "     | > step_time: 1.5015  (1.50146484375)\n",
            "     | > loader_time: 2.4667  (2.4666619300842285)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 09:06:25 -- STEP: 200/1327 -- GLOBAL_STEP: 200\u001b[0m\n",
            "     | > current_lr: 5e-06  (5.00000000000001e-06)\n",
            "     | > loss_text_ce: 0.2123258113861084  (0.21143652275204658)\n",
            "     | > loss_mel_ce: 4.739647388458252  (4.909989533424375)\n",
            "     | > loss: 0.10316610336303711  (0.10669637881219388)\n",
            "     | > step_time: 0.3574  (0.4753853929042816)\n",
            "     | > loader_time: 1.1773  (1.8421905314922333)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 09:28:56 -- STEP: 400/1327 -- GLOBAL_STEP: 400\u001b[0m\n",
            "     | > current_lr: 5e-06  (5.0000000000000055e-06)\n",
            "     | > loss_text_ce: 0.21927011013031006  (0.20928287550806993)\n",
            "     | > loss_mel_ce: 4.614048957824707  (4.783107336759567)\n",
            "     | > loss: 0.10069414973258972  (0.10400813203305004)\n",
            "     | > step_time: 0.3437  (0.4793089139461517)\n",
            "     | > loader_time: 0.0205  (3.9385781949758543)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 09:45:12 -- STEP: 600/1327 -- GLOBAL_STEP: 600\u001b[0m\n",
            "     | > current_lr: 5e-06  (4.9999999999999335e-06)\n",
            "     | > loss_text_ce: 0.1978997439146042  (0.20735730181137715)\n",
            "     | > loss_mel_ce: 4.306251049041748  (4.694538737932842)\n",
            "     | > loss: 0.0938364788889885  (0.10212283664693429)\n",
            "     | > step_time: 0.4927  (0.4772432279586792)\n",
            "     | > loader_time: 1.2071  (4.01554821054141)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 09:52:07 -- STEP: 800/1327 -- GLOBAL_STEP: 800\u001b[0m\n",
            "     | > current_lr: 5e-06  (4.999999999999904e-06)\n",
            "     | > loss_text_ce: 0.27155160903930664  (0.20517536567524072)\n",
            "     | > loss_mel_ce: 4.003966808319092  (4.6315985098481205)\n",
            "     | > loss: 0.0890733003616333  (0.10076612481847402)\n",
            "     | > step_time: 0.2491  (0.47398056626319884)\n",
            "     | > loader_time: 0.4179  (3.353411270082)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 09:57:43 -- STEP: 1000/1327 -- GLOBAL_STEP: 1000\u001b[0m\n",
            "     | > current_lr: 5e-06  (4.999999999999882e-06)\n",
            "     | > loss_text_ce: 0.18062083423137665  (0.20279979719221589)\n",
            "     | > loss_mel_ce: 4.3258185386657715  (4.583846369981771)\n",
            "     | > loss: 0.09388415515422821  (0.09972179757803676)\n",
            "     | > step_time: 0.4317  (0.47093053221702574)\n",
            "     | > loader_time: 0.0108  (2.879388844728472)\n",
            "\n",
            "\n",
            "\u001b[1m   --> TIME: 2025-09-02 10:01:48 -- STEP: 1200/1327 -- GLOBAL_STEP: 1200\u001b[0m\n",
            "     | > current_lr: 5e-06  (4.999999999999865e-06)\n",
            "     | > loss_text_ce: 0.16571731865406036  (0.20020455444852503)\n",
            "     | > loss_mel_ce: 4.659317970275879  (4.546544670263925)\n",
            "     | > loss: 0.10052156448364258  (0.09889061122511814)\n",
            "     | > step_time: 0.5418  (0.46839926997820536)\n",
            "     | > loader_time: 2.5054  (2.4879299521446243)\n",
            "\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_text_ce: 0.20550870895385742  (0.20550870895385742)\n",
            "     | > loss_mel_ce: 3.715508222579956  (3.715508222579956)\n",
            "     | > loss: 3.9210169315338135  (3.9210169315338135)\n",
            "\n",
            "\u001b[1m   --> STEP: 1\u001b[0m\n",
            "     | > loss_text_ce: 0.16680219769477844  (0.16680219769477844)\n",
            "     | > loss_mel_ce: 4.33697509765625  (4.33697509765625)\n",
            "     | > loss: 4.503777503967285  (4.503777503967285)\n",
            "\n",
            "\u001b[1m   --> STEP: 2\u001b[0m\n",
            "     | > loss_text_ce: 0.19006535410881042  (0.17843377590179443)\n",
            "     | > loss_mel_ce: 4.27119255065918  (4.304083824157715)\n",
            "     | > loss: 4.4612579345703125  (4.482517719268799)\n",
            "\n",
            "\u001b[1m   --> STEP: 3\u001b[0m\n",
            "     | > loss_text_ce: 0.16374288499355316  (0.17353681226571402)\n",
            "     | > loss_mel_ce: 4.518248558044434  (4.375472068786621)\n",
            "     | > loss: 4.6819915771484375  (4.549009005228679)\n",
            "\n",
            "\u001b[1m   --> STEP: 4\u001b[0m\n",
            "     | > loss_text_ce: 0.18310901522636414  (0.17592986300587654)\n",
            "     | > loss_mel_ce: 4.667556285858154  (4.448493123054504)\n",
            "     | > loss: 4.850665092468262  (4.624423027038574)\n",
            "\n",
            "\u001b[1m   --> STEP: 5\u001b[0m\n",
            "     | > loss_text_ce: 0.17130227386951447  (0.17500434517860414)\n",
            "     | > loss_mel_ce: 4.4999165534973145  (4.458777809143067)\n",
            "     | > loss: 4.6712188720703125  (4.633782196044922)\n",
            "\n",
            "\u001b[1m   --> STEP: 6\u001b[0m\n",
            "     | > loss_text_ce: 0.16987910866737366  (0.17415013909339905)\n",
            "     | > loss_mel_ce: 4.24496603012085  (4.423142512639363)\n",
            "     | > loss: 4.414844989776611  (4.59729266166687)\n",
            "\n",
            "\u001b[1m   --> STEP: 7\u001b[0m\n",
            "     | > loss_text_ce: 0.17500397562980652  (0.17427211574145726)\n",
            "     | > loss_mel_ce: 4.1328277587890625  (4.3816689763750345)\n",
            "     | > loss: 4.307831764221191  (4.555941104888916)\n",
            "\n",
            "\u001b[1m   --> STEP: 8\u001b[0m\n",
            "     | > loss_text_ce: 0.18827751278877258  (0.17602279037237167)\n",
            "     | > loss_mel_ce: 4.037768363952637  (4.338681399822235)\n",
            "     | > loss: 4.226046085357666  (4.51470422744751)\n",
            "\n",
            "\u001b[1m   --> STEP: 9\u001b[0m\n",
            "     | > loss_text_ce: 0.18136565387248993  (0.17661644187238482)\n",
            "     | > loss_mel_ce: 4.199512481689453  (4.32321818669637)\n",
            "     | > loss: 4.38087797164917  (4.499834643469916)\n",
            "\n",
            "\u001b[1m   --> STEP: 10\u001b[0m\n",
            "     | > loss_text_ce: 0.1733747273683548  (0.1762922704219818)\n",
            "     | > loss_mel_ce: 4.358778953552246  (4.326774263381958)\n",
            "     | > loss: 4.532153606414795  (4.503066539764404)\n",
            "\n",
            "\u001b[1m   --> STEP: 11\u001b[0m\n",
            "     | > loss_text_ce: 0.1748064160346985  (0.1761571927504106)\n",
            "     | > loss_mel_ce: 4.365818023681641  (4.330323696136475)\n",
            "     | > loss: 4.540624618530273  (4.506480910561302)\n",
            "\n",
            "\u001b[1m   --> STEP: 12\u001b[0m\n",
            "     | > loss_text_ce: 0.1684541404247284  (0.17551527172327042)\n",
            "     | > loss_mel_ce: 4.42825984954834  (4.33848504225413)\n",
            "     | > loss: 4.596714019775391  (4.5140003363291425)\n",
            "\n",
            "\u001b[1m   --> STEP: 13\u001b[0m\n",
            "     | > loss_text_ce: 0.1789093017578125  (0.17577635095669672)\n",
            "     | > loss_mel_ce: 4.38019323348999  (4.341693364656889)\n",
            "     | > loss: 4.559102535247803  (4.517469736245962)\n",
            "\n",
            "\u001b[1m   --> STEP: 14\u001b[0m\n",
            "     | > loss_text_ce: 0.18337440490722656  (0.1763190690960203)\n",
            "     | > loss_mel_ce: 4.317831039428711  (4.339988912854876)\n",
            "     | > loss: 4.5012054443359375  (4.516308001109532)\n",
            "\n",
            "\u001b[1m   --> STEP: 15\u001b[0m\n",
            "     | > loss_text_ce: 0.17328977584838867  (0.17611711621284484)\n",
            "     | > loss_mel_ce: 4.2869181632995605  (4.336450862884521)\n",
            "     | > loss: 4.460207939147949  (4.51256799697876)\n",
            "\n",
            "\u001b[1m   --> STEP: 16\u001b[0m\n",
            "     | > loss_text_ce: 0.1696963608264923  (0.17571581900119781)\n",
            "     | > loss_mel_ce: 4.191553592681885  (4.327394783496857)\n",
            "     | > loss: 4.361249923706055  (4.503110617399216)\n",
            "\n",
            "\u001b[1m   --> STEP: 17\u001b[0m\n",
            "     | > loss_text_ce: 0.1827973872423172  (0.17613238183891072)\n",
            "     | > loss_mel_ce: 4.229084491729736  (4.321611825157614)\n",
            "     | > loss: 4.411881923675537  (4.497744223650764)\n",
            "\n",
            "\u001b[1m   --> STEP: 18\u001b[0m\n",
            "     | > loss_text_ce: 0.18705594539642334  (0.17673924648099476)\n",
            "     | > loss_mel_ce: 4.255827903747559  (4.317957162857056)\n",
            "     | > loss: 4.4428839683532715  (4.494696431689793)\n",
            "\n",
            "\u001b[1m   --> STEP: 19\u001b[0m\n",
            "     | > loss_text_ce: 0.17226001620292664  (0.17650349751899116)\n",
            "     | > loss_mel_ce: 4.213974475860596  (4.312484389857242)\n",
            "     | > loss: 4.386234283447266  (4.488987897571764)\n",
            "\n",
            "\u001b[1m   --> STEP: 20\u001b[0m\n",
            "     | > loss_text_ce: 0.1825488805770874  (0.176805766671896)\n",
            "     | > loss_mel_ce: 4.321032524108887  (4.312911796569824)\n",
            "     | > loss: 4.503581523895264  (4.489717578887939)\n",
            "\n",
            "\u001b[1m   --> STEP: 21\u001b[0m\n",
            "     | > loss_text_ce: 0.16090352833271027  (0.17604851722717285)\n",
            "     | > loss_mel_ce: 4.599081993103027  (4.326538948785691)\n",
            "     | > loss: 4.759985446929932  (4.50258747736613)\n",
            "\n",
            "\u001b[1m   --> STEP: 22\u001b[0m\n",
            "     | > loss_text_ce: 0.1680433303117752  (0.17568464509465478)\n",
            "     | > loss_mel_ce: 4.698885440826416  (4.343463789332997)\n",
            "     | > loss: 4.866928577423096  (4.519148436459628)\n",
            "\n",
            "\u001b[1m   --> STEP: 23\u001b[0m\n",
            "     | > loss_text_ce: 0.1832210123538971  (0.17601231323636096)\n",
            "     | > loss_mel_ce: 4.180991172790527  (4.336399762526803)\n",
            "     | > loss: 4.3642120361328125  (4.512412071228027)\n",
            "\n",
            "\u001b[1m   --> STEP: 24\u001b[0m\n",
            "     | > loss_text_ce: 0.17193947732448578  (0.17584261174003282)\n",
            "     | > loss_mel_ce: 4.527822971343994  (4.344375729560852)\n",
            "     | > loss: 4.699762344360352  (4.520218332608541)\n",
            "\n",
            "\u001b[1m   --> STEP: 25\u001b[0m\n",
            "     | > loss_text_ce: 0.1764647513628006  (0.17586749732494356)\n",
            "     | > loss_mel_ce: 4.371512413024902  (4.345461196899414)\n",
            "     | > loss: 4.547976970672607  (4.521328678131104)\n",
            "\n",
            "\u001b[1m   --> STEP: 26\u001b[0m\n",
            "     | > loss_text_ce: 0.1894984394311905  (0.17639176432902998)\n",
            "     | > loss_mel_ce: 4.4226555824279785  (4.348430211727436)\n",
            "     | > loss: 4.612154006958008  (4.524821960009062)\n",
            "\n",
            "\u001b[1m   --> STEP: 27\u001b[0m\n",
            "     | > loss_text_ce: 0.1758863776922226  (0.1763730463054445)\n",
            "     | > loss_mel_ce: 4.605837345123291  (4.357963809260616)\n",
            "     | > loss: 4.781723499298096  (4.534336831834581)\n",
            "\n",
            "\u001b[1m   --> STEP: 28\u001b[0m\n",
            "     | > loss_text_ce: 0.1770600974559784  (0.176397583846535)\n",
            "     | > loss_mel_ce: 4.507577419281006  (4.363307152475629)\n",
            "     | > loss: 4.684637546539307  (4.539704714502607)\n",
            "\n",
            "\u001b[1m   --> STEP: 29\u001b[0m\n",
            "     | > loss_text_ce: 0.18233148753643036  (0.17660220121515208)\n",
            "     | > loss_mel_ce: 4.706562519073486  (4.37514354442728)\n",
            "     | > loss: 4.888894081115723  (4.551745727144438)\n",
            "\n",
            "\u001b[1m   --> STEP: 30\u001b[0m\n",
            "     | > loss_text_ce: 0.1725689321756363  (0.1764677589138349)\n",
            "     | > loss_mel_ce: 4.443299293518066  (4.377415402730306)\n",
            "     | > loss: 4.615868091583252  (4.553883139292399)\n",
            "\n",
            "\u001b[1m   --> STEP: 31\u001b[0m\n",
            "     | > loss_text_ce: 0.17275318503379822  (0.17634793394996273)\n",
            "     | > loss_mel_ce: 4.698705673217773  (4.387779605004095)\n",
            "     | > loss: 4.871459007263184  (4.564127522130167)\n",
            "\n",
            "\u001b[1m   --> STEP: 32\u001b[0m\n",
            "     | > loss_text_ce: 0.18938283622264862  (0.17675527464598417)\n",
            "     | > loss_mel_ce: 4.074203968048096  (4.37798036634922)\n",
            "     | > loss: 4.26358699798584  (4.554735630750656)\n",
            "\n",
            "\u001b[1m   --> STEP: 33\u001b[0m\n",
            "     | > loss_text_ce: 0.18078309297561646  (0.1768773297468821)\n",
            "     | > loss_mel_ce: 4.1347808837890625  (4.3706106850595186)\n",
            "     | > loss: 4.315564155578613  (4.547488010290897)\n",
            "\n",
            "\u001b[1m   --> STEP: 34\u001b[0m\n",
            "     | > loss_text_ce: 0.17825698852539062  (0.17691790794625)\n",
            "     | > loss_mel_ce: 4.11886739730835  (4.363206470713896)\n",
            "     | > loss: 4.29712438583374  (4.540124374277451)\n",
            "\n",
            "\u001b[1m   --> STEP: 35\u001b[0m\n",
            "     | > loss_text_ce: 0.17903581261634827  (0.17697841950825283)\n",
            "     | > loss_mel_ce: 4.047585964202881  (4.354188741956438)\n",
            "     | > loss: 4.226621627807617  (4.531167152949742)\n",
            "\n",
            "\u001b[1m   --> STEP: 36\u001b[0m\n",
            "     | > loss_text_ce: 0.16695651412010193  (0.17670003324747086)\n",
            "     | > loss_mel_ce: 4.210306167602539  (4.3501920037799415)\n",
            "     | > loss: 4.377262592315674  (4.526892026265462)\n",
            "\n",
            "\u001b[1m   --> STEP: 37\u001b[0m\n",
            "     | > loss_text_ce: 0.19083982706069946  (0.17708218983701757)\n",
            "     | > loss_mel_ce: 4.497838973999023  (4.354182462434511)\n",
            "     | > loss: 4.688678741455078  (4.531264640189506)\n",
            "\n",
            "\u001b[1m   --> STEP: 38\u001b[0m\n",
            "     | > loss_text_ce: 0.17218945920467377  (0.17695343376774536)\n",
            "     | > loss_mel_ce: 4.241247177124023  (4.35121048124213)\n",
            "     | > loss: 4.413436412811279  (4.528163897363763)\n",
            "\n",
            "\u001b[1m   --> STEP: 39\u001b[0m\n",
            "     | > loss_text_ce: 0.17558418214321136  (0.17691832475173166)\n",
            "     | > loss_mel_ce: 4.268317222595215  (4.349085013071696)\n",
            "     | > loss: 4.443901538848877  (4.52600332406851)\n",
            "\n",
            "\u001b[1m   --> STEP: 40\u001b[0m\n",
            "     | > loss_text_ce: 0.18886785209178925  (0.1772170629352331)\n",
            "     | > loss_mel_ce: 4.176130294799805  (4.344761145114899)\n",
            "     | > loss: 4.3649983406066895  (4.521978199481965)\n",
            "\n",
            "\u001b[1m   --> STEP: 41\u001b[0m\n",
            "     | > loss_text_ce: 0.1770874708890915  (0.17721390215361987)\n",
            "     | > loss_mel_ce: 4.377133369445801  (4.345550711561994)\n",
            "     | > loss: 4.554220676422119  (4.522764601358554)\n",
            "\n",
            "\u001b[1m   --> STEP: 42\u001b[0m\n",
            "     | > loss_text_ce: 0.19007617235183716  (0.17752014668214885)\n",
            "     | > loss_mel_ce: 3.8665695190429688  (4.334146397454398)\n",
            "     | > loss: 4.05664587020874  (4.511666536331178)\n",
            "\n",
            "\u001b[1m   --> STEP: 43\u001b[0m\n",
            "     | > loss_text_ce: 0.18440985679626465  (0.17768037249875618)\n",
            "     | > loss_mel_ce: 4.048165798187256  (4.327495685843534)\n",
            "     | > loss: 4.232575416564941  (4.505176045173823)\n",
            "\n",
            "\u001b[1m   --> STEP: 44\u001b[0m\n",
            "     | > loss_text_ce: 0.17235346138477325  (0.17755930633707473)\n",
            "     | > loss_mel_ce: 4.2806077003479  (4.326430049809542)\n",
            "     | > loss: 4.452960968017578  (4.503989338874818)\n",
            "\n",
            "\u001b[1m   --> STEP: 45\u001b[0m\n",
            "     | > loss_text_ce: 0.188504159450531  (0.17780252529515153)\n",
            "     | > loss_mel_ce: 4.17727518081665  (4.323115497165255)\n",
            "     | > loss: 4.365779399871826  (4.500918006896973)\n",
            "\n",
            "\u001b[1m   --> STEP: 46\u001b[0m\n",
            "     | > loss_text_ce: 0.18985483050346375  (0.17806453193011484)\n",
            "     | > loss_mel_ce: 3.862527847290039  (4.313102722167968)\n",
            "     | > loss: 4.052382469177246  (4.491167234337849)\n",
            "\n",
            "\u001b[1m   --> STEP: 47\u001b[0m\n",
            "     | > loss_text_ce: 0.17563867568969727  (0.17801291796755275)\n",
            "     | > loss_mel_ce: 4.365763187408447  (4.314223157598617)\n",
            "     | > loss: 4.5414018630981445  (4.492236056226366)\n",
            "\n",
            "\u001b[1m   --> STEP: 48\u001b[0m\n",
            "     | > loss_text_ce: 0.1802472323179245  (0.1780594661831855)\n",
            "     | > loss_mel_ce: 3.826287031173706  (4.304057821631432)\n",
            "     | > loss: 4.006534099578857  (4.482117265462876)\n",
            "\n",
            "\u001b[1m   --> STEP: 49\u001b[0m\n",
            "     | > loss_text_ce: 0.17490750551223755  (0.17799514045520698)\n",
            "     | > loss_mel_ce: 4.2036004066467285  (4.302007670305213)\n",
            "     | > loss: 4.3785080909729  (4.480002792514101)\n",
            "\n",
            "\u001b[1m   --> STEP: 50\u001b[0m\n",
            "     | > loss_text_ce: 0.2018202543258667  (0.17847164273262017)\n",
            "     | > loss_mel_ce: 3.93526029586792  (4.2946727228164665)\n",
            "     | > loss: 4.137080669403076  (4.473144350051881)\n",
            "\n",
            "\u001b[1m   --> STEP: 51\u001b[0m\n",
            "     | > loss_text_ce: 0.17813432216644287  (0.1784650286038716)\n",
            "     | > loss_mel_ce: 4.692833423614502  (4.302479795381134)\n",
            "     | > loss: 4.870967864990234  (4.4809448111291035)\n",
            "\n",
            "\u001b[1m   --> STEP: 52\u001b[0m\n",
            "     | > loss_text_ce: 0.18463118374347687  (0.17858360851040248)\n",
            "     | > loss_mel_ce: 4.305540561676025  (4.3025386562714205)\n",
            "     | > loss: 4.490171909332275  (4.481122255325318)\n",
            "\n",
            "\u001b[1m   --> STEP: 53\u001b[0m\n",
            "     | > loss_text_ce: 0.18214738368988037  (0.1786508495515247)\n",
            "     | > loss_mel_ce: 4.209097862243652  (4.30077562242184)\n",
            "     | > loss: 4.391245365142822  (4.479426464944517)\n",
            "\n",
            "\u001b[1m   --> STEP: 54\u001b[0m\n",
            "     | > loss_text_ce: 0.1841922551393509  (0.17875346817352147)\n",
            "     | > loss_mel_ce: 4.388821125030518  (4.302406094692371)\n",
            "     | > loss: 4.5730133056640625  (4.481159554587471)\n",
            "\n",
            "\u001b[1m   --> STEP: 55\u001b[0m\n",
            "     | > loss_text_ce: 0.18125514686107635  (0.1787989532405679)\n",
            "     | > loss_mel_ce: 4.068699836730957  (4.298156890002163)\n",
            "     | > loss: 4.249955177307129  (4.476955838636919)\n",
            "\n",
            "\u001b[1m   --> STEP: 56\u001b[0m\n",
            "     | > loss_text_ce: 0.1789020150899887  (0.17880079363073614)\n",
            "     | > loss_mel_ce: 3.9882121086120605  (4.292622161763054)\n",
            "     | > loss: 4.1671142578125  (4.471422953265054)\n",
            "\n",
            "\u001b[1m   --> STEP: 57\u001b[0m\n",
            "     | > loss_text_ce: 0.1938379555940628  (0.17906460348974187)\n",
            "     | > loss_mel_ce: 4.1190361976623535  (4.289576793971814)\n",
            "     | > loss: 4.3128743171691895  (4.468641398245828)\n",
            "\n",
            "\u001b[1m   --> STEP: 58\u001b[0m\n",
            "     | > loss_text_ce: 0.17354127764701843  (0.17896937373383287)\n",
            "     | > loss_mel_ce: 4.403809070587158  (4.291546315982424)\n",
            "     | > loss: 4.57735013961792  (4.470515686890175)\n",
            "\n",
            "\u001b[1m   --> STEP: 59\u001b[0m\n",
            "     | > loss_text_ce: 0.18955306708812714  (0.17914875836695648)\n",
            "     | > loss_mel_ce: 3.6712024211883545  (4.2810320126808294)\n",
            "     | > loss: 3.860755443572998  (4.460180767511917)\n",
            "\n",
            "\u001b[1m   --> STEP: 60\u001b[0m\n",
            "     | > loss_text_ce: 0.18971936404705048  (0.1793249351282914)\n",
            "     | > loss_mel_ce: 3.7575490474700928  (4.272307296593984)\n",
            "     | > loss: 3.947268486022949  (4.451632229487101)\n",
            "\n",
            "\u001b[1m   --> STEP: 61\u001b[0m\n",
            "     | > loss_text_ce: 0.174261674284935  (0.1792419308521708)\n",
            "     | > loss_mel_ce: 4.711799621582031  (4.279512088806903)\n",
            "     | > loss: 4.886061191558838  (4.458754015750572)\n",
            "\n",
            "\u001b[1m   --> STEP: 62\u001b[0m\n",
            "     | > loss_text_ce: 0.17582960426807404  (0.17918689332662086)\n",
            "     | > loss_mel_ce: 4.260068416595459  (4.279198481190589)\n",
            "     | > loss: 4.4358978271484375  (4.458385367547312)\n",
            "\n",
            "\u001b[1m   --> STEP: 63\u001b[0m\n",
            "     | > loss_text_ce: 0.17400652170181274  (0.17910466520559215)\n",
            "     | > loss_mel_ce: 4.340902805328369  (4.280177914907062)\n",
            "     | > loss: 4.514909267425537  (4.459282572307284)\n",
            "\n",
            "\u001b[1m   --> STEP: 64\u001b[0m\n",
            "     | > loss_text_ce: 0.17717772722244263  (0.17907455679960543)\n",
            "     | > loss_mel_ce: 4.1520466804504395  (4.278175864368677)\n",
            "     | > loss: 4.329224586486816  (4.457250416278839)\n",
            "\n",
            "\u001b[1m   --> STEP: 65\u001b[0m\n",
            "     | > loss_text_ce: 0.18625150620937347  (0.17918497140590955)\n",
            "     | > loss_mel_ce: 4.205633640289307  (4.277059830152071)\n",
            "     | > loss: 4.391885280609131  (4.456244798806997)\n",
            "\n",
            "\u001b[1m   --> STEP: 66\u001b[0m\n",
            "     | > loss_text_ce: 0.1816232055425644  (0.17922191434737403)\n",
            "     | > loss_mel_ce: 3.9510581493377686  (4.272120410745794)\n",
            "     | > loss: 4.132681369781494  (4.451342322609642)\n",
            "\n",
            "\u001b[1m   --> STEP: 67\u001b[0m\n",
            "     | > loss_text_ce: 0.17684824764728546  (0.17918648648617866)\n",
            "     | > loss_mel_ce: 4.353842258453369  (4.273340139816057)\n",
            "     | > loss: 4.530690670013428  (4.452526626302235)\n",
            "\n",
            "\u001b[1m   --> STEP: 68\u001b[0m\n",
            "     | > loss_text_ce: 0.17728209495544434  (0.17915848072837373)\n",
            "     | > loss_mel_ce: 4.2122883796691895  (4.272442319813897)\n",
            "     | > loss: 4.389570236206055  (4.451600797036114)\n",
            "\n",
            "\u001b[1m   --> STEP: 69\u001b[0m\n",
            "     | > loss_text_ce: 0.17585749924182892  (0.17911064041697453)\n",
            "     | > loss_mel_ce: 4.422715187072754  (4.274620187455329)\n",
            "     | > loss: 4.598572731018066  (4.453730825064839)\n",
            "\n",
            "\u001b[1m   --> STEP: 70\u001b[0m\n",
            "     | > loss_text_ce: 0.18480275571346283  (0.1791919563497815)\n",
            "     | > loss_mel_ce: 4.457193851470947  (4.277228382655553)\n",
            "     | > loss: 4.641996383666992  (4.4564203330448695)\n",
            "\n",
            "\u001b[1m   --> STEP: 71\u001b[0m\n",
            "     | > loss_text_ce: 0.16802212595939636  (0.17903463479498735)\n",
            "     | > loss_mel_ce: 4.205399036407471  (4.276216701722481)\n",
            "     | > loss: 4.3734211921691895  (4.455251331060705)\n",
            "\n",
            "\u001b[1m   --> STEP: 72\u001b[0m\n",
            "     | > loss_text_ce: 0.18236352503299713  (0.17908086938162637)\n",
            "     | > loss_mel_ce: 4.18040132522583  (4.274885932604472)\n",
            "     | > loss: 4.362764835357666  (4.453966796398163)\n",
            "\n",
            "\u001b[1m   --> STEP: 73\u001b[0m\n",
            "     | > loss_text_ce: 0.2103932648897171  (0.17950980630639474)\n",
            "     | > loss_mel_ce: 4.5765862464904785  (4.2790188136166085)\n",
            "     | > loss: 4.786979675292969  (4.458528616656996)\n",
            "\n",
            "\u001b[1m   --> STEP: 74\u001b[0m\n",
            "     | > loss_text_ce: 0.17687152326107025  (0.17947415383280926)\n",
            "     | > loss_mel_ce: 4.269637584686279  (4.278892040252685)\n",
            "     | > loss: 4.446508884429932  (4.458366187843117)\n",
            "\n",
            "\u001b[1m   --> STEP: 75\u001b[0m\n",
            "     | > loss_text_ce: 0.17616716027259827  (0.1794300605853398)\n",
            "     | > loss_mel_ce: 4.43113374710083  (4.280921929677326)\n",
            "     | > loss: 4.607300758361816  (4.460351982116699)\n",
            "\n",
            "\u001b[1m   --> STEP: 76\u001b[0m\n",
            "     | > loss_text_ce: 0.16732898354530334  (0.1792708358874446)\n",
            "     | > loss_mel_ce: 4.53518009185791  (4.284267431811282)\n",
            "     | > loss: 4.702508926391602  (4.4635382576992635)\n",
            "\n",
            "\u001b[1m   --> STEP: 77\u001b[0m\n",
            "     | > loss_text_ce: 0.19701442122459412  (0.17950127206065433)\n",
            "     | > loss_mel_ce: 4.16388463973999  (4.282704018927239)\n",
            "     | > loss: 4.360898971557617  (4.462205279957164)\n",
            "\n",
            "\u001b[1m   --> STEP: 78\u001b[0m\n",
            "     | > loss_text_ce: 0.19040143489837646  (0.17964101773806102)\n",
            "     | > loss_mel_ce: 4.357049942016602  (4.283657171787359)\n",
            "     | > loss: 4.547451496124268  (4.463298180164435)\n",
            "\n",
            "\u001b[1m   --> STEP: 79\u001b[0m\n",
            "     | > loss_text_ce: 0.1810878962278366  (0.1796593326556531)\n",
            "     | > loss_mel_ce: 4.576109409332275  (4.28735909884489)\n",
            "     | > loss: 4.757197380065918  (4.467018423201163)\n",
            "\n",
            "\u001b[1m   --> STEP: 80\u001b[0m\n",
            "     | > loss_text_ce: 0.175151988863945  (0.17960299085825676)\n",
            "     | > loss_mel_ce: 4.002559185028076  (4.283799099922179)\n",
            "     | > loss: 4.177711009979248  (4.463402080535889)\n",
            "\n",
            "\u001b[1m   --> STEP: 81\u001b[0m\n",
            "     | > loss_text_ce: 0.18576426804065704  (0.17967905600865677)\n",
            "     | > loss_mel_ce: 4.142697811126709  (4.282057108702482)\n",
            "     | > loss: 4.32846212387085  (4.461736155144963)\n",
            "\n",
            "\u001b[1m   --> STEP: 82\u001b[0m\n",
            "     | > loss_text_ce: 0.1687692254781723  (0.17954600929487038)\n",
            "     | > loss_mel_ce: 4.288015842437744  (4.282129776187058)\n",
            "     | > loss: 4.456785202026367  (4.461675777667907)\n",
            "\n",
            "\u001b[1m   --> STEP: 83\u001b[0m\n",
            "     | > loss_text_ce: 0.2314104288816452  (0.18017088182001226)\n",
            "     | > loss_mel_ce: 3.959519624710083  (4.278242906892155)\n",
            "     | > loss: 4.190929889678955  (4.4584137790174365)\n",
            "\n",
            "\u001b[1m   --> STEP: 84\u001b[0m\n",
            "     | > loss_text_ce: 0.18556049466133118  (0.18023504387764702)\n",
            "     | > loss_mel_ce: 3.934262275695801  (4.274147899377913)\n",
            "     | > loss: 4.119822978973389  (4.454382936159769)\n",
            "\n",
            "\u001b[1m   --> STEP: 85\u001b[0m\n",
            "     | > loss_text_ce: 0.17440171539783478  (0.18016641648376688)\n",
            "     | > loss_mel_ce: 4.360617637634277  (4.275165190416224)\n",
            "     | > loss: 4.535019397735596  (4.4553316004136025)\n",
            "\n",
            "\u001b[1m   --> STEP: 86\u001b[0m\n",
            "     | > loss_text_ce: 0.17421038448810577  (0.18009716029777084)\n",
            "     | > loss_mel_ce: 3.9435501098632812  (4.271309201107469)\n",
            "     | > loss: 4.11776065826416  (4.451406356900237)\n",
            "\n",
            "\u001b[1m   --> STEP: 87\u001b[0m\n",
            "     | > loss_text_ce: 0.18163366615772247  (0.18011482128466683)\n",
            "     | > loss_mel_ce: 4.38221549987793  (4.272583986150807)\n",
            "     | > loss: 4.563848972320557  (4.452698800755643)\n",
            "\n",
            "\u001b[1m   --> STEP: 88\u001b[0m\n",
            "     | > loss_text_ce: 0.17102521657943726  (0.1800115303221074)\n",
            "     | > loss_mel_ce: 4.411573886871338  (4.274163416840813)\n",
            "     | > loss: 4.58259916305542  (4.454174941236322)\n",
            "\n",
            "\u001b[1m   --> STEP: 89\u001b[0m\n",
            "     | > loss_text_ce: 0.16440358757972717  (0.17983616017893458)\n",
            "     | > loss_mel_ce: 4.5283942222595215  (4.277019942744395)\n",
            "     | > loss: 4.692797660827637  (4.456856095389034)\n",
            "\n",
            "\u001b[1m   --> STEP: 90\u001b[0m\n",
            "     | > loss_text_ce: 0.19691850244998932  (0.1800259639819463)\n",
            "     | > loss_mel_ce: 4.1688313484191895  (4.275817847251892)\n",
            "     | > loss: 4.365749835968018  (4.4558438036176895)\n",
            "\n",
            "\u001b[1m   --> STEP: 91\u001b[0m\n",
            "     | > loss_text_ce: 0.1869356483221054  (0.18010189457909093)\n",
            "     | > loss_mel_ce: 4.218329906463623  (4.275186111638834)\n",
            "     | > loss: 4.4052653312683105  (4.455287996229234)\n",
            "\n",
            "\u001b[1m   --> STEP: 92\u001b[0m\n",
            "     | > loss_text_ce: 0.1737486869096756  (0.18003283797398859)\n",
            "     | > loss_mel_ce: 4.206292152404785  (4.274437264255855)\n",
            "     | > loss: 4.380040645599365  (4.454470090244127)\n",
            "\n",
            "\u001b[1m   --> STEP: 93\u001b[0m\n",
            "     | > loss_text_ce: 0.17996113002300262  (0.18003206692075216)\n",
            "     | > loss_mel_ce: 4.125905990600586  (4.2728401537864436)\n",
            "     | > loss: 4.3058671951293945  (4.452872209651495)\n",
            "\n",
            "\u001b[1m   --> STEP: 94\u001b[0m\n",
            "     | > loss_text_ce: 0.1822398453950882  (0.1800555539257983)\n",
            "     | > loss_mel_ce: 4.499092102050781  (4.275247089406276)\n",
            "     | > loss: 4.681332111358643  (4.455302634137741)\n",
            "\n",
            "\u001b[1m   --> STEP: 95\u001b[0m\n",
            "     | > loss_text_ce: 0.17380349338054657  (0.17998974276216406)\n",
            "     | > loss_mel_ce: 4.337036609649658  (4.275897505408839)\n",
            "     | > loss: 4.510839939117432  (4.455887237348054)\n",
            "\n",
            "\u001b[1m   --> STEP: 96\u001b[0m\n",
            "     | > loss_text_ce: 0.19091029465198517  (0.18010349851101637)\n",
            "     | > loss_mel_ce: 4.252153396606445  (4.275650170942147)\n",
            "     | > loss: 4.443063735961914  (4.455753659208615)\n",
            "\n",
            "\u001b[1m   --> STEP: 97\u001b[0m\n",
            "     | > loss_text_ce: 0.17619827389717102  (0.18006323846345096)\n",
            "     | > loss_mel_ce: 3.889080047607422  (4.2716649119386965)\n",
            "     | > loss: 4.06527853012085  (4.451728142413897)\n",
            "\n",
            "\u001b[1m   --> STEP: 98\u001b[0m\n",
            "     | > loss_text_ce: 0.17577165365219116  (0.1800194467817034)\n",
            "     | > loss_mel_ce: 4.312776565551758  (4.272084418608217)\n",
            "     | > loss: 4.488548278808594  (4.452103858091394)\n",
            "\n",
            "\u001b[1m   --> STEP: 99\u001b[0m\n",
            "     | > loss_text_ce: 0.18766072392463684  (0.1800966313993088)\n",
            "     | > loss_mel_ce: 4.0921759605407715  (4.270267161456021)\n",
            "     | > loss: 4.279836654663086  (4.4503637853294915)\n",
            "\n",
            "\u001b[1m   --> STEP: 100\u001b[0m\n",
            "     | > loss_text_ce: 0.1883026510477066  (0.18017869159579278)\n",
            "     | > loss_mel_ce: 4.143211364746094  (4.2689966034889215)\n",
            "     | > loss: 4.33151388168335  (4.4491752862930305)\n",
            "\n",
            "\u001b[1m   --> STEP: 101\u001b[0m\n",
            "     | > loss_text_ce: 0.18316742777824402  (0.18020828304314376)\n",
            "     | > loss_mel_ce: 4.448200225830078  (4.270770896779428)\n",
            "     | > loss: 4.6313676834106445  (4.450979171413007)\n",
            "\n",
            "\u001b[1m   --> STEP: 102\u001b[0m\n",
            "     | > loss_text_ce: 0.17591820657253265  (0.1801662234699025)\n",
            "     | > loss_mel_ce: 4.159002780914307  (4.269675130937613)\n",
            "     | > loss: 4.334920883178711  (4.449841345057768)\n",
            "\n",
            "\u001b[1m   --> STEP: 103\u001b[0m\n",
            "     | > loss_text_ce: 0.1868453323841095  (0.18023106918751614)\n",
            "     | > loss_mel_ce: 4.242325782775879  (4.269409603285557)\n",
            "     | > loss: 4.429171085357666  (4.449640662924758)\n",
            "\n",
            "\u001b[1m   --> STEP: 104\u001b[0m\n",
            "     | > loss_text_ce: 0.18688365817070007  (0.18029503638927752)\n",
            "     | > loss_mel_ce: 4.368330478668213  (4.270360765548852)\n",
            "     | > loss: 4.555213928222656  (4.450655790475699)\n",
            "\n",
            "\u001b[1m   --> STEP: 105\u001b[0m\n",
            "     | > loss_text_ce: 0.18574108183383942  (0.18034690348874954)\n",
            "     | > loss_mel_ce: 4.353895664215088  (4.27115633601234)\n",
            "     | > loss: 4.539636611938477  (4.4515032268705825)\n",
            "\n",
            "\u001b[1m   --> STEP: 106\u001b[0m\n",
            "     | > loss_text_ce: 0.17130401730537415  (0.18026159324173657)\n",
            "     | > loss_mel_ce: 4.371363162994385  (4.272101683436699)\n",
            "     | > loss: 4.542667388916016  (4.452363266135162)\n",
            "\n",
            "\u001b[1m   --> STEP: 107\u001b[0m\n",
            "     | > loss_text_ce: 0.17653948068618774  (0.1802268071430866)\n",
            "     | > loss_mel_ce: 4.116070747375488  (4.270643450389398)\n",
            "     | > loss: 4.292610168457031  (4.450870246530693)\n",
            "\n",
            "\u001b[1m   --> STEP: 108\u001b[0m\n",
            "     | > loss_text_ce: 0.18959805369377136  (0.1803135779444818)\n",
            "     | > loss_mel_ce: 4.156308650970459  (4.269584794839223)\n",
            "     | > loss: 4.345906734466553  (4.449898362159728)\n",
            "\n",
            "\u001b[1m   --> STEP: 109\u001b[0m\n",
            "     | > loss_text_ce: 0.17173711955547333  (0.1802348948399955)\n",
            "     | > loss_mel_ce: 4.411075592041016  (4.270882875547496)\n",
            "     | > loss: 4.582812786102295  (4.4511177605445225)\n",
            "\n",
            "\u001b[1m   --> STEP: 110\u001b[0m\n",
            "     | > loss_text_ce: 0.16952700912952423  (0.18013755042444576)\n",
            "     | > loss_mel_ce: 4.3504862785339355  (4.271606542847373)\n",
            "     | > loss: 4.520013332366943  (4.451744083924726)\n",
            "\n",
            "\u001b[1m   --> STEP: 111\u001b[0m\n",
            "     | > loss_text_ce: 0.19265314936637878  (0.1802503035680668)\n",
            "     | > loss_mel_ce: 4.246689796447754  (4.271382067654583)\n",
            "     | > loss: 4.439342975616455  (4.451632362228255)\n",
            "\n",
            "\u001b[1m   --> STEP: 112\u001b[0m\n",
            "     | > loss_text_ce: 0.16607701778411865  (0.18012375637356723)\n",
            "     | > loss_mel_ce: 4.227304458618164  (4.270988517573901)\n",
            "     | > loss: 4.393381595611572  (4.451112266097749)\n",
            "\n",
            "\u001b[1m   --> STEP: 113\u001b[0m\n",
            "     | > loss_text_ce: 0.17460204660892487  (0.18007489168538457)\n",
            "     | > loss_mel_ce: 4.0593109130859375  (4.26911526443684)\n",
            "     | > loss: 4.233912944793701  (4.449190148210103)\n",
            "\n",
            "\u001b[1m   --> STEP: 114\u001b[0m\n",
            "     | > loss_text_ce: 0.17177529633045197  (0.18000208821735883)\n",
            "     | > loss_mel_ce: 4.490133762359619  (4.2710540231905485)\n",
            "     | > loss: 4.661909103393555  (4.451056103957326)\n",
            "\n",
            "\u001b[1m   --> STEP: 115\u001b[0m\n",
            "     | > loss_text_ce: 0.1722893863916397  (0.17993502124496127)\n",
            "     | > loss_mel_ce: 4.337507724761963  (4.271631881465082)\n",
            "     | > loss: 4.509797096252441  (4.4515668951946745)\n",
            "\n",
            "\u001b[1m   --> STEP: 116\u001b[0m\n",
            "     | > loss_text_ce: 0.17149941623210907  (0.17986230051209187)\n",
            "     | > loss_mel_ce: 4.138193607330322  (4.270481551515645)\n",
            "     | > loss: 4.309692859649658  (4.450343843164114)\n",
            "\n",
            "\u001b[1m   --> STEP: 117\u001b[0m\n",
            "     | > loss_text_ce: 0.17487184703350067  (0.1798196470635569)\n",
            "     | > loss_mel_ce: 3.957960844039917  (4.267810434357733)\n",
            "     | > loss: 4.1328325271606445  (4.44763007123246)\n",
            "\n",
            "\u001b[1m   --> STEP: 118\u001b[0m\n",
            "     | > loss_text_ce: 0.17420336604118347  (0.17977205146167238)\n",
            "     | > loss_mel_ce: 4.33402156829834  (4.268371545662315)\n",
            "     | > loss: 4.508224964141846  (4.448143587274066)\n",
            "\n",
            "\u001b[1m   --> STEP: 119\u001b[0m\n",
            "     | > loss_text_ce: 0.1884455382823944  (0.17984493790554398)\n",
            "     | > loss_mel_ce: 4.154975414276123  (4.267418636995203)\n",
            "     | > loss: 4.34342098236084  (4.447263565384039)\n",
            "\n",
            "\u001b[1m   --> STEP: 120\u001b[0m\n",
            "     | > loss_text_ce: 0.18440525233745575  (0.17988294052580991)\n",
            "     | > loss_mel_ce: 3.9861550331115723  (4.2650747736295065)\n",
            "     | > loss: 4.170560359954834  (4.444957705338795)\n",
            "\n",
            "\u001b[1m   --> STEP: 121\u001b[0m\n",
            "     | > loss_text_ce: 0.17822431027889252  (0.17986923283781886)\n",
            "     | > loss_mel_ce: 4.384515762329102  (4.266061889238594)\n",
            "     | > loss: 4.562739849090576  (4.445931111485504)\n",
            "\n",
            "\u001b[1m   --> STEP: 122\u001b[0m\n",
            "     | > loss_text_ce: 0.17119444906711578  (0.17979812805281312)\n",
            "     | > loss_mel_ce: 4.3248066902160645  (4.266543404000704)\n",
            "     | > loss: 4.496001243591309  (4.446341522404404)\n",
            "\n",
            "\u001b[1m   --> STEP: 123\u001b[0m\n",
            "     | > loss_text_ce: 0.1773846596479416  (0.17977850635846457)\n",
            "     | > loss_mel_ce: 4.2769551277160645  (4.266628052160992)\n",
            "     | > loss: 4.454339981079102  (4.446406550523711)\n",
            "\n",
            "\u001b[1m   --> STEP: 124\u001b[0m\n",
            "     | > loss_text_ce: 0.18122747540473938  (0.1797901915927087)\n",
            "     | > loss_mel_ce: 4.047516822814941  (4.264861026117878)\n",
            "     | > loss: 4.2287445068359375  (4.4446512114617125)\n",
            "\n",
            "\u001b[1m   --> STEP: 125\u001b[0m\n",
            "     | > loss_text_ce: 0.17591755092144012  (0.17975921046733856)\n",
            "     | > loss_mel_ce: 4.331181049346924  (4.265391586303711)\n",
            "     | > loss: 4.50709867477417  (4.445150791168212)\n",
            "\n",
            "\u001b[1m   --> STEP: 126\u001b[0m\n",
            "     | > loss_text_ce: 0.1805141270160675  (0.17976520186851894)\n",
            "     | > loss_mel_ce: 4.176840782165527  (4.264688802143884)\n",
            "     | > loss: 4.357355117797852  (4.444454000109717)\n",
            "\n",
            "\u001b[1m   --> STEP: 127\u001b[0m\n",
            "     | > loss_text_ce: 0.18296344578266144  (0.17979038489146495)\n",
            "     | > loss_mel_ce: 4.520235061645508  (4.26670097741555)\n",
            "     | > loss: 4.703198432922363  (4.446491357848399)\n",
            "\n",
            "\u001b[1m   --> STEP: 128\u001b[0m\n",
            "     | > loss_text_ce: 0.21113276481628418  (0.1800352472346276)\n",
            "     | > loss_mel_ce: 3.924388885498047  (4.264026664197445)\n",
            "     | > loss: 4.13552188873291  (4.4440619088709346)\n",
            "\n",
            "\u001b[1m   --> STEP: 129\u001b[0m\n",
            "     | > loss_text_ce: 0.18433591723442078  (0.18006858576175777)\n",
            "     | > loss_mel_ce: 4.310107707977295  (4.264383881591087)\n",
            "     | > loss: 4.494443416595459  (4.444452463194381)\n",
            "\n",
            "\u001b[1m   --> STEP: 130\u001b[0m\n",
            "     | > loss_text_ce: 0.16504009068012238  (0.1799529819534375)\n",
            "     | > loss_mel_ce: 4.31178617477417  (4.264748514615572)\n",
            "     | > loss: 4.476826190948486  (4.444701491869412)\n",
            "\n",
            "\u001b[1m   --> STEP: 131\u001b[0m\n",
            "     | > loss_text_ce: 0.17390070855617523  (0.17990678139315305)\n",
            "     | > loss_mel_ce: 4.574375629425049  (4.267112080377477)\n",
            "     | > loss: 4.748276233673096  (4.447018856310661)\n",
            "\n",
            "\u001b[1m   --> STEP: 132\u001b[0m\n",
            "     | > loss_text_ce: 0.17944414913654327  (0.17990327660333028)\n",
            "     | > loss_mel_ce: 4.4424662590026855  (4.268440521124638)\n",
            "     | > loss: 4.621910572052002  (4.4483437935511265)\n",
            "\n",
            "\u001b[1m   --> STEP: 133\u001b[0m\n",
            "     | > loss_text_ce: 0.17024599015712738  (0.179830665427043)\n",
            "     | > loss_mel_ce: 4.548656940460205  (4.270547411495581)\n",
            "     | > loss: 4.718903064727783  (4.450378073785537)\n",
            "\n",
            "\u001b[1m   --> STEP: 134\u001b[0m\n",
            "     | > loss_text_ce: 0.1650443822145462  (0.17972032002993485)\n",
            "     | > loss_mel_ce: 4.520108699798584  (4.272409809169485)\n",
            "     | > loss: 4.685153007507324  (4.45213012552973)\n",
            "\n",
            "\u001b[1m   --> STEP: 135\u001b[0m\n",
            "     | > loss_text_ce: 0.19042780995368958  (0.1797996347701108)\n",
            "     | > loss_mel_ce: 4.063385486602783  (4.270861480854176)\n",
            "     | > loss: 4.25381326675415  (4.450661111761023)\n",
            "\n",
            "\u001b[1m   --> STEP: 136\u001b[0m\n",
            "     | > loss_text_ce: 0.1822277158498764  (0.17981748830746203)\n",
            "     | > loss_mel_ce: 4.244747161865234  (4.270669463802786)\n",
            "     | > loss: 4.426974773406982  (4.450486947508419)\n",
            "\n",
            "\u001b[1m   --> STEP: 137\u001b[0m\n",
            "     | > loss_text_ce: 0.17565402388572693  (0.1797870980562085)\n",
            "     | > loss_mel_ce: 4.2783613204956055  (4.27072560874215)\n",
            "     | > loss: 4.454015254974365  (4.450512701577514)\n",
            "\n",
            "\u001b[1m   --> STEP: 138\u001b[0m\n",
            "     | > loss_text_ce: 0.17224372923374176  (0.17973243596329205)\n",
            "     | > loss_mel_ce: 4.336024284362793  (4.271198787550995)\n",
            "     | > loss: 4.508267879486084  (4.45093121735946)\n",
            "\n",
            "\u001b[1m   --> STEP: 139\u001b[0m\n",
            "     | > loss_text_ce: 0.1897682547569275  (0.17980463609849806)\n",
            "     | > loss_mel_ce: 3.936495780944824  (4.26879085225167)\n",
            "     | > loss: 4.1262640953063965  (4.448595482668431)\n",
            "\n",
            "\u001b[1m   --> STEP: 140\u001b[0m\n",
            "     | > loss_text_ce: 0.17178095877170563  (0.1797473241175924)\n",
            "     | > loss_mel_ce: 4.237485408782959  (4.26856724194118)\n",
            "     | > loss: 4.409266471862793  (4.448314561162676)\n",
            "\n",
            "\u001b[1m   --> STEP: 141\u001b[0m\n",
            "     | > loss_text_ce: 0.17771151661872864  (0.17973288576653662)\n",
            "     | > loss_mel_ce: 4.091379642486572  (4.267310592299658)\n",
            "     | > loss: 4.2690911293029785  (4.4470434729934585)\n",
            "\n",
            "\u001b[1m   --> STEP: 142\u001b[0m\n",
            "     | > loss_text_ce: 0.17811653017997742  (0.1797215029807158)\n",
            "     | > loss_mel_ce: 4.288050651550293  (4.267456649054944)\n",
            "     | > loss: 4.466166973114014  (4.447178145529518)\n",
            "\n",
            "\u001b[1m   --> STEP: 143\u001b[0m\n",
            "     | > loss_text_ce: 0.1833244264125824  (0.1797466982494701)\n",
            "     | > loss_mel_ce: 4.190495491027832  (4.266918459138671)\n",
            "     | > loss: 4.373819828033447  (4.446665150302273)\n",
            "\n",
            "\u001b[1m   --> STEP: 144\u001b[0m\n",
            "     | > loss_text_ce: 0.19095566868782043  (0.17982453832195866)\n",
            "     | > loss_mel_ce: 4.186885833740234  (4.266362677017849)\n",
            "     | > loss: 4.377841472625732  (4.446187208096187)\n",
            "\n",
            "\u001b[1m   --> STEP: 145\u001b[0m\n",
            "     | > loss_text_ce: 0.18670158088207245  (0.17987196620168358)\n",
            "     | > loss_mel_ce: 3.9825599193573  (4.26440541662019)\n",
            "     | > loss: 4.169261455535889  (4.444277375319908)\n",
            "\n",
            "\u001b[1m   --> STEP: 146\u001b[0m\n",
            "     | > loss_text_ce: 0.178363636136055  (0.1798616351738368)\n",
            "     | > loss_mel_ce: 4.308882236480713  (4.264710052372659)\n",
            "     | > loss: 4.487246036529541  (4.444571681218605)\n",
            "\n",
            "\u001b[1m   --> STEP: 147\u001b[0m\n",
            "     | > loss_text_ce: 0.16985079646110535  (0.17979353423021277)\n",
            "     | > loss_mel_ce: 4.524531364440918  (4.26647754429149)\n",
            "     | > loss: 4.694382190704346  (4.446271072439596)\n",
            "\n",
            "\u001b[1m   --> STEP: 148\u001b[0m\n",
            "     | > loss_text_ce: 0.1804911196231842  (0.17979824764503016)\n",
            "     | > loss_mel_ce: 4.520837783813477  (4.26819619455853)\n",
            "     | > loss: 4.701328754425049  (4.447994435155714)\n",
            "\n",
            "\u001b[1m   --> STEP: 149\u001b[0m\n",
            "     | > loss_text_ce: 0.18516677618026733  (0.1798342780378841)\n",
            "     | > loss_mel_ce: 3.9811007976531982  (4.266269379814198)\n",
            "     | > loss: 4.166267395019531  (4.446103649651445)\n",
            "\n",
            "\u001b[1m   --> STEP: 150\u001b[0m\n",
            "     | > loss_text_ce: 0.17665904760360718  (0.1798131098349889)\n",
            "     | > loss_mel_ce: 4.331828594207764  (4.266706441243489)\n",
            "     | > loss: 4.508487701416016  (4.446519543329875)\n",
            "\n",
            "\u001b[1m   --> STEP: 151\u001b[0m\n",
            "     | > loss_text_ce: 0.19476883113384247  (0.17991215434690186)\n",
            "     | > loss_mel_ce: 4.01905632019043  (4.265066374216647)\n",
            "     | > loss: 4.213825225830078  (4.444978521359677)\n",
            "\n",
            "\u001b[1m   --> STEP: 152\u001b[0m\n",
            "     | > loss_text_ce: 0.18280071020126343  (0.17993115800383844)\n",
            "     | > loss_mel_ce: 3.9184086322784424  (4.26278573117758)\n",
            "     | > loss: 4.1012091636657715  (4.442716880848534)\n",
            "\n",
            "\u001b[1m   --> STEP: 153\u001b[0m\n",
            "     | > loss_text_ce: 0.17847715318202972  (0.17992165470434948)\n",
            "     | > loss_mel_ce: 4.43664026260376  (4.263922035304548)\n",
            "     | > loss: 4.61511754989624  (4.443843682607015)\n",
            "\n",
            "\u001b[1m   --> STEP: 154\u001b[0m\n",
            "     | > loss_text_ce: 0.23523566126823425  (0.18028083656515395)\n",
            "     | > loss_mel_ce: 4.548730373382568  (4.265771440097263)\n",
            "     | > loss: 4.783966064453125  (4.446052269502119)\n",
            "\n",
            "\u001b[1m   --> STEP: 155\u001b[0m\n",
            "     | > loss_text_ce: 0.1786244660615921  (0.18027015030384066)\n",
            "     | > loss_mel_ce: 4.012024402618408  (4.264134362436108)\n",
            "     | > loss: 4.190649032592773  (4.444404506683349)\n",
            "\n",
            "\u001b[1m   --> STEP: 156\u001b[0m\n",
            "     | > loss_text_ce: 0.18137817084789276  (0.18027725299963587)\n",
            "     | > loss_mel_ce: 4.048827171325684  (4.262754188134118)\n",
            "     | > loss: 4.230205535888672  (4.443031436357742)\n",
            "\n",
            "\u001b[1m   --> STEP: 157\u001b[0m\n",
            "     | > loss_text_ce: 0.17196865379810333  (0.18022433198561336)\n",
            "     | > loss_mel_ce: 4.177250385284424  (4.262209577924884)\n",
            "     | > loss: 4.349218845367432  (4.442433903931053)\n",
            "\n",
            "\u001b[1m   --> STEP: 158\u001b[0m\n",
            "     | > loss_text_ce: 0.17584896087646484  (0.18019663976340355)\n",
            "     | > loss_mel_ce: 4.729371547698975  (4.265166299252568)\n",
            "     | > loss: 4.9052205085754395  (4.445362933074372)\n",
            "\n",
            "\u001b[1m   --> STEP: 159\u001b[0m\n",
            "     | > loss_text_ce: 0.17398568987846375  (0.18015757718551087)\n",
            "     | > loss_mel_ce: 4.254485130310059  (4.265099122089408)\n",
            "     | > loss: 4.428470611572266  (4.445256692058635)\n",
            "\n",
            "\u001b[1m   --> STEP: 160\u001b[0m\n",
            "     | > loss_text_ce: 0.18610705435276031  (0.18019476141780616)\n",
            "     | > loss_mel_ce: 4.387576103210449  (4.265864603221415)\n",
            "     | > loss: 4.573683261871338  (4.446059358119965)\n",
            "\n",
            "\u001b[1m   --> STEP: 161\u001b[0m\n",
            "     | > loss_text_ce: 0.1987667679786682  (0.18031011549582393)\n",
            "     | > loss_mel_ce: 4.123895645141602  (4.264982808450733)\n",
            "     | > loss: 4.322662353515625  (4.445292917097578)\n",
            "\n",
            "\u001b[1m   --> STEP: 162\u001b[0m\n",
            "     | > loss_text_ce: 0.18112973868846893  (0.18031517489824767)\n",
            "     | > loss_mel_ce: 4.254705429077148  (4.264919367837316)\n",
            "     | > loss: 4.435835361480713  (4.445234537124634)\n",
            "\n",
            "\u001b[1m   --> STEP: 163\u001b[0m\n",
            "     | > loss_text_ce: 0.17532168328762054  (0.18028453998039104)\n",
            "     | > loss_mel_ce: 4.155686855316162  (4.264249229723689)\n",
            "     | > loss: 4.331008434295654  (4.444533763487646)\n",
            "\n",
            "\u001b[1m   --> STEP: 164\u001b[0m\n",
            "     | > loss_text_ce: 0.17172887921333313  (0.18023237131717726)\n",
            "     | > loss_mel_ce: 4.012019634246826  (4.262711244385415)\n",
            "     | > loss: 4.183748722076416  (4.442943610796115)\n",
            "\n",
            "\u001b[1m   --> STEP: 165\u001b[0m\n",
            "     | > loss_text_ce: 0.1824912577867508  (0.180246061538205)\n",
            "     | > loss_mel_ce: 3.9604074954986572  (4.260879100452769)\n",
            "     | > loss: 4.1428985595703125  (4.441125155940201)\n",
            "\n",
            "\u001b[1m   --> STEP: 166\u001b[0m\n",
            "     | > loss_text_ce: 0.16883908212184906  (0.180177344794733)\n",
            "     | > loss_mel_ce: 4.2488112449646  (4.260806402528141)\n",
            "     | > loss: 4.41765022277832  (4.440983740680189)\n",
            "\n",
            "\u001b[1m   --> STEP: 167\u001b[0m\n",
            "     | > loss_text_ce: 0.162650927901268  (0.18007239619058052)\n",
            "     | > loss_mel_ce: 4.575503349304199  (4.262690815383087)\n",
            "     | > loss: 4.738154411315918  (4.442763205773816)\n",
            "\n",
            "\u001b[1m   --> STEP: 168\u001b[0m\n",
            "     | > loss_text_ce: 0.18012972176074982  (0.18007273741421248)\n",
            "     | > loss_mel_ce: 4.1977009773254395  (4.262303971108935)\n",
            "     | > loss: 4.377830505371094  (4.442376701604752)\n",
            "\n",
            "\u001b[1m   --> STEP: 169\u001b[0m\n",
            "     | > loss_text_ce: 0.17508868873119354  (0.18004324600188693)\n",
            "     | > loss_mel_ce: 4.164126873016357  (4.261723041534422)\n",
            "     | > loss: 4.3392157554626465  (4.441766281805095)\n",
            "\n",
            "\u001b[1m   --> STEP: 170\u001b[0m\n",
            "     | > loss_text_ce: 0.17323483526706696  (0.18000319652697622)\n",
            "     | > loss_mel_ce: 4.252895832061768  (4.261671116772818)\n",
            "     | > loss: 4.426130771636963  (4.4416743082158705)\n",
            "\n",
            "\u001b[1m   --> STEP: 171\u001b[0m\n",
            "     | > loss_text_ce: 0.1933434009552002  (0.18008120941836933)\n",
            "     | > loss_mel_ce: 3.942335367202759  (4.259803656249017)\n",
            "     | > loss: 4.135678768157959  (4.439884860613192)\n",
            "\n",
            "\u001b[1m   --> STEP: 172\u001b[0m\n",
            "     | > loss_text_ce: 0.17095409333705902  (0.18002814478998963)\n",
            "     | > loss_mel_ce: 4.629666805267334  (4.2619540233944715)\n",
            "     | > loss: 4.800621032714844  (4.4419821639393655)\n",
            "\n",
            "\u001b[1m   --> STEP: 173\u001b[0m\n",
            "     | > loss_text_ce: 0.1760745793581009  (0.18000529181061456)\n",
            "     | > loss_mel_ce: 4.181512355804443  (4.261489042656957)\n",
            "     | > loss: 4.357586860656738  (4.4414943298163445)\n",
            "\n",
            "\u001b[1m   --> STEP: 174\u001b[0m\n",
            "     | > loss_text_ce: 0.17795804142951965  (0.17999352600382668)\n",
            "     | > loss_mel_ce: 4.3736186027526855  (4.262133465416128)\n",
            "     | > loss: 4.551576614379883  (4.4421269866241815)\n",
            "\n",
            "\u001b[1m   --> STEP: 175\u001b[0m\n",
            "     | > loss_text_ce: 0.17793671786785126  (0.17998177281447825)\n",
            "     | > loss_mel_ce: 4.1474456787109375  (4.261478106634955)\n",
            "     | > loss: 4.325382232666016  (4.44145987374442)\n",
            "\n",
            "\u001b[1m   --> STEP: 176\u001b[0m\n",
            "     | > loss_text_ce: 0.19568097591400146  (0.18007097283208917)\n",
            "     | > loss_mel_ce: 4.085709095001221  (4.260479419068854)\n",
            "     | > loss: 4.281390190124512  (4.440550386905671)\n",
            "\n",
            "\u001b[1m   --> STEP: 177\u001b[0m\n",
            "     | > loss_text_ce: 0.1651024967432022  (0.17998640517057002)\n",
            "     | > loss_mel_ce: 4.5617780685424805  (4.262181671325767)\n",
            "     | > loss: 4.7268805503845215  (4.442168071445099)\n",
            "\n",
            "\u001b[1m   --> STEP: 178\u001b[0m\n",
            "     | > loss_text_ce: 0.17653213441371918  (0.1799669991550821)\n",
            "     | > loss_mel_ce: 4.391635417938232  (4.262908939565163)\n",
            "     | > loss: 4.568167686462402  (4.442875934450814)\n",
            "\n",
            "\u001b[1m   --> STEP: 179\u001b[0m\n",
            "     | > loss_text_ce: 0.212900772690773  (0.180150986716734)\n",
            "     | > loss_mel_ce: 4.080488681793213  (4.261889831979845)\n",
            "     | > loss: 4.293389320373535  (4.442040813701779)\n",
            "\n",
            "\u001b[1m   --> STEP: 180\u001b[0m\n",
            "     | > loss_text_ce: 0.18980318307876587  (0.1802046100298564)\n",
            "     | > loss_mel_ce: 4.137628555297852  (4.261199491553834)\n",
            "     | > loss: 4.327431678771973  (4.441404096285503)\n",
            "\n",
            "\u001b[1m   --> STEP: 181\u001b[0m\n",
            "     | > loss_text_ce: 0.17615680396556854  (0.18018224646044045)\n",
            "     | > loss_mel_ce: 4.170767784118652  (4.260699868860822)\n",
            "     | > loss: 4.346924781799316  (4.440882111122596)\n",
            "\n",
            "\u001b[1m   --> STEP: 182\u001b[0m\n",
            "     | > loss_text_ce: 0.17416594922542572  (0.18014918988222609)\n",
            "     | > loss_mel_ce: 4.481499195098877  (4.26191305197202)\n",
            "     | > loss: 4.655664920806885  (4.442062236450531)\n",
            "\n",
            "\u001b[1m   --> STEP: 183\u001b[0m\n",
            "     | > loss_text_ce: 0.17542991042137146  (0.18012340146987169)\n",
            "     | > loss_mel_ce: 4.322325706481934  (4.262243175767157)\n",
            "     | > loss: 4.497755527496338  (4.442366571374279)\n",
            "\n",
            "\u001b[1m   --> STEP: 184\u001b[0m\n",
            "     | > loss_text_ce: 0.17964930832386017  (0.18012082487668685)\n",
            "     | > loss_mel_ce: 4.197152137756348  (4.261889420125795)\n",
            "     | > loss: 4.376801490783691  (4.442010239414548)\n",
            "\n",
            "\u001b[1m   --> STEP: 185\u001b[0m\n",
            "     | > loss_text_ce: 0.21206055581569672  (0.18029347207095175)\n",
            "     | > loss_mel_ce: 4.072085857391357  (4.260863454921825)\n",
            "     | > loss: 4.284146308898926  (4.441156920871221)\n",
            "\n",
            "\u001b[1m   --> STEP: 186\u001b[0m\n",
            "     | > loss_text_ce: 0.17120112478733063  (0.1802445884834054)\n",
            "     | > loss_mel_ce: 4.10380220413208  (4.260019039594998)\n",
            "     | > loss: 4.275003433227539  (4.440263622550557)\n",
            "\n",
            "\u001b[1m   --> STEP: 187\u001b[0m\n",
            "     | > loss_text_ce: 0.17461776733398438  (0.1802144985307347)\n",
            "     | > loss_mel_ce: 4.395444869995117  (4.260743241896603)\n",
            "     | > loss: 4.570062637329102  (4.440957734929052)\n",
            "\n",
            "\u001b[1m   --> STEP: 188\u001b[0m\n",
            "     | > loss_text_ce: 0.1936182975769043  (0.18028579533417177)\n",
            "     | > loss_mel_ce: 4.418292999267578  (4.261581272520917)\n",
            "     | > loss: 4.611911296844482  (4.4418670623860494)\n",
            "\n",
            "\u001b[1m   --> STEP: 189\u001b[0m\n",
            "     | > loss_text_ce: 0.18764853477478027  (0.18032475162750833)\n",
            "     | > loss_mel_ce: 4.146791934967041  (4.260973921528568)\n",
            "     | > loss: 4.334440231323242  (4.4412986664545)\n",
            "\n",
            "\u001b[1m   --> STEP: 190\u001b[0m\n",
            "     | > loss_text_ce: 0.18069884181022644  (0.18032672052320683)\n",
            "     | > loss_mel_ce: 4.238363265991211  (4.260854918078371)\n",
            "     | > loss: 4.41906213760376  (4.441181632092128)\n",
            "\n",
            "\u001b[1m   --> STEP: 191\u001b[0m\n",
            "     | > loss_text_ce: 0.17967922985553741  (0.1803233305197112)\n",
            "     | > loss_mel_ce: 4.612391948699951  (4.262695426092096)\n",
            "     | > loss: 4.792071342468262  (4.443018750994621)\n",
            "\n",
            "\u001b[1m   --> STEP: 192\u001b[0m\n",
            "     | > loss_text_ce: 0.1798732727766037  (0.18032098646896585)\n",
            "     | > loss_mel_ce: 4.20467472076416  (4.2623932349185125)\n",
            "     | > loss: 4.384548187255859  (4.442714216808481)\n",
            "\n",
            "\u001b[1m   --> STEP: 193\u001b[0m\n",
            "     | > loss_text_ce: 0.16749601066112518  (0.18025453581711176)\n",
            "     | > loss_mel_ce: 4.522360324859619  (4.26374021465914)\n",
            "     | > loss: 4.68985652923584  (4.443994746924685)\n",
            "\n",
            "\u001b[1m   --> STEP: 194\u001b[0m\n",
            "     | > loss_text_ce: 0.1831875592470169  (0.18026965449458549)\n",
            "     | > loss_mel_ce: 4.5274834632873535  (4.265099715940728)\n",
            "     | > loss: 4.7106709480285645  (4.445369366518004)\n",
            "\n",
            "\u001b[1m   --> STEP: 195\u001b[0m\n",
            "     | > loss_text_ce: 0.17569999396800995  (0.18024622033803894)\n",
            "     | > loss_mel_ce: 4.325530052185059  (4.265409615100955)\n",
            "     | > loss: 4.501230239868164  (4.445655832535184)\n",
            "\n",
            "\u001b[1m   --> STEP: 196\u001b[0m\n",
            "     | > loss_text_ce: 0.19046097993850708  (0.1802983364584495)\n",
            "     | > loss_mel_ce: 3.8552653789520264  (4.263317042467542)\n",
            "     | > loss: 4.045726299285889  (4.443615375732892)\n",
            "\n",
            "\u001b[1m   --> STEP: 197\u001b[0m\n",
            "     | > loss_text_ce: 0.1724792867898941  (0.1802586458509949)\n",
            "     | > loss_mel_ce: 4.071929454803467  (4.262345531870262)\n",
            "     | > loss: 4.24440860748291  (4.442604173863603)\n",
            "\n",
            "\u001b[1m   --> STEP: 198\u001b[0m\n",
            "     | > loss_text_ce: 0.1945449411869049  (0.18033079885774192)\n",
            "     | > loss_mel_ce: 3.920344352722168  (4.260618253187697)\n",
            "     | > loss: 4.114889144897461  (4.440949047454683)\n",
            "\n",
            "\u001b[1m   --> STEP: 199\u001b[0m\n",
            "     | > loss_text_ce: 0.16407881677150726  (0.1802491306060523)\n",
            "     | > loss_mel_ce: 4.189626693725586  (4.260261511682862)\n",
            "     | > loss: 4.353705406188965  (4.440510637197066)\n",
            "\n",
            "\u001b[1m   --> STEP: 200\u001b[0m\n",
            "     | > loss_text_ce: 0.17738889157772064  (0.18023482941091065)\n",
            "     | > loss_mel_ce: 4.400116920471191  (4.260960788726804)\n",
            "     | > loss: 4.577505588531494  (4.441195611953739)\n",
            "\n",
            "\u001b[1m   --> STEP: 201\u001b[0m\n",
            "     | > loss_text_ce: 0.18425504863262177  (0.18025483050156593)\n",
            "     | > loss_mel_ce: 4.245004177093506  (4.260881402599275)\n",
            "     | > loss: 4.429259300231934  (4.441136227318307)\n",
            "\n",
            "\u001b[1m   --> STEP: 202\u001b[0m\n",
            "     | > loss_text_ce: 0.1787819266319275  (0.18024753889825088)\n",
            "     | > loss_mel_ce: 4.354733943939209  (4.261346019140562)\n",
            "     | > loss: 4.533515930175781  (4.441593552579977)\n",
            "\n",
            "\u001b[1m   --> STEP: 203\u001b[0m\n",
            "     | > loss_text_ce: 0.17585551738739014  (0.18022590332430574)\n",
            "     | > loss_mel_ce: 4.106400489807129  (4.260582740670939)\n",
            "     | > loss: 4.282256126403809  (4.4408086391505375)\n",
            "\n",
            "\u001b[1m   --> STEP: 204\u001b[0m\n",
            "     | > loss_text_ce: 0.1716976761817932  (0.18018409828929344)\n",
            "     | > loss_mel_ce: 4.16676664352417  (4.260122857841788)\n",
            "     | > loss: 4.338464260101318  (4.440306951017943)\n",
            "\n",
            "\u001b[1m   --> STEP: 205\u001b[0m\n",
            "     | > loss_text_ce: 0.20671813189983368  (0.18031353259958877)\n",
            "     | > loss_mel_ce: 4.108678340911865  (4.259384104100666)\n",
            "     | > loss: 4.315396308898926  (4.439697630812485)\n",
            "\n",
            "\u001b[1m   --> STEP: 206\u001b[0m\n",
            "     | > loss_text_ce: 0.17001137137413025  (0.18026352210820304)\n",
            "     | > loss_mel_ce: 4.247323513031006  (4.259325557542075)\n",
            "     | > loss: 4.417335033416748  (4.439589074514448)\n",
            "\n",
            "\u001b[1m   --> STEP: 207\u001b[0m\n",
            "     | > loss_text_ce: 0.1733541041612625  (0.18023014327754147)\n",
            "     | > loss_mel_ce: 4.479310512542725  (4.260388286793286)\n",
            "     | > loss: 4.652664661407471  (4.440618425175767)\n",
            "\n",
            "\u001b[1m   --> STEP: 208\u001b[0m\n",
            "     | > loss_text_ce: 0.17046600580215454  (0.18018320030890983)\n",
            "     | > loss_mel_ce: 4.247422218322754  (4.2603259499256385)\n",
            "     | > loss: 4.417888164520264  (4.440509145076462)\n",
            "\n",
            "\u001b[1m   --> STEP: 209\u001b[0m\n",
            "     | > loss_text_ce: 0.1871037483215332  (0.18021631297882668)\n",
            "     | > loss_mel_ce: 4.146579742431641  (4.2597817096983945)\n",
            "     | > loss: 4.333683490753174  (4.439998017543815)\n",
            "\n",
            "\u001b[1m   --> STEP: 210\u001b[0m\n",
            "     | > loss_text_ce: 0.17186976969242096  (0.1801765675346057)\n",
            "     | > loss_mel_ce: 4.362763404846191  (4.260272098722908)\n",
            "     | > loss: 4.534633159637451  (4.440448661077594)\n",
            "\n",
            "\u001b[1m   --> STEP: 211\u001b[0m\n",
            "     | > loss_text_ce: 0.17769016325473785  (0.18016478362806604)\n",
            "     | > loss_mel_ce: 4.6304240226745605  (4.262026373244005)\n",
            "     | > loss: 4.808114051818848  (4.442191151081108)\n",
            "\n",
            "\u001b[1m   --> STEP: 212\u001b[0m\n",
            "     | > loss_text_ce: 0.18082502484321594  (0.18016789797342053)\n",
            "     | > loss_mel_ce: 3.9470722675323486  (4.260540740669893)\n",
            "     | > loss: 4.127897262573242  (4.4407086327390894)\n",
            "\n",
            "\u001b[1m   --> STEP: 213\u001b[0m\n",
            "     | > loss_text_ce: 0.1770363748073578  (0.18015319598672538)\n",
            "     | > loss_mel_ce: 4.400416374206543  (4.261197433785089)\n",
            "     | > loss: 4.577452659606934  (4.441350623475558)\n",
            "\n",
            "\u001b[1m   --> STEP: 214\u001b[0m\n",
            "     | > loss_text_ce: 0.1752195656299591  (0.18013014163926386)\n",
            "     | > loss_mel_ce: 4.194743633270264  (4.260886902006982)\n",
            "     | > loss: 4.3699631690979  (4.441017037240148)\n",
            "\n",
            "\u001b[1m   --> STEP: 215\u001b[0m\n",
            "     | > loss_text_ce: 0.18623562157154083  (0.18015853922034422)\n",
            "     | > loss_mel_ce: 4.19910192489624  (4.2605995300204205)\n",
            "     | > loss: 4.3853373527526855  (4.440758061963463)\n",
            "\n",
            "\u001b[1m   --> STEP: 216\u001b[0m\n",
            "     | > loss_text_ce: 0.17234951257705688  (0.1801223863192179)\n",
            "     | > loss_mel_ce: 4.561434745788574  (4.261992285648977)\n",
            "     | > loss: 4.733784198760986  (4.442114664448637)\n",
            "\n",
            "\u001b[1m   --> STEP: 217\u001b[0m\n",
            "     | > loss_text_ce: 0.18172894418239594  (0.18012978981167493)\n",
            "     | > loss_mel_ce: 4.425320625305176  (4.262744950808682)\n",
            "     | > loss: 4.607049465179443  (4.4428747326547695)\n",
            "\n",
            "\u001b[1m   --> STEP: 218\u001b[0m\n",
            "     | > loss_text_ce: 0.1735685169696808  (0.1800996922298309)\n",
            "     | > loss_mel_ce: 4.210375785827637  (4.262504725281246)\n",
            "     | > loss: 4.383944511413574  (4.442604410538984)\n",
            "\n",
            "\u001b[1m   --> STEP: 219\u001b[0m\n",
            "     | > loss_text_ce: 0.1866527497768402  (0.18012961486703188)\n",
            "     | > loss_mel_ce: 4.036534786224365  (4.261472899075506)\n",
            "     | > loss: 4.223187446594238  (4.441602506594031)\n",
            "\n",
            "\u001b[1m   --> STEP: 220\u001b[0m\n",
            "     | > loss_text_ce: 0.18376298248767853  (0.18014613017439846)\n",
            "     | > loss_mel_ce: 4.377886772155762  (4.2620020530440526)\n",
            "     | > loss: 4.561649799346924  (4.442148176106544)\n",
            "\n",
            "\u001b[1m   --> STEP: 221\u001b[0m\n",
            "     | > loss_text_ce: 0.171404629945755  (0.18010657587472134)\n",
            "     | > loss_mel_ce: 4.6013031005859375  (4.263537351901708)\n",
            "     | > loss: 4.772707939147949  (4.443643921640668)\n",
            "\n",
            "\u001b[1m   --> STEP: 222\u001b[0m\n",
            "     | > loss_text_ce: 0.17149367928504944  (0.18006777904323631)\n",
            "     | > loss_mel_ce: 4.187804222106934  (4.263196211677407)\n",
            "     | > loss: 4.359297752380371  (4.443263983941297)\n",
            "\n",
            "\u001b[1m   --> STEP: 223\u001b[0m\n",
            "     | > loss_text_ce: 0.17464977502822876  (0.18004348306110624)\n",
            "     | > loss_mel_ce: 4.600552558898926  (4.2647090204093425)\n",
            "     | > loss: 4.77520227432251  (4.4447524964542175)\n",
            "\n",
            "\u001b[1m   --> STEP: 224\u001b[0m\n",
            "     | > loss_text_ce: 0.18658742308616638  (0.18007269707907528)\n",
            "     | > loss_mel_ce: 4.228573322296143  (4.26454770032848)\n",
            "     | > loss: 4.415160655975342  (4.444620390023508)\n",
            "\n",
            "\u001b[1m   --> STEP: 225\u001b[0m\n",
            "     | > loss_text_ce: 0.1732870191335678  (0.18004253851042856)\n",
            "     | > loss_mel_ce: 4.216699600219727  (4.264335042105775)\n",
            "     | > loss: 4.389986515045166  (4.444377572801382)\n",
            "\n",
            "\u001b[1m   --> STEP: 226\u001b[0m\n",
            "     | > loss_text_ce: 0.17569687962532043  (0.18002330993129093)\n",
            "     | > loss_mel_ce: 4.072113037109375  (4.263484502260658)\n",
            "     | > loss: 4.247809886932373  (4.443507804279838)\n",
            "\n",
            "\u001b[1m   --> STEP: 227\u001b[0m\n",
            "     | > loss_text_ce: 0.17034703493118286  (0.1799806831691759)\n",
            "     | > loss_mel_ce: 4.293447017669678  (4.263616495720609)\n",
            "     | > loss: 4.463794231414795  (4.443597171800256)\n",
            "\n",
            "\u001b[1m   --> STEP: 228\u001b[0m\n",
            "     | > loss_text_ce: 0.18631455302238464  (0.18000846330011103)\n",
            "     | > loss_mel_ce: 4.335752010345459  (4.263932879556683)\n",
            "     | > loss: 4.522066593170166  (4.443941335929072)\n",
            "\n",
            "\u001b[1m   --> STEP: 229\u001b[0m\n",
            "     | > loss_text_ce: 0.18678851425647736  (0.18003807050952747)\n",
            "     | > loss_mel_ce: 4.384644031524658  (4.264460002491041)\n",
            "     | > loss: 4.571432590484619  (4.44449806629831)\n",
            "\n",
            "\u001b[1m   --> STEP: 230\u001b[0m\n",
            "     | > loss_text_ce: 0.17623326182365417  (0.18002152786306716)\n",
            "     | > loss_mel_ce: 4.248226165771484  (4.26438942059226)\n",
            "     | > loss: 4.424459457397461  (4.4444109419117845)\n",
            "\n",
            "\u001b[1m   --> STEP: 231\u001b[0m\n",
            "     | > loss_text_ce: 0.1865570843219757  (0.18004982031527023)\n",
            "     | > loss_mel_ce: 4.255559921264648  (4.264351197651448)\n",
            "     | > loss: 4.442117214202881  (4.444401012354603)\n",
            "\n",
            "\u001b[1m   --> STEP: 232\u001b[0m\n",
            "     | > loss_text_ce: 0.17560748755931854  (0.18003067232925318)\n",
            "     | > loss_mel_ce: 4.156603813171387  (4.2638867692700675)\n",
            "     | > loss: 4.332211494445801  (4.443917436846375)\n",
            "\n",
            "\u001b[1m   --> STEP: 233\u001b[0m\n",
            "     | > loss_text_ce: 0.17413555085659027  (0.18000537137872674)\n",
            "     | > loss_mel_ce: 4.157501697540283  (4.263430180979381)\n",
            "     | > loss: 4.331637382507324  (4.443435548201143)\n",
            "\n",
            "\u001b[1m   --> STEP: 234\u001b[0m\n",
            "     | > loss_text_ce: 0.16399317979812622  (0.17993694320957887)\n",
            "     | > loss_mel_ce: 4.900008201599121  (4.266150599870919)\n",
            "     | > loss: 5.064001560211182  (4.446087539705459)\n",
            "\n",
            "\u001b[1m   --> STEP: 235\u001b[0m\n",
            "     | > loss_text_ce: 0.17425990104675293  (0.17991278558335408)\n",
            "     | > loss_mel_ce: 4.197527885437012  (4.265858588320136)\n",
            "     | > loss: 4.371788024902344  (4.4457713715573615)\n",
            "\n",
            "\u001b[1m   --> STEP: 236\u001b[0m\n",
            "     | > loss_text_ce: 0.17481137812137604  (0.17989116945004063)\n",
            "     | > loss_mel_ce: 4.275701999664307  (4.265900297690239)\n",
            "     | > loss: 4.4505133628845215  (4.445791464740951)\n",
            "\n",
            "\u001b[1m   --> STEP: 237\u001b[0m\n",
            "     | > loss_text_ce: 0.18586930632591248  (0.17991639365626794)\n",
            "     | > loss_mel_ce: 4.311222076416016  (4.266091528824103)\n",
            "     | > loss: 4.497091293334961  (4.446007919713922)\n",
            "\n",
            "\u001b[1m   --> STEP: 238\u001b[0m\n",
            "     | > loss_text_ce: 0.17179937660694122  (0.1798822885426153)\n",
            "     | > loss_mel_ce: 4.223275184631348  (4.265911628218251)\n",
            "     | > loss: 4.395074367523193  (4.445793913192111)\n",
            "\n",
            "\u001b[1m   --> STEP: 239\u001b[0m\n",
            "     | > loss_text_ce: 0.19889423251152039  (0.17996183642533037)\n",
            "     | > loss_mel_ce: 4.228211879730225  (4.265753888684828)\n",
            "     | > loss: 4.427105903625488  (4.445715720683465)\n",
            "\n",
            "\u001b[1m   --> STEP: 240\u001b[0m\n",
            "     | > loss_text_ce: 0.18668203055858612  (0.17998983723421894)\n",
            "     | > loss_mel_ce: 4.085509300231934  (4.265002869566275)\n",
            "     | > loss: 4.272191524505615  (4.44499270319939)\n",
            "\n",
            "\u001b[1m   --> STEP: 241\u001b[0m\n",
            "     | > loss_text_ce: 0.2052377611398697  (0.18009460040395195)\n",
            "     | > loss_mel_ce: 3.7298309803009033  (4.2627822393203605)\n",
            "     | > loss: 3.9350688457489014  (4.442876836570965)\n",
            "\n",
            "\u001b[1m   --> STEP: 242\u001b[0m\n",
            "     | > loss_text_ce: 0.17361672222614288  (0.1800678323123081)\n",
            "     | > loss_mel_ce: 4.370362281799316  (4.263226784950439)\n",
            "     | > loss: 4.543979167938232  (4.443294614799756)\n",
            "\n",
            "\u001b[1m   --> STEP: 243\u001b[0m\n",
            "     | > loss_text_ce: 0.19560803472995758  (0.18013178376258648)\n",
            "     | > loss_mel_ce: 3.94203519821167  (4.261905008873325)\n",
            "     | > loss: 4.137643337249756  (4.442036790612307)\n",
            "\n",
            "\u001b[1m   --> STEP: 244\u001b[0m\n",
            "     | > loss_text_ce: 0.17164939641952515  (0.18009701988003296)\n",
            "     | > loss_mel_ce: 4.388187408447266  (4.262422559691251)\n",
            "     | > loss: 4.5598368644714355  (4.442519577800254)\n",
            "\n",
            "\u001b[1m   --> STEP: 245\u001b[0m\n",
            "     | > loss_text_ce: 0.19925935566425323  (0.18017523349547876)\n",
            "     | > loss_mel_ce: 4.230931282043457  (4.262294023864117)\n",
            "     | > loss: 4.430190563201904  (4.442469255291689)\n",
            "\n",
            "\u001b[1m   --> STEP: 246\u001b[0m\n",
            "     | > loss_text_ce: 0.18131782114505768  (0.18017987816072095)\n",
            "     | > loss_mel_ce: 4.336320877075195  (4.2625949460316415)\n",
            "     | > loss: 4.517638683319092  (4.4427748220722885)\n",
            "\n",
            "\u001b[1m   --> STEP: 247\u001b[0m\n",
            "     | > loss_text_ce: 0.18119849264621735  (0.18018400210600635)\n",
            "     | > loss_mel_ce: 4.221076011657715  (4.262426853179925)\n",
            "     | > loss: 4.4022746086120605  (4.442610853596741)\n",
            "\n",
            "\u001b[1m   --> STEP: 248\u001b[0m\n",
            "     | > loss_text_ce: 0.1900215893983841  (0.18022366979670143)\n",
            "     | > loss_mel_ce: 3.901655435562134  (4.260972129721789)\n",
            "     | > loss: 4.091677188873291  (4.44119579849705)\n",
            "\n",
            "\u001b[1m   --> STEP: 249\u001b[0m\n",
            "     | > loss_text_ce: 0.17498937249183655  (0.1802026485223847)\n",
            "     | > loss_mel_ce: 4.512628078460693  (4.261982796182588)\n",
            "     | > loss: 4.687617301940918  (4.442185443089194)\n",
            "\n",
            "\u001b[1m   --> STEP: 250\u001b[0m\n",
            "     | > loss_text_ce: 0.16972030699253082  (0.18016071915626528)\n",
            "     | > loss_mel_ce: 4.1854729652404785  (4.26167675685882)\n",
            "     | > loss: 4.355193138122559  (4.441837473869327)\n",
            "\n",
            "\u001b[1m   --> STEP: 251\u001b[0m\n",
            "     | > loss_text_ce: 0.19632430374622345  (0.1802251159076197)\n",
            "     | > loss_mel_ce: 4.046637535095215  (4.260820026891634)\n",
            "     | > loss: 4.242961883544922  (4.441045140840147)\n",
            "\n",
            "\u001b[1m   --> STEP: 252\u001b[0m\n",
            "     | > loss_text_ce: 0.17492109537124634  (0.18020406820707854)\n",
            "     | > loss_mel_ce: 4.336111068725586  (4.261118800867165)\n",
            "     | > loss: 4.5110321044921875  (4.4413228668863844)\n",
            "\n",
            "\u001b[1m   --> STEP: 253\u001b[0m\n",
            "     | > loss_text_ce: 0.1875470131635666  (0.18023309170493026)\n",
            "     | > loss_mel_ce: 4.720863342285156  (4.2629359729676315)\n",
            "     | > loss: 4.908410549163818  (4.443169063259023)\n",
            "\n",
            "\u001b[1m   --> STEP: 254\u001b[0m\n",
            "     | > loss_text_ce: 0.1877482682466507  (0.18026267901414963)\n",
            "     | > loss_mel_ce: 4.33597469329834  (4.263223526984682)\n",
            "     | > loss: 4.523723125457764  (4.443486205236183)\n",
            "\n",
            "\u001b[1m   --> STEP: 255\u001b[0m\n",
            "     | > loss_text_ce: 0.1708276867866516  (0.18022567904463005)\n",
            "     | > loss_mel_ce: 4.545986652374268  (4.264332401986209)\n",
            "     | > loss: 4.7168145179748535  (4.4445580809724134)\n",
            "\n",
            "\u001b[1m   --> STEP: 256\u001b[0m\n",
            "     | > loss_text_ce: 0.1727241426706314  (0.1801963761681691)\n",
            "     | > loss_mel_ce: 4.45205020904541  (4.265065674670034)\n",
            "     | > loss: 4.62477445602417  (4.445262051187459)\n",
            "\n",
            "\u001b[1m   --> STEP: 257\u001b[0m\n",
            "     | > loss_text_ce: 0.17439986765384674  (0.18017382166033127)\n",
            "     | > loss_mel_ce: 4.353764533996582  (4.265410806418386)\n",
            "     | > loss: 4.528164386749268  (4.445584628368634)\n",
            "\n",
            "\u001b[1m   --> STEP: 258\u001b[0m\n",
            "     | > loss_text_ce: 0.17677322030067444  (0.18016064103490625)\n",
            "     | > loss_mel_ce: 4.510593414306641  (4.266361126604)\n",
            "     | > loss: 4.687366485595703  (4.446521767350134)\n",
            "\n",
            "\u001b[1m   --> STEP: 259\u001b[0m\n",
            "     | > loss_text_ce: 0.17409121990203857  (0.1801372069764782)\n",
            "     | > loss_mel_ce: 4.359628677368164  (4.2667212329776065)\n",
            "     | > loss: 4.533720016479492  (4.446858440126695)\n",
            "\n",
            "\u001b[1m   --> STEP: 260\u001b[0m\n",
            "     | > loss_text_ce: 0.17588897049427032  (0.18012086760539278)\n",
            "     | > loss_mel_ce: 4.247753143310547  (4.26664827878658)\n",
            "     | > loss: 4.423642158508301  (4.446769146735855)\n",
            "\n",
            "\u001b[1m   --> STEP: 261\u001b[0m\n",
            "     | > loss_text_ce: 0.1843336671590805  (0.18013700859985135)\n",
            "     | > loss_mel_ce: 4.32253360748291  (4.266862398819899)\n",
            "     | > loss: 4.506867408752441  (4.446999408276149)\n",
            "\n",
            "\u001b[1m   --> STEP: 262\u001b[0m\n",
            "     | > loss_text_ce: 0.18110166490077972  (0.1801406904941297)\n",
            "     | > loss_mel_ce: 4.365085124969482  (4.267237294721232)\n",
            "     | > loss: 4.546186923980713  (4.447377986580364)\n",
            "\n",
            "\u001b[1m   --> STEP: 263\u001b[0m\n",
            "     | > loss_text_ce: 0.17722027003765106  (0.1801295862338389)\n",
            "     | > loss_mel_ce: 4.381216049194336  (4.267670674015807)\n",
            "     | > loss: 4.558436393737793  (4.44780026189275)\n",
            "\n",
            "\u001b[1m   --> STEP: 264\u001b[0m\n",
            "     | > loss_text_ce: 0.1778765618801117  (0.18012105205068085)\n",
            "     | > loss_mel_ce: 3.987406015396118  (4.266609065460429)\n",
            "     | > loss: 4.165282726287842  (4.446730119712428)\n",
            "\n",
            "\u001b[1m   --> STEP: 265\u001b[0m\n",
            "     | > loss_text_ce: 0.1945151686668396  (0.1801753694718739)\n",
            "     | > loss_mel_ce: 4.124070167541504  (4.2660711828267734)\n",
            "     | > loss: 4.318585395812988  (4.4462465547165815)\n",
            "\n",
            "\u001b[1m   --> STEP: 266\u001b[0m\n",
            "     | > loss_text_ce: 0.17741675674915314  (0.1801649987473524)\n",
            "     | > loss_mel_ce: 4.596424102783203  (4.267313111097286)\n",
            "     | > loss: 4.77384090423584  (4.447478112421541)\n",
            "\n",
            "\u001b[1m   --> STEP: 267\u001b[0m\n",
            "     | > loss_text_ce: 0.1890893578529358  (0.18019842331329092)\n",
            "     | > loss_mel_ce: 4.274692058563232  (4.267340747604649)\n",
            "     | > loss: 4.463781356811523  (4.447539173261953)\n",
            "\n",
            "\u001b[1m   --> STEP: 268\u001b[0m\n",
            "     | > loss_text_ce: 0.1768673062324524  (0.1801859937719445)\n",
            "     | > loss_mel_ce: 4.666353702545166  (4.2688296019141285)\n",
            "     | > loss: 4.843221187591553  (4.449015598688556)\n",
            "\n",
            "\u001b[1m   --> STEP: 269\u001b[0m\n",
            "     | > loss_text_ce: 0.1845434308052063  (0.18020219242262578)\n",
            "     | > loss_mel_ce: 4.369073867797852  (4.2692022571776365)\n",
            "     | > loss: 4.553617477416992  (4.44940445325632)\n",
            "\n",
            "\u001b[1m   --> STEP: 270\u001b[0m\n",
            "     | > loss_text_ce: 0.18168766796588898  (0.18020769418389712)\n",
            "     | > loss_mel_ce: 4.121260166168213  (4.268654323507231)\n",
            "     | > loss: 4.302947998046875  (4.448862021940729)\n",
            "\n",
            "\u001b[1m   --> STEP: 271\u001b[0m\n",
            "     | > loss_text_ce: 0.18024806678295135  (0.18020784316027738)\n",
            "     | > loss_mel_ce: 4.0008087158203125  (4.2676659633312655)\n",
            "     | > loss: 4.181056976318359  (4.447873811440278)\n",
            "\n",
            "\u001b[1m   --> STEP: 272\u001b[0m\n",
            "     | > loss_text_ce: 0.16836488246917725  (0.18016430286361892)\n",
            "     | > loss_mel_ce: 4.360475540161133  (4.268007175010787)\n",
            "     | > loss: 4.5288405418396  (4.448171483243216)\n",
            "\n",
            "\u001b[1m   --> STEP: 273\u001b[0m\n",
            "     | > loss_text_ce: 0.1685679405927658  (0.1801218253461433)\n",
            "     | > loss_mel_ce: 4.415701866149902  (4.2685481812054356)\n",
            "     | > loss: 4.584270000457764  (4.448670012610303)\n",
            "\n",
            "\u001b[1m   --> STEP: 274\u001b[0m\n",
            "     | > loss_text_ce: 0.18018361926078796  (0.18012205087137922)\n",
            "     | > loss_mel_ce: 4.109128952026367  (4.267966359201132)\n",
            "     | > loss: 4.289312362670898  (4.44808841534775)\n",
            "\n",
            "\u001b[1m   --> STEP: 275\u001b[0m\n",
            "     | > loss_text_ce: 0.1952369660139084  (0.18017701419917026)\n",
            "     | > loss_mel_ce: 4.0725250244140625  (4.26725566343827)\n",
            "     | > loss: 4.267762184143066  (4.447432683597915)\n",
            "\n",
            "\u001b[1m   --> STEP: 276\u001b[0m\n",
            "     | > loss_text_ce: 0.17408567667007446  (0.18015494413565902)\n",
            "     | > loss_mel_ce: 4.374001502990723  (4.267642423726504)\n",
            "     | > loss: 4.548087120056152  (4.447797373585083)\n",
            "\n",
            "\u001b[1m   --> STEP: 277\u001b[0m\n",
            "     | > loss_text_ce: 0.17471139132976532  (0.18013529232047532)\n",
            "     | > loss_mel_ce: 4.294590950012207  (4.267739710825008)\n",
            "     | > loss: 4.469302177429199  (4.447875008256)\n",
            "\n",
            "\u001b[1m   --> STEP: 278\u001b[0m\n",
            "     | > loss_text_ce: 0.17659249901771545  (0.18012254845967401)\n",
            "     | > loss_mel_ce: 4.280377388000488  (4.267785170095423)\n",
            "     | > loss: 4.456969738006592  (4.447907723111219)\n",
            "\n",
            "\u001b[1m   --> STEP: 279\u001b[0m\n",
            "     | > loss_text_ce: 0.16887840628623962  (0.180082246874823)\n",
            "     | > loss_mel_ce: 4.040179252624512  (4.2669693782765306)\n",
            "     | > loss: 4.209057807922363  (4.4470516302252365)\n",
            "\n",
            "\u001b[1m   --> STEP: 280\u001b[0m\n",
            "     | > loss_text_ce: 0.16477417945861816  (0.1800275752054794)\n",
            "     | > loss_mel_ce: 4.324975490570068  (4.267176542963293)\n",
            "     | > loss: 4.489749908447266  (4.44720412407603)\n",
            "\n",
            "\u001b[1m   --> STEP: 281\u001b[0m\n",
            "     | > loss_text_ce: 0.17311377823352814  (0.18000297094579273)\n",
            "     | > loss_mel_ce: 4.457604885101318  (4.267854223896169)\n",
            "     | > loss: 4.63071870803833  (4.447857200887284)\n",
            "\n",
            "\u001b[1m   --> STEP: 282\u001b[0m\n",
            "     | > loss_text_ce: 0.19013121724128723  (0.18003888671279805)\n",
            "     | > loss_mel_ce: 4.179636478424072  (4.2675413950115155)\n",
            "     | > loss: 4.369767665863037  (4.447580287642517)\n",
            "\n",
            "\u001b[1m   --> STEP: 283\u001b[0m\n",
            "     | > loss_text_ce: 0.17426395416259766  (0.18001848059071252)\n",
            "     | > loss_mel_ce: 4.462852954864502  (4.26823154186612)\n",
            "     | > loss: 4.6371169090271  (4.448250028354124)\n",
            "\n",
            "\u001b[1m   --> STEP: 284\u001b[0m\n",
            "     | > loss_text_ce: 0.20397751033306122  (0.18010284337149546)\n",
            "     | > loss_mel_ce: 3.7888216972351074  (4.266543479032912)\n",
            "     | > loss: 3.9927992820739746  (4.446646328543278)\n",
            "\n",
            "\u001b[1m   --> STEP: 285\u001b[0m\n",
            "     | > loss_text_ce: 0.1791076809167862  (0.18009935157340878)\n",
            "     | > loss_mel_ce: 4.28426456451416  (4.2666056582802145)\n",
            "     | > loss: 4.463372230529785  (4.44670501591867)\n",
            "\n",
            "\u001b[1m   --> STEP: 286\u001b[0m\n",
            "     | > loss_text_ce: 0.16711026430130005  (0.18005393518434548)\n",
            "     | > loss_mel_ce: 4.526857852935791  (4.267515630988801)\n",
            "     | > loss: 4.693968296051025  (4.447569572842209)\n",
            "\n",
            "\u001b[1m   --> STEP: 287\u001b[0m\n",
            "     | > loss_text_ce: 0.16400885581970215  (0.17999802898446868)\n",
            "     | > loss_mel_ce: 4.4381208419799805  (4.2681100742326725)\n",
            "     | > loss: 4.602129936218262  (4.448108110693694)\n",
            "\n",
            "\u001b[1m   --> STEP: 288\u001b[0m\n",
            "     | > loss_text_ce: 0.17629939317703247  (0.17998518649902617)\n",
            "     | > loss_mel_ce: 4.327236175537109  (4.268315373195535)\n",
            "     | > loss: 4.503535747528076  (4.448300567766036)\n",
            "\n",
            "\u001b[1m   --> STEP: 289\u001b[0m\n",
            "     | > loss_text_ce: 0.1678752452135086  (0.17994328358800363)\n",
            "     | > loss_mel_ce: 4.481470584869385  (4.2690529344816035)\n",
            "     | > loss: 4.649345874786377  (4.448996226267836)\n",
            "\n",
            "\u001b[1m   --> STEP: 290\u001b[0m\n",
            "     | > loss_text_ce: 0.18952436745166779  (0.17997632180822318)\n",
            "     | > loss_mel_ce: 4.180978298187256  (4.2687492288392095)\n",
            "     | > loss: 4.370502471923828  (4.448725558149408)\n",
            "\n",
            "\u001b[1m   --> STEP: 291\u001b[0m\n",
            "     | > loss_text_ce: 0.1761680394411087  (0.17996323492723654)\n",
            "     | > loss_mel_ce: 4.485533714294434  (4.269494192706754)\n",
            "     | > loss: 4.661701679229736  (4.449457434854152)\n",
            "\n",
            "\u001b[1m   --> STEP: 292\u001b[0m\n",
            "     | > loss_text_ce: 0.17941312491893768  (0.17996135098885196)\n",
            "     | > loss_mel_ce: 4.303463935852051  (4.269610527443552)\n",
            "     | > loss: 4.482877254486084  (4.449571886291247)\n",
            "\n",
            "\u001b[1m   --> STEP: 293\u001b[0m\n",
            "     | > loss_text_ce: 0.1843593567609787  (0.1799763612474599)\n",
            "     | > loss_mel_ce: 4.250589370727539  (4.269545608819948)\n",
            "     | > loss: 4.434948921203613  (4.449521978560573)\n",
            "\n",
            "\u001b[1m   --> STEP: 294\u001b[0m\n",
            "     | > loss_text_ce: 0.18312497437000275  (0.17998707081590393)\n",
            "     | > loss_mel_ce: 3.9887876510620117  (4.268590649779955)\n",
            "     | > loss: 4.171912670135498  (4.448577729212188)\n",
            "\n",
            "\u001b[1m   --> STEP: 295\u001b[0m\n",
            "     | > loss_text_ce: 0.18049417436122894  (0.17998878981097283)\n",
            "     | > loss_mel_ce: 4.224959373474121  (4.26844274714841)\n",
            "     | > loss: 4.405453681945801  (4.448431546001115)\n",
            "\n",
            "\u001b[1m   --> STEP: 296\u001b[0m\n",
            "     | > loss_text_ce: 0.1853894144296646  (0.18000703516441435)\n",
            "     | > loss_mel_ce: 4.301006317138672  (4.268552759209188)\n",
            "     | > loss: 4.486395835876465  (4.44855980373718)\n",
            "\n",
            "\u001b[1m   --> STEP: 297\u001b[0m\n",
            "     | > loss_text_ce: 0.18525788187980652  (0.18002471478298468)\n",
            "     | > loss_mel_ce: 3.7495434284210205  (4.266805253044918)\n",
            "     | > loss: 3.9348013401031494  (4.446829977260298)\n",
            "\n",
            "\u001b[1m   --> STEP: 298\u001b[0m\n",
            "     | > loss_text_ce: 0.19059807062149048  (0.18006019584284544)\n",
            "     | > loss_mel_ce: 4.048148155212402  (4.266071504394473)\n",
            "     | > loss: 4.238746166229248  (4.446131709438046)\n",
            "\n",
            "\u001b[1m   --> STEP: 299\u001b[0m\n",
            "     | > loss_text_ce: 0.18518252670764923  (0.1800773273841993)\n",
            "     | > loss_mel_ce: 4.4109015464782715  (4.266555885806125)\n",
            "     | > loss: 4.596084117889404  (4.4466332225097895)\n",
            "\n",
            "\u001b[1m   --> STEP: 300\u001b[0m\n",
            "     | > loss_text_ce: 0.18232537806034088  (0.18008482088645308)\n",
            "     | > loss_mel_ce: 4.330835342407227  (4.266770150661461)\n",
            "     | > loss: 4.513160705566406  (4.446854980786645)\n",
            "\n",
            "\u001b[1m   --> STEP: 301\u001b[0m\n",
            "     | > loss_text_ce: 0.18064139783382416  (0.1800866699793015)\n",
            "     | > loss_mel_ce: 4.296457767486572  (4.26686878061769)\n",
            "     | > loss: 4.4770989418029785  (4.446955459062447)\n",
            "\n",
            "\u001b[1m   --> STEP: 302\u001b[0m\n",
            "     | > loss_text_ce: 0.17318271100521088  (0.18006380918799655)\n",
            "     | > loss_mel_ce: 4.105661392211914  (4.266334981318333)\n",
            "     | > loss: 4.278843879699707  (4.44639879820363)\n",
            "\n",
            "\u001b[1m   --> STEP: 303\u001b[0m\n",
            "     | > loss_text_ce: 0.17376399040222168  (0.18004301770685538)\n",
            "     | > loss_mel_ce: 4.010878086090088  (4.265491889254873)\n",
            "     | > loss: 4.1846418380737305  (4.445534913846766)\n",
            "\n",
            "\u001b[1m   --> STEP: 304\u001b[0m\n",
            "     | > loss_text_ce: 0.17205092310905457  (0.1800167279219942)\n",
            "     | > loss_mel_ce: 4.076300144195557  (4.264869547988231)\n",
            "     | > loss: 4.248351097106934  (4.444886282870648)\n",
            "\n",
            "\u001b[1m   --> STEP: 305\u001b[0m\n",
            "     | > loss_text_ce: 0.17968381941318512  (0.18001563641868662)\n",
            "     | > loss_mel_ce: 4.35045051574707  (4.265150141325145)\n",
            "     | > loss: 4.530134201049805  (4.445165784241728)\n",
            "\n",
            "\u001b[1m   --> STEP: 306\u001b[0m\n",
            "     | > loss_text_ce: 0.16983474791049957  (0.17998236554120886)\n",
            "     | > loss_mel_ce: 4.466103553771973  (4.265806851823337)\n",
            "     | > loss: 4.6359381675720215  (4.4457892234029375)\n",
            "\n",
            "\u001b[1m   --> STEP: 307\u001b[0m\n",
            "     | > loss_text_ce: 0.17451512813568115  (0.17996455695031136)\n",
            "     | > loss_mel_ce: 4.604450225830078  (4.266909924702837)\n",
            "     | > loss: 4.778965473175049  (4.446874488060176)\n",
            "\n",
            "\u001b[1m   --> STEP: 308\u001b[0m\n",
            "     | > loss_text_ce: 0.17147022485733032  (0.17993697795000949)\n",
            "     | > loss_mel_ce: 4.131773948669434  (4.266471171533897)\n",
            "     | > loss: 4.303244113922119  (4.446408155676611)\n",
            "\n",
            "\u001b[1m   --> STEP: 309\u001b[0m\n",
            "     | > loss_text_ce: 0.16989070177078247  (0.17990446572936472)\n",
            "     | > loss_mel_ce: 4.145732879638672  (4.266080432725175)\n",
            "     | > loss: 4.315623760223389  (4.445984905205888)\n",
            "\n",
            "\u001b[1m   --> STEP: 310\u001b[0m\n",
            "     | > loss_text_ce: 0.16247813403606415  (0.17984825175616054)\n",
            "     | > loss_mel_ce: 4.718997478485107  (4.267541455453433)\n",
            "     | > loss: 4.881475448608398  (4.447389713410413)\n",
            "\n",
            "\u001b[1m   --> STEP: 311\u001b[0m\n",
            "     | > loss_text_ce: 0.17309889197349548  (0.17982654963467287)\n",
            "     | > loss_mel_ce: 4.158544540405273  (4.267190983057779)\n",
            "     | > loss: 4.331643581390381  (4.44701753935247)\n",
            "\n",
            "\u001b[1m   --> STEP: 312\u001b[0m\n",
            "     | > loss_text_ce: 0.18172788619995117  (0.17983264366212567)\n",
            "     | > loss_mel_ce: 4.0893096923828125  (4.2666208507158725)\n",
            "     | > loss: 4.271037578582764  (4.446453501016669)\n",
            "\n",
            "\u001b[1m   --> STEP: 313\u001b[0m\n",
            "     | > loss_text_ce: 0.17627927660942078  (0.17982129105173367)\n",
            "     | > loss_mel_ce: 4.524633407592773  (4.26744517198385)\n",
            "     | > loss: 4.7009124755859375  (4.447266468986539)\n",
            "\n",
            "\u001b[1m   --> STEP: 314\u001b[0m\n",
            "     | > loss_text_ce: 0.17488685250282288  (0.17980557627928492)\n",
            "     | > loss_mel_ce: 4.371033191680908  (4.267775070135751)\n",
            "     | > loss: 4.545919895172119  (4.447580651872481)\n",
            "\n",
            "\u001b[1m   --> STEP: 315\u001b[0m\n",
            "     | > loss_text_ce: 0.18774503469467163  (0.17983078090917504)\n",
            "     | > loss_mel_ce: 4.344989776611328  (4.268020196188054)\n",
            "     | > loss: 4.5327348709106445  (4.44785098272657)\n",
            "\n",
            "\u001b[1m   --> STEP: 316\u001b[0m\n",
            "     | > loss_text_ce: 0.1699046492576599  (0.17979936910015126)\n",
            "     | > loss_mel_ce: 4.2437005043029785  (4.2679432351377855)\n",
            "     | > loss: 4.413605213165283  (4.447742610038085)\n",
            "\n",
            "\u001b[1m   --> STEP: 317\u001b[0m\n",
            "     | > loss_text_ce: 0.17561845481395721  (0.17978618009609387)\n",
            "     | > loss_mel_ce: 4.342936992645264  (4.268179808505317)\n",
            "     | > loss: 4.518555641174316  (4.447965994994351)\n",
            "\n",
            "\u001b[1m   --> STEP: 318\u001b[0m\n",
            "     | > loss_text_ce: 0.17412935197353363  (0.17976839132841288)\n",
            "     | > loss_mel_ce: 4.443541526794434  (4.268731260449622)\n",
            "     | > loss: 4.617671012878418  (4.448499658572603)\n",
            "\n",
            "\u001b[1m   --> STEP: 319\u001b[0m\n",
            "     | > loss_text_ce: 0.17123374342918396  (0.17974163694628362)\n",
            "     | > loss_mel_ce: 4.742157459259033  (4.2702153551167354)\n",
            "     | > loss: 4.91339111328125  (4.449956998556015)\n",
            "\n",
            "\u001b[1m   --> STEP: 320\u001b[0m\n",
            "     | > loss_text_ce: 0.17369632422924042  (0.17972274534404287)\n",
            "     | > loss_mel_ce: 4.137509822845459  (4.269800650328388)\n",
            "     | > loss: 4.311206340789795  (4.449523402750495)\n",
            "\n",
            "\u001b[1m   --> STEP: 321\u001b[0m\n",
            "     | > loss_text_ce: 0.1734297275543213  (0.17970314092725245)\n",
            "     | > loss_mel_ce: 4.4147868156433105  (4.270252320625319)\n",
            "     | > loss: 4.588216781616211  (4.449955469351322)\n",
            "\n",
            "\u001b[1m   --> STEP: 322\u001b[0m\n",
            "     | > loss_text_ce: 0.17963968217372894  (0.17970294385037816)\n",
            "     | > loss_mel_ce: 4.202388286590576  (4.270041562755646)\n",
            "     | > loss: 4.382028102874756  (4.449744514797047)\n",
            "\n",
            "\u001b[1m   --> STEP: 323\u001b[0m\n",
            "     | > loss_text_ce: 0.1743249148130417  (0.17968629360568053)\n",
            "     | > loss_mel_ce: 4.219775676727295  (4.269885940817478)\n",
            "     | > loss: 4.394100666046143  (4.44957224281949)\n",
            "\n",
            "\u001b[1m   --> STEP: 324\u001b[0m\n",
            "     | > loss_text_ce: 0.17074301838874817  (0.1796586909043937)\n",
            "     | > loss_mel_ce: 4.327230930328369  (4.270062931525844)\n",
            "     | > loss: 4.497973918914795  (4.449721630708673)\n",
            "\n",
            "\u001b[1m   --> STEP: 325\u001b[0m\n",
            "     | > loss_text_ce: 0.17163413763046265  (0.17963399997124316)\n",
            "     | > loss_mel_ce: 4.1652936935424805  (4.269740564639742)\n",
            "     | > loss: 4.336927890777588  (4.449374573047346)\n",
            "\n",
            "\u001b[1m   --> STEP: 326\u001b[0m\n",
            "     | > loss_text_ce: 0.18788200616836548  (0.17965930060374968)\n",
            "     | > loss_mel_ce: 4.129627227783203  (4.269310769127912)\n",
            "     | > loss: 4.317509174346924  (4.4489700779593075)\n",
            "\n",
            "\u001b[1m   --> STEP: 327\u001b[0m\n",
            "     | > loss_text_ce: 0.176322802901268  (0.17964909724686137)\n",
            "     | > loss_mel_ce: 4.4250922203063965  (4.269787165003075)\n",
            "     | > loss: 4.601415157318115  (4.449436270862546)\n",
            "\n",
            "\u001b[1m   --> STEP: 328\u001b[0m\n",
            "     | > loss_text_ce: 0.1963663101196289  (0.17970006435927835)\n",
            "     | > loss_mel_ce: 4.094188690185547  (4.269251803799363)\n",
            "     | > loss: 4.290555000305176  (4.448951876744993)\n",
            "\n",
            "\u001b[1m   --> STEP: 329\u001b[0m\n",
            "     | > loss_text_ce: 0.17601223289966583  (0.1796888551451154)\n",
            "     | > loss_mel_ce: 4.25553035736084  (4.269210097275233)\n",
            "     | > loss: 4.43154239654541  (4.448898960391803)\n",
            "\n",
            "\u001b[1m   --> STEP: 330\u001b[0m\n",
            "     | > loss_text_ce: 0.1776505410671234  (0.17968267843578817)\n",
            "     | > loss_mel_ce: 4.19554328918457  (4.268986864523444)\n",
            "     | > loss: 4.373193740844727  (4.4486695506356)\n",
            "\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.10248620871341585 \u001b[0m(+0.0)\n",
            "     | > avg_loss_text_ce: 0.17968267843578817 \u001b[0m(+0.0)\n",
            "     | > avg_loss_mel_ce: 4.268986864523444 \u001b[0m(+0.0)\n",
            "     | > avg_loss: 4.4486695506356 \u001b[0m(+0.0)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/XTTS_Maltese_Training/training/GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78/best_model_1328.pth\n",
            "Training finished!\n",
            "Saving final model...\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/XTTS_Maltese_Training/training/GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78/checkpoint_1328.pth\n",
            "Saving configuration...\n",
            "Error: Configuration file not found at /content/drive/MyDrive/XTTS_Maltese_Training/training/config.json. It was not created.\n",
            "Parameters that would have been used are:\n",
            "  gpt_batch_size: 1\n",
            "  enable_redaction: False\n",
            "  kv_cache: True\n",
            "  gpt_checkpoint: None\n",
            "  clvp_checkpoint: \n",
            "  decoder_checkpoint: \n",
            "  num_chars: 255\n",
            "  tokenizer_file: /content/drive/MyDrive/XTTS_Maltese_Training/vocab.json\n",
            "  gpt_max_audio_tokens: 605\n",
            "  gpt_max_text_tokens: 402\n",
            "  gpt_max_prompt_tokens: 70\n",
            "  gpt_layers: 30\n",
            "  gpt_n_model_channels: 1024\n",
            "  gpt_n_heads: 16\n",
            "  gpt_number_text_tokens: 10537\n",
            "  gpt_start_text_token: 261\n",
            "  gpt_stop_text_token: 0\n",
            "  gpt_num_audio_tokens: 1026\n",
            "  gpt_start_audio_token: 1024\n",
            "  gpt_stop_audio_token: 1025\n",
            "  gpt_code_stride_len: 1024\n",
            "  gpt_use_masking_gt_prompt_approach: True\n",
            "  gpt_use_perceiver_resampler: True\n",
            "  input_sample_rate: 22050\n",
            "  output_sample_rate: 24000\n",
            "  output_hop_length: 256\n",
            "  decoder_input_dim: 1024\n",
            "  d_vector_dim: 512\n",
            "  cond_d_vector_in_each_upsampling_layer: True\n",
            "  duration_const: 102400\n",
            "  min_conditioning_length: 66150\n",
            "  max_conditioning_length: 132300\n",
            "  gpt_loss_text_ce_weight: 0.01\n",
            "  gpt_loss_mel_ce_weight: 1.0\n",
            "  debug_loading_failures: False\n",
            "  max_wav_length: 255995\n",
            "  max_text_length: 200\n",
            "  mel_norm_file: /content/drive/MyDrive/XTTS_Maltese_Training/mel_stats.pth\n",
            "  dvae_checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/dvae.pth\n",
            "  xtts_checkpoint: /content/drive/MyDrive/XTTS_Maltese_Training/model.pth\n",
            "  vocoder: \n",
            "Speaker reference: /content/drive/MyDrive/XTTS_Maltese_Data/wavs/MSRHS_M_11_P24U082_0147.wav\n",
            "Checkpoint saved in dir: /content/drive/MyDrive/XTTS_Maltese_Training/training/GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78\n",
            "Finetuning process completed!\n"
          ]
        }
      ],
      "source": [
        "%env OUTPUT_PATH=/content/drive/MyDrive/XTTS_Maltese_Training\n",
        "%env META_TRAIN=/content/drive/MyDrive/XTTS_Maltese_Data/metadata_train.csv\n",
        "%env META_EVAL=/content/drive/MyDrive/XTTS_Maltese_Data/metadata_eval.csv\n",
        "%env LANG=mt\n",
        "%env TOKENIZERS_PARALLELISM=false\n",
        "%env OMP_NUM_THREADS=1\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import os\n",
        "output_path = os.environ.get(\"OUTPUT_PATH\")\n",
        "metadata_train_path = os.environ.get(\"META_TRAIN\")\n",
        "metadata_eval_path = os.environ.get(\"META_EVAL\")\n",
        "language_code = os.environ.get(\"LANG\")\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path, exist_ok=True)\n",
        "output_redirect(output_path, True)\n",
        "\n",
        "\n",
        "# 35min/epoch on one T4 with batch_size=1, grad_acumm=48, audio_length=255995, max_text=200, weight=1e-2\n",
        "print(f\"Finetuning for {language_code}...\")\n",
        "%cd /content/Malta-TTS/FineTuning/NewLanguage\n",
        "!python new_language_training_cli.py \\\n",
        "    --is_download \\\n",
        "    --is_tokenizer_extension \\\n",
        "    --output_path $OUTPUT_PATH \\\n",
        "    --metadatas \"$META_TRAIN,$META_EVAL,$LANG\" \\\n",
        "    --num_epochs 1 \\\n",
        "    --batch_size 3 \\\n",
        "    --grad_acumm 48 \\\n",
        "    --max_audio_length 255995 \\\n",
        "    --max_text_length 200 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --lr 5e-6 \\\n",
        "    --save_step 50000 \\\n",
        "    --version=main \\\n",
        "    --language $LANG \\\n",
        "    --forgetting_mitigation \"none\" # or LORA or Freeze\n",
        "# --multi-gpu\n",
        "\n",
        "# lr = 5e-7, weight_decay = 0.1 for avoiding forgetting\n",
        "\n",
        "\n",
        "print(\"Finetuning process completed!\")\n",
        "output_redirect(output_path, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4A-u_YIQsDA",
      "metadata": {
        "id": "c4A-u_YIQsDA"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "RWDQzkXxQtEK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RWDQzkXxQtEK",
        "outputId": "8b599992-9b01-4365-ded7-1e55be6983f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ff\n",
            "/content/Malta-TTS/FineTuning/NewLanguage\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Malta-TTS/FineTuning/NewLanguage/inference.py\", line 1, in <module>\n",
            "    import compatibility\n",
            "  File \"/content/Malta-TTS/FineTuning/NewLanguage/compatibility.py\", line 5, in <module>\n",
            "    import TTS\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/TTS/__init__.py\", line 3, in <module>\n",
            "    from TTS.utils.generic_utils import is_pytorch_at_least_2_4\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/TTS/utils/generic_utils.py\", line 12, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\", line 250, in <module>\n",
            "    import torch._refs\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_refs/__init__.py\", line 1112, in <module>\n",
            "    @_make_elementwise_binary_reference(\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_refs/__init__.py\", line 1026, in inner\n",
            "    @elementwise_type_promotion_wrapper(\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_prims_common/wrappers.py\", line 117, in __call__\n",
            "    sig = inspect.signature(fn)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3348, in signature\n",
            "    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3085, in from_callable\n",
            "    return _signature_from_callable(obj, sigcls=cls,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 2597, in _signature_from_callable\n",
            "    return _signature_from_function(sigcls, obj,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 2487, in _signature_from_function\n",
            "    return cls(parameters,\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3076, in __init__\n",
            "    params = OrderedDict((param.name, param) for param in parameters)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3076, in <genexpr>\n",
            "    params = OrderedDict((param.name, param) for param in parameters)\n",
            "                          ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 2796, in name\n",
            "    @property\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: '/content/drive/MyDrive/XTTS_Maltese_Training/output_mt.wav'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1512151846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python inference.py      --xtts_checkpoint=\"{xtts_checkpoint}\"      --xtts_config=\"{xtts_config}\"      --xtts_vocab=\"{xtts_vocab}\"      --tts_text=\"{tts_text}\"      --speaker_audio_file=\"{speaker_audio_file}\"      --lang=\"{lang}\"      --output_file=\"{output_file}\"      --temperature 0.7      --length_penalty 1.0      --repetition_penalty 10.0      --top_k 50      --top_p 0.8  '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_make_wav\u001b[0;34m(data, rate, normalize)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_with_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_without_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m_validate_and_normalize_with_numpy\u001b[0;34m(data, normalize)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mnchan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '/content/drive/MyDrive/XTTS_Maltese_Training/output_mt.wav'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "output_path = os.environ.get(\"OUTPUT_PATH\") or \"/content/drive/MyDrive/XTTS_Maltese_Training\"\n",
        "xtts_checkpoint = os.path.join(output_path, \"training/\", \"GPT_XTTS_FT-September-02-2025_08+57AM-6f60f78/\", \"best_model.pth\")\n",
        "xtts_checkpoint = os.path.join(output_path, \"training/\", \"GPT_XTTS_FT-September-02-2025_08+21AM-6f60f78/\", \"checkpoint_77.pth\")\n",
        "xtts_config = os.path.join(output_path, \"config.json\")\n",
        "xtts_vocab = os.path.join(output_path, \"vocab.json\")\n",
        "\n",
        "tts_text = \"Il-kelma Maltija 'bonġu' tfisser 'good morning'.\"\n",
        "speaker_audio_file = \"/content/drive/MyDrive/XTTS_Maltese_Data/wavs/MSRHS_M_11_P24U082_0147.wav\"\n",
        "lang = \"mt\"\n",
        "\n",
        "# tts_text = \"Hi, how are you?\"\n",
        "# speaker_audio_file = \"/content/drive/MyDrive/english_speaker.mp3\"\n",
        "# lang=\"en\"\n",
        "output_file = os.path.join(output_path, \"output_\"+lang+\".wav\")\n",
        "\n",
        "%cd /content/Malta-TTS/FineTuning/NewLanguage\n",
        "!python inference.py \\\n",
        "    --xtts_checkpoint=\"{xtts_checkpoint}\" \\\n",
        "    --xtts_config=\"{xtts_config}\" \\\n",
        "    --xtts_vocab=\"{xtts_vocab}\" \\\n",
        "    --tts_text=\"{tts_text}\" \\\n",
        "    --speaker_audio_file=\"{speaker_audio_file}\" \\\n",
        "    --lang=\"{lang}\" \\\n",
        "    --output_file=\"{output_file}\" \\\n",
        "    --temperature 0.7 \\\n",
        "    --length_penalty 1.0 \\\n",
        "    --repetition_penalty 10.0 \\\n",
        "    --top_k 50 \\\n",
        "    --top_p 0.8 \\\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(output_file, rate=24000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SXpeL_2OO_nc",
      "metadata": {
        "id": "SXpeL_2OO_nc"
      },
      "source": [
        "### Widget use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "AzE3QnxIMzUp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63,
          "referenced_widgets": [
            "c3db9b1aeb1b4f40888929e06d048f29",
            "c077f41102b24fbab69727f69586684d",
            "ba27955965c547dabbcd0633ca9bfb01"
          ]
        },
        "id": "AzE3QnxIMzUp",
        "outputId": "a9d58133-d7b0-4f9c-96ea-afb8fb99797b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3db9b1aeb1b4f40888929e06d048f29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Textarea(value=\"Il-kelma Maltija 'bonġu' tfisser 'good morning'.\", description='Text:', placeholder='Type some…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "text = widgets.Textarea(\n",
        "   value=\"Il-kelma Maltija 'bonġu' tfisser 'good morning'.\",\n",
        "   placeholder='Type something',\n",
        "   description='Text:',\n",
        "   disabled=False\n",
        ")\n",
        "display(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E3oAZLkaO-nr",
      "metadata": {
        "id": "E3oAZLkaO-nr"
      },
      "outputs": [],
      "source": [
        "print(text.value)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "29a1c709",
        "697307d4",
        "e498624b"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "xtts_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba27955965c547dabbcd0633ca9bfb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c077f41102b24fbab69727f69586684d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3db9b1aeb1b4f40888929e06d048f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextareaModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Text:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c077f41102b24fbab69727f69586684d",
            "placeholder": "Type something",
            "rows": null,
            "style": "IPY_MODEL_ba27955965c547dabbcd0633ca9bfb01",
            "value": "Ff"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
